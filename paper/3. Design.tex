\section{\DeepVis System Design}
\label{sec:design}

In this section, we present the design of \DeepVis, a scalable integrity verification framework for hyperscale cloud environments. \DeepVis does not rely on sequential file scanning or heavy kernel instrumentation, but instead employs a snapshot-based hybrid architecture that decouples metadata ingestion from anomaly detection. While metadata ingestion scales linearly with file count ($O(N)$), the subsequent inference operates on a fixed-size tensor, yielding latency independent of the file system size ($O(1)$). To overcome the I/O bottlenecks inherent in scanning millions of files, it utilizes a parallelized asynchronous pipeline for metadata collection and leverages a deterministic hash-based mapping to transform unordered file systems into fixed-size tensor representations.

\subsection{Overall Procedure}
\label{design_1}

Figure~\ref{fig:overall} shows the overall procedure of \DeepVis. \DeepVis provides two main phases to support distributed integrity verification: the \textit{Snapshot} phase and the \textit{Verification} phase.

\begin{figure}[t]
    \centering
    % [User Check] 파일명 유지함. 
    % 단, PDF 내용이 텍스트와 일치하는지(128x128 등) 확인 필요.
    \includegraphics[width=9cm]{Figures/Design/Overall_Arch.pdf}
    \caption{Overall procedure of \DeepVis. It illustrates the transformation of raw file system metadata into spatially mapped tensors, followed by reconstruction via an autoencoder and anomaly detection using Local Max ($L_\infty$) logic.}
    \label{fig:overall}
\end{figure}

\noindent
\textbf{Snapshot Phase.} When integrity verification starts, the data collection process is initiated. Unlike existing synchronous tools (e.g., \texttt{find} or \texttt{ls}) that block on every file access, \DeepVis utilizes a hybrid parallel architecture. Multiple worker threads traverse the directory tree and collect file paths (\ding{182}), feeding them into a lock-free queue. These paths are batched and submitted to the kernel using the \texttt{io\_uring} interface, ensuring that I/O throughput saturates the storage bandwidth rather than being latency-bound (\ding{183}).

After collecting raw metadata and file headers, secure spatial mapping is performed. A deterministic coordinate is calculated for each file using a Keyed-Hash Message Authentication Code (HMAC) (\ding{184}), and multi-modal features (entropy, permissions) are extracted (\ding{185}). These features are aggregated into a fixed-size 2D tensor ($128 \times 128 \times 3$), effectively transforming the file system state into an image-like representation (\ding{186}).

\noindent\textbf{Verification Phase.} After the first phase is completed, \DeepVis enters the verification phase. The generated tensor is fed into a pre-trained 1$\times$1 Convolutional Autoencoder (CAE). While standard CNNs exploit spatial locality to find shapes, the hash-based mapping lacks semantic neighborhood relationships. Therefore, 1$\times$1 Convolutions are employed not to extract spatial features, but to learn complex cross-channel non-linear correlations (e.g., distinguishing a high-entropy zip file in a user directory from a high-entropy packed binary in a system path). This effectively acts as a learnable, non-linear per-pixel thresholding mechanism (\ding{187}).

The pixel-wise difference between the input and reconstructed tensors is then computed. To resolve the statistical asymmetry between legitimate diffuse updates and sparse attacks, Local Max Detection ($L_\infty$) is utilized (\ding{188}). This mechanism isolates the single highest deviation in the grid. Finally, if the $L_\infty$ score exceeds a dynamically learned threshold, an alert is raised, identifying the presence of a stealthy anomaly such as a rootkit (\ding{189}).



\subsection{Asynchronous File System Traversal}
\label{design_2}

Before generating the tensor representation, \DeepVis processes metadata and file headers across thousands of cloud instances. Existing synchronous system calls (e.g., \texttt{stat}, \texttt{open}, \texttt{read}) cause context switching overhead and CPU blocking at scale. Network-attached storage in cloud environments exacerbates I/O latency. \DeepVis overcomes this with a hybrid pipeline separating CPU-bound path traversal from I/O-bound data reading.

\begin{figure}[t]
    \centering
    \includegraphics[width=8cm]{Figures/Design/io_Arch.pdf}
    \caption{Hybrid snapshot pipeline of \DeepVis using Rayon for parallel path collection and \texttt{io\_uring} for asynchronous I/O.}
    \label{fig:pipeline}
\end{figure}

\noindent\textbf{Parallel Path Collection.} \DeepVis uses work-stealing parallelism from the Rust \texttt{rayon} library for path collection.
As shown on the left of Figure~\ref{fig:pipeline}, the \textit{Path Collector Threadpool} spawns worker threads that execute synchronous \texttt{fs::read\_dir} operations recursively. This CPU-bound phase parses directory entries and builds path strings across all cores, filling the \textit{Pending Path Queue} faster than I/O consumption.

\noindent\textbf{Asynchronous I/O Processing.} With paths enqueued, reading file headers becomes the bottleneck. \DeepVis employs Linux \texttt{io\_uring} to eliminate per-file system call overhead. As shown in Figure~\ref{fig:pipeline}, the \textit{io\_uring Submitter} batches paths into Submission Queue (SQ) read requests (\texttt{OP\_READ}). Unlike traditional async I/O, \texttt{io\_uring} uses shared ring buffers for kernel-user communication. \textit{Data Processor} threads poll the Completion Queue (CQ) for finished reads. Completed events trigger data retrieval from pre-allocated \textit{Buffer Slab}s, followed by immediate hashing and entropy calculation. CPU threads never block on disk I/O. The kernel handles data movement while user-space processes features. This achieves throughput competitive with raw disk bandwidth.




\subsection{Header Sampling}
\label{design_sampling}

Traditional FIM tools hash entire files, causing massive I/O overhead ($O(N \times Size)$). Conversely, metadata-only scanning (e.g., file size, name) produces high false negatives against padded malware. To balance these extremes, \DeepVis adopts header-based entropy sampling.

As discussed in Section~\ref{sec:background}, packed malware and ransomware inevitably modify file headers to accommodate unpacker stubs or encrypted payloads, significantly increasing entropy in the first few blocks. To detect this, \DeepVis reads only the first 4KB of each file asynchronously. File systems use 4KB block size, so sub-4KB requests incur full 4KB I/O padded with zeros, while larger reads require additional requests per file. As Linux headers reside in the first 128 bytes, evaluation results show 4KB suffices for most malicious files.

Reading the first 4KB enables header sampling to excel against executable malware requiring loader compatibility (ELF/PE). Packed binaries, ransomware, and rootkits must alter headers for unpackers or encrypted payloads, unlike data files hiding payloads at arbitrary offsets. This 4KB page-aligned approach reduces per-file I/O independent of file size while retaining binary format anomaly sensitivity, providing high-frequency first-line defense.


\subsection{Hash-Based Spatial Mapping}
\label{design_3}

\noindent\textbf{Spatial Invariance.} After asynchronously reading metadata and 4KB header blocks, \DeepVis must transform thousands of unordered files into a fixed-size tensor for neural processing. Traditional ordering-based approaches (e.g., alphabetical sorting by path or directory tree) suffer from the Ordering Problem. Inserting a single new file shifts the indices of all subsequent files, destroying spatial locality across scans and invalidating trained neural network models. To overcome this, \DeepVis employs a deterministic hash-based spatial mapping to project each file onto a fixed-size grid.



\begin{figure}[t]
    \centering
    \includegraphics[width=9cm]{Figures/Design/Hash_Arch.pdf}
    \caption{Shift invariance comparison. Top: Traditional ordered approach creates catastrophic global index shifts when File D inserts between B and C. Bottom: Hash-based mapping ensures local stability—only affected pixels change positions independently.}
    \label{fig:hash}
\end{figure}

Figure~\ref{fig:hash} illustrates the comparison between the traditional ordered approach and the hash-based mapping employed by \DeepVis. As depicted in the upper panel, inserting File B forces updates to the indices of files C and D. This cascading shift becomes critically expensive in large-scale cloud systems containing millions of files. In contrast, the lower panel demonstrates how \DeepVis calculates stable coordinates $\Phi(p)$ for each file path $p$. By utilizing a high-entropy secret key $K$ generated at startup, the system maps files onto a fixed-size $128 \times 128$ tensor. This process generates a uniform representation of the file system state and facilitates the visualization of system health.

To derive coordinates for each file, \DeepVis employs the first 32 bits of the HMAC output modulo 128 for the x-coordinate and the subsequent 32 bits (bits 32 to 64) modulo 128 for the y-coordinate. The utilization of HMAC to establish stable coordinates provides two critical benefits. First, deterministic bucket mapping ensures that coordinates depend solely on the file path and the secret key, producing reproducible $128 \times 128$ grids across successive scans. Second, cryptographic key $K$ defeats targeted mapping attacks. Adversaries cannot craft filenames to reach specific low-risk coordinates. Ephemeral keys and privileged memory access restrict $K$ extraction.

\noindent\textbf{Multi-Modal RGB Encoding} Within the hash-mapped coordinate space, \DeepVis encodes each pixel through three risk channels: Red, Green, and Blue. These represent file characteristics for image-based visualization. The channels leverage malware feature orthogonality, ensuring evasion of one channel increases risk in others.

\begin{itemize}[leftmargin=*]
    \item \textbf{R (Entropy)} The Shannon entropy of the 4KB file header exploits the inherent trade-off in malware design. Legitimate ELF binaries incorporate zero-padding for section alignment, resulting in low entropy, whereas packed malware employs high information density to obfuscate signatures. This channel effectively distinguishes packed threats from standard system executables.

\item \textbf{G (Context Hazard) as Spatial Prior.} To eliminate heuristic ambiguity, spatial risk is quantified using a deterministic mapping derived from the Filesystem Hierarchy Standard (FHS). A risk function $G(p) \in [0, 1]$ is defined for a file path $p$:
\begin{equation}
    G(p) = \min\left(1.0, \sum_{k \in \mathcal{K}_{\text{FHS}}} w_k \cdot \mathbb{I}(p \in \text{Zone}_k) + \alpha \cdot \text{Depth}(p)\right)
\end{equation}
where $\mathcal{K}_{\text{FHS}}$ represents standard directory roles (e.g., volatile memory \texttt{/dev/shm} [$w=0.9$] vs. static binaries \texttt{/usr/bin} [$w=0.1$]). These specific values maximize the separation between immutable system paths and writable attack surfaces. By anchoring weights $w_k$ to the FHS violation probability rather than arbitrary tuning, this channel acts as a hard attention mechanism, forcing the model to prioritize structural anomalies in locations deemed volatile by OS standards.

\item \textbf{B (Structure) as Compliance Distance.} This channel quantifies the distance from specification compliance rather than employing arbitrary header parsing. Kernel modules (\texttt{.ko}) and shared objects (\texttt{.so}) residing outside system-privileged paths (e.g., \texttt{/lib/modules}) are assigned $B=1.0$. This metric is not a heuristic but a direct boolean constraint reflecting OS loader policies: relocatable code in user-writable paths is inherently anomalous. This compresses complex loader logic into an $O(1)$ lookup.
\end{itemize}

This RGB encoding transforms the abstract file system state into a dense numerical tensor $T \in \mathbb{R}^{128 \times 128 \times 3}$. This transformation enables the downstream Hash-Grid Parallel CAE to learn complex cross-channel correlations. Unlike simple linear thresholding, which fails to capture conditional dependencies (e.g., high entropy is permissible in static binaries but anomalous in temporary paths), the CAE learns the non-linear manifold of valid structural states. The scanner maintains an inverted index that maps each pixel coordinate back to its constituent file paths. This ensures that operators can attribute the violation to a specific file once an anomaly is detected.

\noindent\textbf{Mapping Robustness and Security.} Since the grid size remains fixed while cloud systems scale to massive file counts, hash collisions become inevitable. \DeepVis addresses both natural collisions and adversarial targeting by projecting files onto a fixed $128 \times 128$ grid and applying max-risk pooling, where each pixel retains the maximum feature value across all colliding files per RGB channel. A single high-risk file therefore determines the pixel value, ensuring that malicious signals dominate despite benign collisions. To prevent bucket targeting attacks, where adversaries engineer filenames to land in specific coordinates, the system uses a high-entropy secret key $K$ and periodically rotates it, shuffling the entire grid without retraining because downstream $1 \times 1$ convolutions are spatially agnostic.




\subsection{Hash-Grid Parallel CAE}
\label{design_4}

\begin{figure}[t]
    \centering
    \includegraphics[width=9cm]{Figures/Design/CNN_Arch.pdf}
    \caption{Set-AE vs. Hash-Grid CAE. Top: Set-AE global pooling dilutes single malicious signal among benign files (Spike Lost). Bottom: Hash-Grid CAE processes 16K pixels independently; $L_\infty$ pooling captures isolated spikes.}
    \label{fig:comparison}
\end{figure}

\noindent\textbf{Signal Preservation via Parallel Processing.} To ensure robust detection in hyperscale environments, DeepVis fundamentally alters the aggregation strategy by abandoning global pooling in favor of a parallelized pixel-wise architecture. As illustrated in the top panel of Figure~\ref{fig:comparison}, traditional models such as Set-AE aggregate features into a single global vector. This architectural choice results in signal dilution, where the distinct feature vector of a single compromised file is aggregated with an increasing volume of benign metadata. In a hyperscale scenario where $N_{benign} \gg N_{malicious}$, the attack spike drowns in the statistical variance of millions of benign files, rendering detection impossible. In contrast, as shown in the bottom of Figure~\ref{fig:comparison}, the Hash-Grid Parallel CAE processes the $128 \times 128$ spatial grid as independent feature channels. By utilizing $1 \times 1$ convolutions, the model maintains strict isolation between pixels throughout the encoding and decoding phases. This design prevents error propagation from benign regions to malicious ones, ensuring that the structural footprint of a rootkit remains mathematically distinct regardless of the total filesystem size.

\noindent\textbf{Pixel-Wise Reconstruction and Error Map Generation.} The core of the detection pipeline is the transformation of the raw system snapshot into a fine-grained anomaly map without cross-pixel contamination. The input tensor is fed into a $1 \times 1$ Convolutional Autoencoder which effectively functions as a massive array of shared Multi-Layer Perceptrons (MLP) operating simultaneously on all 16,384 pixels. The encoder utilizes point-wise layers (Enc: $3 \rightarrow 16 \rightarrow 8$) to compress the RGB channels into a latent representation that captures the manifold of valid file states, while the decoder ($8 \rightarrow 16 \rightarrow 3$) attempts to reconstruct the original input. Crucially, because the convolution kernel size is $1 \times 1$, the reconstruction of any given pixel depends solely on its own depth-wise features (Entropy, Context, Structure) and is uninfluenced by its neighbors. The system then computes the element-wise difference between the input snapshot and the reconstructed output to generate a Pixel-wise $L_2$ Error Map. This map explicitly visualizes the deviation intensity for every coordinate in the grid, isolating specific files that violate the learned structural norms of the fleet.

\noindent\textbf{Solving the MSE Paradox with $L_\infty$ Scoring.} The final detection phase addresses the MSE Paradox where legitimate system updates generate high global error while stealthy attacks generate low global error. Production environments are characterized by frequent legitimate updates such as package upgrades, which introduce diffuse noise across the grid. Using global Mean Squared Error (MSE) metrics in this context misclassifies benign churn as anomalous because the cumulative error of thousands of legitimate updates exceeds the error of a single stealthy rootkit. DeepVis resolves this by applying Maximum Deviation ($L_\infty$) pooling over the generated error map. By calculating the score as $Score = \max_{i,j} |T_{i,j} - T'_{i,j}|$, the system ignores the cumulative sum of low-magnitude noise and focuses exclusively on the single highest anomaly peak. As shown in the bottom of Figure~\ref{fig:comparison}, this strategy captures the $L_\infty$ spike generated by the malicious file while filtering out the background noise of valid system updates. This approach decouples detection sensitivity from the volume of benign churn, allowing the system to isolate the most distinct structural violation within the file system.

\noindent\textbf{Anomaly Decision and Unsupervised Calibration.} To distinguish malicious anomalies from benign variations, DeepVis establishes a decision boundary based on the reconstruction capability of the CAE. The system defines an anomaly condition as $Score > \tau$, where $\tau$ represents the maximum reconstruction error observed in a clean environment. To derive this threshold, we perform an offline calibration using a validation dataset consisting exclusively of benign Linux ELF binaries. The system processes each benign file to generate a tensor, passes it through the trained CAE, and calculates the $L_\infty$ score from the resulting error map. The threshold $\tau$ is then set to the highest $L_\infty$ score recorded across this validation corpus. This method effectively measures the worst-case structural variance of legitimate software that the model can reconstruct. While detection on other platforms such as Windows or Android would require learning their specific binary formats (e.g., PE or DEX), this study limits its scope to Linux to strictly evaluate efficacy against server-grade rootkits. Consequently, during online monitoring, any input producing a residual error exceeding this calibrated maximum indicates a data pattern that deviates from the learned characteristics of the Linux ELF format, triggering a security alert.


\subsection{\DeepVis Implementation}

\DeepVis employs a hybrid Rust-Python architecture to reconcile the conflicting requirements of low-level I/O throughput and high-level deep learning flexibility. The high-performance core is implemented as the \texttt{deepvis\_scanner} Rust module, which orchestrates asynchronous filesystem operations to minimize kernel-to-user context switching. This module leverages the Linux \texttt{io\_uring} interface with a submission queue depth of 512, enabling the batching of 4KB header reads to saturate storage bandwidth. To maintain pipeline saturation, the \texttt{rayon} library manages a work-stealing thread pool for parallel path traversal, ensuring that metadata ingestion fills the pending queue faster than the I/O subsystem can consume it. Within this native layer, the system executes computationally intensive feature engineering tasks—including Shannon entropy calculation, HMAC-based coordinate generation, and max-risk tensor construction ($3 \times 128 \times 128$)—effectively offloading overhead from the Python interpreter.

To interface with the anomaly detection logic, \texttt{pyo3} bindings expose the \texttt{DeepVisScanner} class, providing a direct Foreign Function Interface (FFI) to the underlying Rust engine. This abstraction enables zero-copy data transfer of the constructed tensors via methods such as \texttt{scan\_to\_tensor()}, facilitating immediate integration with PyTorch inference engines or ONNX runtimes on CPU-constrained edge devices. Furthermore, the implementation incorporates operational heuristics to ensure stability in production environments, such as the automatic exclusion of volatile pseudo-filesystems (e.g., \texttt{/proc}, \texttt{/sys}, \texttt{/dev}) and the synchronization of CPU-bound path discovery with kernel-level data movement. This design guarantees that the memory-safe Rust backend handles all data-intensive operations while the Python frontend retains flexibility for model deployment and threshold management.