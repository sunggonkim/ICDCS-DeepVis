\section{System Design}
\label{Design}

In this section, we present the design of \DeepVis, a high-throughput file integrity verification system optimized for hyperscale distributed storage. \DeepVis does not rely on sequential file enumeration or heavy kernel instrumentation. Instead, it adopts a hybrid architecture combining an asynchronous metadata snapshot engine implemented in Rust with a constant-time neural inference pipeline implemented in Python. This design enables practical deployment as a Kubernetes sidecar while maintaining detection accuracy against modern rootkit threats.

\noindent\textbf{Clarifying Complexity Claims.} We emphasize upfront that \DeepVis does \textit{not} achieve $O(1)$ end-to-end verification latency. The full pipeline consists of:
\begin{enumerate}
    \item \textbf{Snapshot Collection ($O(N)$):} Enumeration of file metadata via \texttt{io\_uring}.
    \item \textbf{Feature Extraction ($O(N)$):} Parallel computation of entropy from file headers.
    \item \textbf{Tensor Generation ($O(N)$):} Hash-based mapping of files to 2D coordinates.
    \item \textbf{Model Inference ($O(1)$):} Anomaly detection on a fixed-size $128 \times 128$ tensor.
\end{enumerate}
The key insight is that the \textit{inference complexity} is decoupled from the file count. While the $O(N)$ phases are accelerated via parallel I/O and batched system calls, the costly analysis phase remains constant regardless of system scale. This enables amortization strategies (e.g., incremental updates) not possible with traditional $O(N)$ hash-comparison tools like AIDE or OSSEC.

\subsection{Overall Procedure}
\label{design_1}

Figure~\ref{fig:deepvis_overall} shows the overall procedure of \DeepVis. The system provides two main phases to support distributed file integrity verification: the \textit{Asynchronous Snapshot Phase} and the \textit{Neural Verification Phase}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/DeepVis_Overall.png}
    \caption{Overall procedure of \DeepVis. The Rust-based Snapshot Engine utilizes \texttt{io\_uring} for asynchronous metadata collection and parallel entropy computation. The Tensor Generator maps files to a 2D grid using SHA-256 hashing. The Inference Engine performs $O(1)$ detection using an INT8-quantized Convolutional Autoencoder.}
    \label{fig:deepvis_overall}
\end{figure*}

\noindent
\textbf{Asynchronous Snapshot Phase.} When integrity verification initiates, the \textit{Snapshot Engine} bypasses standard blocking system calls. Traditional file integrity monitoring tools suffer from significant overhead due to synchronous \texttt{stat()} calls, which block on storage I/O for each file. Instead, \DeepVis utilizes the Linux \texttt{io\_uring} interface to submit batches of \texttt{statx} requests to the kernel submission queue (\ding{182}). Simultaneously, a thread pool reads file headers (first 64 bytes) to estimate entropy in parallel (\ding{183}). This parallelized design saturates NVMe bandwidth, transforming the I/O bottleneck from latency-bound to throughput-bound. Our benchmarks indicate this approach achieves 95\% NVMe bandwidth utilization compared to 23\% for synchronous scanning.

\noindent
\textbf{Neural Verification Phase.} As metadata flows from the snapshot engine, the \textit{Tensor Generator} maps each file to a 2D spatial grid using cryptographic hashing (\ding{184}). This process is lock-free; each worker thread maintains a thread-local partial tensor, and these are aggregated using a channel-wise max-reduction strategy to handle hash collisions. The aggregated tensor---a $128 \times 128 \times 3$ RGB image---represents the current file system state. Finally, the \textit{Inference Engine} executes a quantized Convolutional Autoencoder (CAE) on this tensor (\ding{185}). The CAE was trained on benign system states and produces a reconstructed tensor. Anomalies are detected by computing the $L_\infty$ norm (maximum absolute difference) between the input and reconstructed tensors. If this value exceeds a learned threshold $\tau$, an alert is raised (\ding{186}).

\subsection{High-Throughput Snapshot Engine}
\label{design_2}

The snapshot engine is the performance-critical component of \DeepVis, responsible for collecting metadata from potentially hundreds of thousands of system files within sub-second latency. We implement this component in Rust (approximately 350 lines) to achieve native performance with memory safety guarantees.

\noindent
\textbf{io\_uring Integration.} \DeepVis employs the \texttt{io-uring} crate to manage file system metadata collection asynchronously. By maintaining a ring buffer between user space and kernel space, we submit metadata requests in batches (default: 256 \texttt{statx} operations per batch). This avoids the overhead of context switching for every file. When the kernel completes a batch, the results are retrieved in a single system call, dramatically reducing per-file overhead. Our implementation uses the \texttt{rayon} crate for work-stealing parallelism across CPU cores, enabling linear scaling on multi-core systems.

\noindent
\textbf{Header-Only Entropy Estimation.} Calculating Shannon entropy typically requires reading the entire file contents, incurring $O(N \cdot \bar{S})$ I/O where $\bar{S}$ is the average file size. To minimize I/O overhead, \DeepVis reads only the first 64 bytes (the ``magic header'') of each file:
\begin{equation}
    S_{\text{est}}(f) = -\sum_{b=0}^{255} p_b \log_2 p_b \quad (\text{computed on first 64 bytes})
\end{equation}

\noindent
\textbf{Why Entropy Detects Rootkits.} Shannon entropy measures the \textit{randomness} of byte distributions: values range from 0 (all bytes identical) to 8 bits/byte (all byte values equally probable). Figure~\ref{fig:entropy_dist} illustrates how entropy differs across file types:

\begin{enumerate}
    \item \textbf{Text Files (Entropy $\approx$ 4.2):} ASCII printable characters (32--126) dominate, with common letters (e, t, a, o) appearing frequently. Only $\sim$70 of 256 possible byte values are used, creating a concentrated distribution.
    
    \item \textbf{ELF Binaries (Entropy $\approx$ 6.1):} Structured headers (magic number \texttt{0x7F ELF}), machine code with common opcodes (\texttt{0x48}, \texttt{0x89}), and null padding create a moderately diverse but still patterned distribution.
    
    \item \textbf{Encrypted/Packed Rootkits (Entropy $\approx$ 7.9):} Compression (UPX) or encryption (AES) transforms content into pseudo-random bytes. All 256 byte values appear with nearly equal probability---a uniform distribution approaching maximum entropy.
\end{enumerate}

Figure~\ref{fig:entropy_combined} in Section~\ref{Background} illustrates these entropy differences visually.

\noindent
\textbf{The Attacker's Paradox.} Attackers encrypt or pack malware to evade signature-based detection and hinder reverse engineering. Ironically, this transforms the payload into \textit{high-entropy content}---a statistical anomaly that \DeepVis exploits. Legitimate system files rarely exceed 6.5 bits/byte entropy; values above 7.5 strongly indicate packed or encrypted content.

We explicitly acknowledge this is an approximation. However, empirical validation confirms that packed binaries (e.g., UPX-compressed rootkits) exhibit high entropy in their headers due to the compression stub structure. The first 64 bytes typically contain ELF headers, shebang lines, or compression signatures---all of which are highly indicative of file type and packing status. This optimization reduces total I/O by over 99\% (from average 50KB per file to 64 bytes) while maintaining 97\% detection accuracy against our rootkit threat set.

\noindent
\textbf{Target Directory Scope.} \DeepVis monitors only \textit{system-critical directories} (/etc, /usr/bin, /usr/sbin, /usr/lib) rather than the entire file system. This is justified because: (1) rootkits target system binaries, libraries, and configuration files to achieve persistence; (2) user data integrity is a separate concern (ransomware detection); and (3) in immutable infrastructure deployments, user data resides on separate volumes. A typical Linux server contains 50,000--100,000 system files, which \DeepVis scans in under 1 second.

\subsection{Hash-Based Spatial Mapping}
\label{design_3}

To map the unordered set of files into a structured tensor suitable for CNNs, \DeepVis employs a deterministic spatial mapping function $\Phi$. This mapping transforms file paths into $(x, y)$ coordinates within a $W \times H$ image grid.

\noindent
\textbf{Cryptographic Coordinate Hashing.} We utilize SHA-256 truncated to 64 bits to compute coordinates for each file path $p$:
\begin{equation}
    \Phi(p) = \left( \text{SHA256}(p)_{[0:32]} \bmod W, \ \text{SHA256}(p)_{[32:64]} \bmod H \right)
\end{equation}
We selected SHA-256 over non-cryptographic hashes (e.g., xxHash, CityHash) to ensure \textbf{preimage resistance}. An attacker attempting to mask a rootkit by placing it at a specific pixel coordinate (to collide with a benign file) must find a path $p'$ such that $\Phi(p') = (x_{\text{target}}, y_{\text{target}})$. This requires approximately $2^{64}$ SHA-256 evaluations---computationally infeasible during a runtime attack. The performance overhead of SHA-256 (versus faster non-cryptographic hashes) is negligible because file paths are short (typically $<$256 bytes), contributing less than 1\% to total pipeline time.

\noindent
\textbf{Multi-Channel Feature Encoding.} Each file is encoded into three RGB channels representing distinct security-relevant features:
\begin{itemize}
    \item \textbf{Red (Entropy):} $\text{Encode}_R(f) = \min(S(f) / 8.0, 1.0)$ --- High-entropy files (packed binaries, encrypted payloads) appear bright red.
    \item \textbf{Green (Size):} $\text{Encode}_G(f) = \log(\text{size}(f)) / \log(\text{MaxSize})$ --- Abnormally large or small files are highlighted.
    \item \textbf{Blue (Permissions):} $\text{Encode}_B(f) = \text{mode}(f) / 0o777$ --- Files with unusual permissions (e.g., world-writable executables) stand out.
\end{itemize}
This encoding transforms abstract file metadata into a visual representation where anomalies manifest as unusual color patterns---bright red pixels indicate high-entropy packed binaries, while unusual blue values highlight permission anomalies.

\noindent
\textbf{Collision Handling via Max-Risk Pooling.} In hyperscale file systems where $N > W \times H$ (e.g., 100,000 files mapped to a $128 \times 128 = 16,384$ pixel grid), hash collisions are inevitable. Multiple files may map to the same pixel coordinate. \DeepVis resolves collisions by retaining the \textit{maximum} feature value per channel:
\begin{equation}
    T_{x,y}[c] = \max_{f \in \Phi^{-1}(x,y)} \text{Encode}_c(f)
\end{equation}
This \textit{max-risk pooling} strategy ensures that a high-risk signal (e.g., a high-entropy rootkit binary) is never suppressed by low-risk signals (e.g., text configuration files) sharing the same pixel. We prioritize recall over precision: the system alerts on the presence of \textit{any} anomaly within a pixel bucket. At 128$\times$128 resolution with 10,000 files, approximately 2.8\% of files experience collision---an acceptable trade-off given the max-pooling guarantee.

\subsection{Convolutional Autoencoder for Anomaly Detection}
\label{design_4}

The core detection logic relies on a Convolutional Autoencoder (CAE) trained to reconstruct the ``normal'' manifold of file system states. Anomalies---such as injected rootkit files---produce reconstruction errors that exceed a learned threshold.

\noindent
\textbf{Architecture.} The CAE follows a symmetric encoder-decoder structure:
\begin{itemize}
    \item \textbf{Encoder:} Conv(3$\to$32, k=3, s=2) $\to$ BatchNorm $\to$ ReLU $\to$ Conv(32$\to$64, k=3, s=2) $\to$ BatchNorm $\to$ ReLU $\to$ Conv(64$\to$128, k=3, s=2) $\to$ ReLU
    \item \textbf{Latent Space:} $16 \times 16 \times 128$ (32,768 features)
    \item \textbf{Decoder:} Symmetric transposed convolutions reconstructing to $128 \times 128 \times 3$
    \item \textbf{Total Parameters:} 135,331 (0.52 MB FP32, 0.13 MB INT8)
\end{itemize}
The latent space dimensionality (32K features) is chosen to be significantly smaller than the input (49K pixels $\times$ 3 channels), forcing the autoencoder to learn a compressed representation of normal system states.

\noindent
\textbf{Training Regime.} The CAE is trained on augmented snapshots of a known-clean baseline system:
\begin{itemize}
    \item \textbf{Optimizer:} Adam (lr=$10^{-3}$, $\beta_1$=0.9, $\beta_2$=0.999)
    \item \textbf{Loss:} Mean Squared Error (MSE) for training
    \item \textbf{Epochs:} 50 with early stopping ($\Delta$loss $< 10^{-4}$)
    \item \textbf{Augmentation:} Each training epoch applies 10--40\% random file modifications (size $\pm$30\%, entropy $\pm$0.3) to simulate benign system churn
\end{itemize}
This aggressive augmentation teaches the model to tolerate legitimate system updates (e.g., package upgrades, log rotations) while remaining sensitive to anomalous injections.

\noindent
\textbf{Threshold Selection ($\tau$).} The detection threshold is set to the 99.9th percentile of $L_\infty$ reconstruction errors on the training set:
\begin{equation}
    \tau = \text{Percentile}_{99.9}\left( \{ \|T_i - \hat{T}_i\|_\infty : T_i \in \mathcal{D}_{\text{train}} \} \right)
\end{equation}
This yields $\tau \approx 0.632$ for our Ubuntu-trained model. The high percentile (99.9th rather than 99th) ensures robustness against benign churn while maintaining sensitivity to true anomalies.

\subsection{Spatial Anomaly Isolation ($L_\infty$)}
\label{design_5}

\noindent
\textbf{The MSE Paradox.} Standard autoencoder anomaly detection uses Mean Squared Error (MSE), which averages reconstruction errors across all pixels:
\begin{equation}
    L_2 = \frac{1}{W \cdot H \cdot 3} \sum_{x,y,c} (T[x,y,c] - \hat{T}[x,y,c])^2
\end{equation}
This approach fails in the presence of benign system churn. During package upgrades (\texttt{apt upgrade}), thousands of files change slightly, creating high aggregate $L_2$ noise. A sparse attack (e.g., a single rootkit binary) produces a small localized deviation that is \textit{buried} in this diffuse noise. We observed that MSE-based detection produces 15--30\% FPR during routine system updates.

\noindent
\textbf{$L_\infty$ Isolation.} Instead of averaging, \DeepVis uses the $L_\infty$ norm---the maximum absolute difference across all pixels:
\begin{equation}
    L_\infty = \max_{x,y,c} |T[x,y,c] - \hat{T}[x,y,c]|
\end{equation}
This isolates the single most anomalous pixel regardless of background noise. A rootkit injection produces a sharp spike in reconstruction error at its mapped pixel, which $L_\infty$ captures even when thousands of other files have changed slightly. Our experiments show that $L_\infty$ maintains 0\% FPR under 50\% file churn while detecting all tested rootkits.

\noindent
\textbf{Known Limitation.} $L_\infty$ may miss ``low-and-slow'' attacks that modify many files with small changes (Living-off-the-Land techniques). We discuss mitigation strategies in Section~\ref{Discussion}.

\subsection{Implementation}
\label{design_impl}

We implemented \DeepVis as a hybrid Rust/Python system, totaling approximately 2,500 lines of code across two main components.

\noindent
\textbf{1) Rust Snapshot Engine (350 lines).} The core metadata scanner is implemented in Rust for native performance. It uses: (a) the \texttt{io-uring} crate (v0.7) for asynchronous system calls; (b) the \texttt{rayon} crate for data-parallel entropy computation across CPU cores; (c) the \texttt{sha2} crate for SHA-256 coordinate hashing; and (d) the \texttt{pyo3} crate for zero-copy Python bindings. The scanner exposes a Python-callable \texttt{DeepVisScanner} class that returns structured metadata including file paths, sizes, permissions, entropy estimates, and precomputed hash coordinates.

\noindent
\textbf{2) Python Inference Engine (2,100 lines).} The tensor generation, CAE training, and anomaly detection logic are implemented in Python using PyTorch. We use \texttt{torch.quantization} for INT8 dynamic quantization, reducing model size from 2.1 MB to 0.52 MB and accelerating inference by approximately 3$\times$ on CPU. The engine includes a \texttt{DockerDatasetLoader} component that pulls and scans real OS images (Ubuntu, CentOS, Debian) directly from Docker Hub, enabling realistic evaluation on diverse file system structures.

\noindent
\textbf{3) Deployment.} The complete system is packaged as a Docker container (base image: \texttt{python:3.10-slim}) with the precompiled Rust scanner library. For Kubernetes deployment, \DeepVis runs as a DaemonSet sidecar with resource limits: 0.5 vCPU, 128 MB RAM. The container mounts the host's \texttt{/etc}, \texttt{/usr/bin}, \texttt{/usr/sbin}, and \texttt{/usr/lib} directories as read-only volumes for scanning.
