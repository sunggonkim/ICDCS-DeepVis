\section{Evaluation}~\label{Evaluation}

\subsection{Experimental setup}
%for the evaluation, we use Perlmutter supercomputer at NERSC
%what cluster: Perlmutter supercomputer at NERSC
%cpu or gpu:
\changjong{
\textbf{KCJ: First draft}

We evaluated ScaleMon using a diverse set of I/O Darshan logs collected from the Leadership-scale HPC Systems. To test its generalizability across different workloads, our dataset includes both I/O benchmarks (h5bench\_read, h5bench\_write) and real scientific applications (LAMMPS, PyTorch with AMREX). We generated these logs by running the applications across multiple node counts (1, 4, 8) and process counts (16 to 512), as well as different I/O modes (Independent, Collective).

Our source dataset consists of normal (Benign) execution logs. For the training of our proposed scheme, ScaleMon, this benign data was used as input to our Synthetic Anomaly Generator (SAG) to create a balanced, two-class training dataset composed of both normal samples and synthetically generated anomalous samples. In contrast, the one-class baseline models were trained solely on the normal (Benign) execution logs.

To measure detection accuracy, we use recall and precision. Recall represents the ratio of detected anomalies to the total number of actual anomalies, while precision represents the ratio of true anomalies to all samples detected as anomalies.}

\changjong{}
\subsection{Stage 1}
\taebin{need to connect anomalous behavior and real attack more explicitly}

The Stage 1 detector was evaluated using real user execution logs collected from the Korea Institute of Science and Technology (KIST) in October 2023. We conducted experiments on two representative HPC applications, VASP and GROMACS.
The VASP dataset consists of 1,677 executions performed by 16 users, during which a total of 23,517 files were accessed. The GROMACS dataset contains 647 executions from 6 users, involving 1,986 accessed files.

Anomalous data, which are the target of detection, were generated by manipulating the above normal execution logs. Three types of manipulations were applied:
(1) changing the file type to an unseen type that did not appear in the training data, and
(2) changing the operation of a normally \textt{read\_only} file to \textt{write\_only} or \textt{read\_and\_write},
(3) adding access to a file located far from other accessed files within the same execution (distance 6 to 10, with benign file type and operation)
Both single-manipulation anomalies and combined-manipulation anomalies were considered. Specifically, combining manipulations (1) and (3) corresponds to anomalies with unusual location and anomalous file type, while (2) and (3) corresponds to anomalies with unusual location and anomalous operation type.

A representative set of one-class anomaly detection models — including Isolation Forest, Local Outlier Factor (LoF), K-Means++, One-Class SVM, and Autoencoder — was tested, and their detection performance is summarized below.

\begin{table}[ht]
\centering
\caption{Anomaly Detection Performance on VASP and GROMACS Datasets}
\label{tab:anomaly_perf}
\begin{tabular}{lcc|cc}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{VASP} & \multicolumn{2}{c}{GROMACS} \\
 & Recall & FPR & Recall & FPR \\
\hline
KMeans++           & 1.000 & 0.046 & 0.997 & 0.053 \\
IsolationForest  & 0.510 & 0.026 & 0.210 & 0.048 \\
OneClassSVM      & 0.504 & 0.009 & 0.692 & 0.050 \\
LOF              & 0.999 & 0.006 & 0.997 & 0.066 \\
Autoencoder      & 1.000 & 0.009 & 1.000 & 0.000 \\
\hline
\end{tabular}
\end{table}

K-Means++, LoF, and Autoencoder achieved superior detection performance compared to boundary-based models such as Isolation Forest and One-Class SVM. This suggests that the proposed feature space exhibits a multi-modal or manifold structure rather than a single homogeneous cluster. Such characteristics are reasonable in this context, as multiple file types and their corresponding expected behaviors coexist within HPC applications, resulting in diverse normal behavioral clusters.

Overall, K-Means++, LoF, and Autoencoder demonstrated satisfactory detection capability, confirming that the proposed feature vector effectively captures essential I/O characteristics relevant to anomaly detection. Among the evaluated models, the Autoencoder achieved the highest accuracy, detecting all anomalies in both applications with a negligible false positive rate of 0.009 in GROMACS, while K-Means++ and LoF also demonstrated competitive performance. In scenarios where sufficient training data for the Autoencoder are unavailable, K-Means++ and LoF can serve as effective alternatives. The inference overhead per sample was negligible (K-Means++: 0.714 \(\mu s\), LoF: 78.672 \(\mu s\), Autoencoder: 0.226 \(\mu s\)), demonstrating the practicality of these methods for real-time analysis. 


% Detection Rate (Recall) + False Positive Rate (FPR)
% kmeans++
% IsolationForest
% OneClassSVM
% Autoencoder
% lof
% one-class svm

% result (table) 
\subsection{Stage 2}
\subsubsection{Dataset}

Our dataset consists of log data from benchmarks and real scientific applications, including h5bench, LAMMPS, and a PyTorch/AMReX example. These were executed in the Perlmutter environment across multiple node (1-8) and process counts (16-512) to generate a diverse set of benign execution logs. To create challenging malicious samples that mimic benign behavior, we tampered with these applications by injecting malicious actions designed to go unnoticed by conventional monitoring. Our methodology involved simulating the I/O footprints of canonical threat categories, structured according to the Confidentiality, Integrity, and Availability (CIA) triad—fundamental security goals for any critical system. 

\noindent\textbf{Confidentiality Attack: Data Breach.}
To simulate threats against data confidentiality, we injected malicious code designed to mimic the data collection phase of an exfiltration attack. A common attacker goal in HPC is to steal valuable intellectual property, such as proprietary datasets or final simulation results. Our injected code performs sequential read operations on target files after the primary computation is complete. This simulates an attacker reading and preparing targeted data to be exfiltrated from the system, leaving a distinct, abnormal read pattern at the I/O level.

\noindent\textbf{Integrity Attack: Data Tampering.}
To represent threats against data integrity, we injected code that performs malicious data tampering. An attack compromising data integrity can have devastating consequences in HPC, invalidating results produced through extensive computational time and resources, and potentially leading to incorrect scientific conclusions. In our simulation, the malicious code executes sequential write operations over the application's output files, overwriting the original content. This behavior effectively corrupts or destroys the original data, simulating an attack designed to compromise the integrity of scientific results.

\noindent\textbf{Availability Attack: Resource Exploitation.}
To model threats against system availability, we simulated resource exploitation. In this scenario, an attacker misuses expensive HPC resources for unauthorized computations, such as cryptocurrency mining, thereby denying legitimate users access to those resources. In our simulation, the I/O-intensive benign application pauses its own job while malicious code is executed. The tampered application still produces correct scientific outputs to appear legitimate, but its execution time is prolonged, and resources continuously leak with each execution.

We created malicious samples using two realistic vectors: (1) direct application tampering, injecting malicious logic into an application's source code, and (2) a supply-chain attack, linking applications against a tampered third-party library (HDF5). The latter is especially dangerous since one compromised library can affect many applications across an HPC center. Table X summarizes the number of benign and malicious samples used for training and testing.

% Our dataset consists of log data from some benchmarks and real scientific applications.

% Specifically: h5bench\_read and h5bench\_write, which mimic VPIC’s I/O behavior (VPIC is a well-known general-purpose particle-in-cell simulation); ReaxFF simulations using LAMMPS (a popular molecular dynamics simulation); and a PyTorch machine-learning example using the AMReX library. We ran the above applications in the Perlmutter environment on multiple nodes (1, 4, 8) and with multiple process counts (16, 32, 64, 128, 256, 512). For h5bench, we also ran different I/O modes (Independent, Collective).

% We tampered with the above applications to create malicious samples; there are three kinds of threats in the test data.

% \noindent\textbf{Resource exploitation:} %Availability
% Malicious code that exploits system resources was injected into each normal application. The tampered application performs additional hidden tasks while executing its normal job to appear legitimate, consuming extra resources in the process. The execution time of hidden tasks in our test data varied from 0.5× to 2.5× the normal execution time.

% \noindent\textbf{Ransomware:} For this case, malicious file-encrypting code was injected. Victim files can be either the files the tampered application processes or other files accessible to the user. In the first case, after the normal job finishes and the experiment result is complete, the tampered application reads its result sequentially, encrypts it, and overwrites it so the user cannot obtain the result. An example access pattern is visualized in Figure 5 B. In the second case, the tampered application accesses and encrypts other files during runtime. Both actions may occur simultaneously. %Integrity -> code, data tampering

% \noindent\textbf{Data exfiltration:} In this case, data-exfiltration code is injected into a normal application. Similar to ransomware, the tampered application can exfiltrate its own result data or read and exfiltrate other accessible files. In the first case, when the experiment is finished and the result is ready, the tampered application sequentially reads and transmits the result. In the second case, the tampered application accesses other user-accessible files during runtime and exfiltrates them. The latter can breach more data but may be more easily detected when multiple files are accessed; the former can therefore still be attractive to attackers.%Confidentiality -> code, data tampering

% manipulating excutions behaviour like we does cna be done by attacker too, and it can be done by either directly tampering with the application code or linking a malicously tampered library  to the application-i.e., a supply-chain attack. we made malisous data by both way tampering application code or linking malicously tampered hdf5 library. total number of data used for train and test is summarized in Table x



% We implemented these behaviors via two attack roots. First, we directly tampered with the application code. Second, we linked a maliciously tampered library (specifically an HDF5 library) to the application—i.e., a supply-chain attack. The second approach can affect a broader set of applications if successful and is thus potentially more harmful. Table 4 summarizes the types and numbers of training and test samples used.
%there is too many table already. so, i think it would be better to explain in text about the number of train and test dataset




\subsubsection{Baseline Models}
Since it is difficult to obtain sufficient hacked execution logs for training in a typical HPC cluster, methods that rely solely on normal data can be considered more practical. Therefore, several one-class anomaly detection models, trained only on normal data, are evaluated against our method. Specifically, we evaluate centroid-based, autoencoder-based, Isolation Forest-based, and clustering-based models. The image format proposed and used in our method is also used as the input for these baseline models.

\noindent\textbf{Centroid-based model:} During the training phase, a centroid image for each application is created by averaging the pixel values of all normal training data of each application. The mean squared error (MSE) is then calculated for each training image, and the 95th percentile is set as the threshold, resulting in a false positive rate (alpha) of 0.05 on the training set. During the detection phase, any image with an MSE greater than the threshold is considered anomalous.

\noindent\textbf{Autoencoder-based model:} During the training phase, a CNN-autoencoder is trained for each application to minimize the reconstruction error of normal images. The encoder compresses each image into a low-dimensional latent vector, from which the decoder reconstructs the original image. Some information is inevitably lost during this process, resulting in distortion in the reconstructed image. The reconstruction error, measured here as the mean squared error (MSE), tends to increase as the input image becomes more different from the training images. During detection, images with reconstruction errors exceeding the threshold—determined in the same manner as in the centroid-based model—are considered anomalous.

\noindent\textbf{Isolation Forest-based model:} Each input image is first transformed into a one-dimensional vector using the procedure described for the Isolation Forest model. This is achieved by passing the image through a pre-trained ResNet-50 model (trained on ImageNet) to extract a latent feature vector. During the training phase, an ensemble of isolation trees is constructed to isolate each training sample. In the detection phase, the same tree ensemble is used to score new images: images that are isolated at shallow depths in the trees are considered more likely to be anomalous. The anomaly score and threshold provided by the Isolation Forest algorithm are used to determine whether an image is anomalous.

\noindent\textbf{Clustering-based model:} Each input image is transformed into a one-dimensional latent vector using exactly the same procedure as in the Isolation Forest model. During the training phase, a fixed number k of centroids are computed in the latent space using the k-means++ algorithm. In the detection phase, the distance between a sample and each centroid is calculated, and the distance to the nearest centroid is used as the anomaly score. If this score exceeds the threshold, the sample is considered anomalous. The threshold is determined in the same manner as in the centroid-based model.


\noindent\subsection{Detection Performance}
In this experiment, each application has its own model, and each test sample is evaluated by the model corresponding to its application. The detection results of each baseline model and our Stage 2 detector for each application are summarized in Table X. In the table, recall represents the ratio of detected anomalies to the total number of actual anomalies, while precision represents the ratio of true anomalies to all samples detected as anomalies. In other words, a higher recall indicates that the model misses fewer anomalies, and a higher precision means that the model raises fewer false alarms. 

As shown in Table X, our model outperforms all baseline models in every case in both recall and precision. This result demonstrates that our approach, which injects heuristic guidance via SAG during the training phase, enables the model to better learn the distinction between normal and anomalous behaviors. As a result, the model can effectively discriminate between them even when anomalies closely mimic normal behavior. 

There may be differences in results when the model is tested on real attack data, as real attack patterns can differ from those in our test dataset. However, SGA can generate a wide variety of anomalous patterns by combining basic perturbations. Therefore, when real attack data emerge, the model may have already encountered similar patterns during training. Even for unseen anomalous patterns that were not observed during training, our experiments show that the detector performs reasonably well. Details of this analysis are discussed in Section 5.5.



\subsubsection{Modeling Scope}
The scope covered by each model can be configured. A model can be responsible for analyzing the logs of a specific application or the entire set of logs in a cluster. Detection performance may vary depending on the chosen scope. To evaluate the effect of modeling scope, we consider three cases:

\noindent\textbf{Case 1:} The modeling scope is at the application level, so each model covers only the execution logs of its corresponding application. This case is the same as in Section 5.5.

\noindent\textbf{Case 2:} The model covers all applications in our dataset. Of course, with only four applications in our dataset, we cannot evaluate whether a single model can cover all or most logs in an HPC cluster. However, by comparing Case 2 with Case 1, we can observe the effect of broadening the modeling scope on detection performance.

\noindent\textbf{Case 3:} The model is trained on all applications except the one being tested. This case evaluates whether our Stage 2 detector can generalize to previously unseen applications that were not included during training.

Table X shows the results for each case.

\subsection{Perturbation Function}
The detection performance of the Stage 2 detector may vary depending on the perturbation functions used in the SAG module. To evaluate this effect, we consider three cases:

\noindent\textbf{Case 1:} All perturbation functions are used. This case is the same as Case 1 in Sections 5.5 and 5.6.

\noindent\textbf{Case 2:} All perturbation functions are used except the function that covers the I/O patterns of the tested attack scenario. This case allows us to evaluate detection performance on unseen anomalous patterns.

\noindent\textbf{Case 3:} All perturbation functions are used except for general augmentation functions. The perturbation functions include rotate, reverse, and CutMix, which are general augmentation techniques and are not plausible in real attack patterns. These functions are deployed to broaden the coverage of anomalous patterns generated by SAG. By comparing this case with Case 1, we aim to empirically assess the necessity of these general augmentations.

Table X shows the results for each case.

\subsubsection{Overhead \& Data Compression}

The overhead of deploying \ScaleMon during the detection phase can be divided into two components: data preprocessing and decision making. Model training overhead can vary greatly depending on the size of the training data and the training environment, and it is performed offline. Therefore, in this section, we focus only on the detection-phase overhead, not the training-phase overhead.

\noindent\textbf{Data Preprocessing:} Given an input DXT log of 1 GB in binary format, the conversion to TXT using the Darshan DXT parser takes approximately 1 ms. The subsequent conversion from TXT to CSV takes about 2 ms, followed by transformation into two additional formats: a dictionary file (approximately 0.1 ms) and image files, whose generation time varies depending on the compression settings.

\noindent\textbf{Decision Making:} In \ScaleMon, the decision-making process in Stage 1 is performed almost instantly, and its overhead is therefore negligible. The primary overhead arises from Stage 2, where the decision-making overhead depends on factors such as data compression configuration, hardware accelerators, and other implementation details. However, in this experiment, we focus solely on the impact of data configuration, keeping all other configurations fixed.

The compression configuration in our system is defined by two parameters: image size and data type (dtype). These parameters determine how small the resulting image data becomes, which serves as the input to the Stage 2 detector. Changing them affects detection performance and overhead. We evaluated image sizes of 64, 128, 256, and 512, and data types of float16, float32, and float64, resulting in 12 combinations of (image size, data type) pairs. Table X summarizes the detection performance, image generation time, decision-making time, and GPU memory usage for each configuration.


\begin{table*}[!t]
\footnotesize
\centering
\caption{Comparison of the detection performance of different models for each application and attack type.}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{\textwidth}{l | c | X X X | X X X | X X X | X X X}
\toprule
\multirow{2}{*}{Model} & 
\multirow{2}{*}{Metric} &
\multicolumn{3}{c|}{h5bench\_read} & 
\multicolumn{3}{c|}{h5bench\_write} & 
\multicolumn{3}{c|}{LAMMPS\_ReaxFF} & 
\multicolumn{3}{c}{AMReX\_FFT} \\ 
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
 & & C & I & A
 & C & I & A 
 & C & I & A 
 & C & I & A \\
\midrule

\multirow{3}{*}{Centroid} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{Autoencoder} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{Isolation Forest} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{K-Means} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{ScaleMon} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\bottomrule
\end{tabularx}%
}
\end{table*}

\begin{table*}[!t]
\footnotesize
\centering
\caption{Comparison of detection performance according to the modeling scope for each application and attack type.}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{\textwidth}{l | c | X X X | X X X | X X X | X X X}
\toprule
\multirow{2}{*}{Case} & 
\multirow{2}{*}{Metric} &
\multicolumn{3}{c|}{h5bench\_read} & 
\multicolumn{3}{c|}{h5bench\_write} & 
\multicolumn{3}{c|}{LAMMPS\_ReaxFF} & 
\multicolumn{3}{c}{AMReX\_FFT} \\ 
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
 & & C & I & A
 & C & I & A 
 & C & I & A 
 & C & I & A \\
\midrule

\multirow{3}{*}{case 1} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{case 2} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{case 3} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\bottomrule
\end{tabularx}%
}
\end{table*}

\begin{table*}[!t]
\footnotesize
\centering
\caption{Comparison of detection performance according to the composition of SAG's perturbation functions for each application and attack type.}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{\textwidth}{l | c | X X X | X X X | X X X | X X X}
\toprule
\multirow{2}{*}{Case} & 
\multirow{2}{*}{Metric} &
\multicolumn{3}{c|}{h5bench\_read} & 
\multicolumn{3}{c|}{h5bench\_write} & 
\multicolumn{3}{c|}{LAMMPS\_ReaxFF} & 
\multicolumn{3}{c}{AMReX\_FFT} \\ 
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
 & & C & I & A
 & C & I & A 
 & C & I & A 
 & C & I & A \\
\midrule

\multirow{3}{*}{case 1} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{case 2} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{case 3} 
 & Precision  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Recall     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & Accuracy   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\bottomrule
\end{tabularx}%
}
\end{table*}


\begin{table}[!t]
\footnotesize
\centering
\caption{Comparison of detection performance and overhead according to compression configuration}
\label{table:performance_overhead}
\begin{tabularx}{\linewidth}{p{1.2cm}|c|X|X|X|X}
\toprule
\textbf{Image Size} & \textbf{Data Type} & \textbf{Detection Accuracy} & \textbf{Image Generation Time} & \textbf{Decision-making Time} & \textbf{GPU Memory Usage} \\
\midrule

\multirow{3}{*}{64} 
 & float16 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & float32 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & float64 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{128} 
 & float16 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & float32 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & float64 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{256} 
 & float16 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & float32 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & float64 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule

\multirow{3}{*}{512} 
 & float16 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & float32 & 0.0 & 0.0 & 0.0 & 0.0 \\
 & float64 & 0.0 & 0.0 & 0.0 & 0.0 \\
\bottomrule
\end{tabularx}
\end{table}

