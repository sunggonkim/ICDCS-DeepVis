\section{Introduction}~\label{Introduction}
\taebin{- citation should be added in introduction.}
\taebin{usage of slurm log: identify log, concept drift?}
% IDS(malware classification) can be applied to HPC (cluster, grid, cloud)
%  Behavior Monitoring for security
 
% mpi - whalen2013multiclass %need to find more dominant one

% {system log(syslog maybe) - deeplog
% @inproceedings{du2017deeplog,
%   title={Deeplog: Anomaly detection and diagnosis from system logs through deep learning},
%   author={Du, Min and Li, Feifei and Zheng, Guineng and Srikumar, Vivek},
%   booktitle={Proceedings of the 2017 ACM SIGSAC conference on computer and communications security},
%   pages={1285--1298},
%   year={2017}
% }

% audit log (provenance graph) - KAIROS: Practical Intrusion Detection and Investigation
% using Whole-system Provenance?}

% power - copos2020catch %need to find more dominant one

% I/O - wang2014abnormal %does not target for HPC, but can be apply?

% there was IDS works for distribted system. detctinon 

% % C: data exfiltration, unathoralized access
% % I: alteration of code or data (data poisoning)
% % A : ransomware, misuse of computing cycles, disruptions or denial-of-service

% % -> I/O behaviour change

% IDS in HPC

% % Detection Source:
% % network based (NIDS)
% % host based (HIDS)

% Detection Methodology:
% policy based
% anomaly based

% detection granularity(targeted entity):
% System-level / Cluster-level, Node-level / Host-level, User-level / Session-level, Execution-level / Job-level, Process-level / Container-level, Behavior-level / Function-level

% Observation Data Source
% System logs, Resource usage metrics, I/O traces, MPI communication pattern, Scheduler / Job metadata, Network traffic


High-performance computing (HPC) has become indispensable for modern scientific breakthroughs as applications grow increasingly complex~\cite{jumper2021highly, van2005gromacs, zhang2019amrex}.  
These systems integrate thousands of processing units (e.g., CPU~\cite{bhargava2024amd} and GPU~\cite{choquette2021nvidia}) and high-throughput storage devices (e.g., SSD~\cite{guz2017nvme} and HDD~\cite{islam2015triple}), interconnected via high-speed fabrics such as Slingshot~\cite{de2020depth}, to achieve extreme scalability and computational power.


However, the performance-oriented nature of HPC environments introduces unique security challenges~\cite{peisert2017security, cappello2015improving, guo2023high}. Because performance often outweighs security considerations, rapid data exchange and open collaboration can create weak points in system defenses. As a result, HPC systems face threats such as data leakage, integrity violations (e.g., code or data modification), and availability attacks like cycle misuse or enial-of-service~\cite{peisert2017security}. Moreover, the growing heterogeneity of hardware and software components further expands the attack surface—including the adoption of different computation units, such as CPUs and GPUs, from various vendors that support diverse software stacks~\cite{burford2021ookami, herten2023many, elwasif2023application, sinha2022not, milutinovic2021ultimate}. These factors demand systematic approaches to safeguard these mission-critical systems.



\begin{figure}[t]
    \centering
    \includegraphics[width=8.5cm]{Figures/kisti_runtime.png}
    \vspace{-0.6cm}
    \caption{Total runtime per application in a HPC cluster}
    \label{mariadb}
    \vspace{-.6cm}
    
\end{figure}



\begin{table}[!t]
\caption{Comparison of Modern Intrusion Detection Systems.}
\label{tab:ids_landscape}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Method}} &
\multirow{2}{*}{\textbf{Data Source}} &
\multirow{2}{*}{\makecell{\textbf{HPC-}\\\textbf{Applicable}}} &
\multicolumn{2}{c}{\textbf{I/O Monitoring}} \\
\cmidrule(lr){4-5}
& & & \textbf{Inter-File} & \textbf{Intra-File} \\
\midrule
\multicolumn{5}{l}{\textit{\textbf{NIDS}}} \\
Bro~\cite{paxson1999bro}                 & Network Packets & $\triangle$ & & \\
Cerberus~\cite{zhou2024cerberus}         & Network Packets & $\triangle$ & & \\
\midrule
\multicolumn{5}{l}{\textit{\textbf{HIDS}}} \\
DeepLog~\cite{du2017deeplog}             & System Event Logs & $\triangle$ & $\triangle$ & \\
Kairos~\cite{cheng2024kairos}            & Audit Logs & & \checkmark & \\
Flash~\cite{rehman2024flash}             & Audit Logs & & \checkmark & \\
MAGIC~\cite{jia2024magic}                & Audit Logs & & \checkmark & \\
ORTHRUS~\cite{jiang2025orthrus}           & Audit Logs & & \checkmark & \\
VELOX~\cite{bilot2025sometimes}           & Audit Logs & & \checkmark & \\
\midrule
\textbf{ScaleMon (Ours)}                 & \textbf{HPC I/O Logs} & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
}
\end{table}



\skim{However, these software and hardware hetrogenity, HPC systems have two distinct characteristics that can be utilized for security }

\noindent\textbf{Same applications executed multiple times}
In most HPC clusters, a relatively small and fixed set of applications is executed repeatedly, resulting in much lower diversity compared with general-purpose computing systems. Figure 1 shows the top 7 application runtimes in a production HPC cluster from July 2023 to June 2024. The results indicate that the top application, VASP, accounts for over 70\% of the total execution time, while the top 10 applications together account for more than 97\%. This regularity indicates that HPC workloads are highly concentrated, providing a unique opportunity to enhance security by leveraging machine learning (ML). ML-based approaches have demonstrated strong performance in pattern recognition and have been widely explored for attack detection. However, despite their potential, deploying ML-based cybersecurity systems in real-world environments remains challenging due to the high diversity and variability of benign behavior. In contrast, the repetitive and well-defined workload patterns in HPC systems can help mitigate this challenge, making them an ideal environment for ML-driven anomaly detection.


\noindent\textbf{Feature-rich low-overhead logging systems}
Most HPC clusters are equipped with a variety of logging tools that continuously monitor system behavior in real time. These tools are designed to impose minimal overhead due to the performance-centric nature of HPC environments. For instance, resource- and hardware-level metrics such as CPU/GPU utilization, memory usage, and network traffic can be monitored, alongside Slurm job scheduler logs and Darshan I/O profiling tools, which track various aspects of job execution and application I/O behavior. Consequently, HPC systems generate a rich set of logs, from which valuable information can be extracted. This data can be used not only for performance optimization and identifying system bottlenecks, but also for security monitoring and anomaly detection.

Given the richness of HPC log data, security monitoring should be conducted from multiple perspectives. This is necessary because certain attacks may leave little or no trace from a single viewpoint, either due to the attack’s inherent nature or the attacker’s efforts to conceal their activities. Among the various perspectives, I/O behavior provides particularly informative signals. Many attacks in HPC environments eventually leave traces at the I/O level. Attackers often require I/O operations to accomplish their malicious goals, and even when they do not, attacks can indirectly alter I/O behavior. Even if an attacker attempts to appear benign by renaming binaries or mimicking normal behavior, their malicious intent will eventually produce abnormal I/O patterns when the real payload is executed. Such deviations can be revealed through careful I/O observation. Therefore, monitoring I/O activity is a valuable approach for enhancing security.

While numerous Intrusion Detection Systems (IDS) exist, as summarized in Table~\ref{tab:ids_landscape}, they fall short in addressing the specific constraints of HPC I/O monitoring.
Network-based Intrusion Detection Systems (NIDS) like Bro(Zeek)~\cite{paxson1999bro} and Cerberus~\cite{zhou2024cerberus} are restricted to monitoring network boundaries and cannot inspect the I/O behavior of an executions. 
Host-based Intrusion Detection Systems (HIDS) offer deeper analysis. However, approaches like DeepLog~\cite{du2017deeplog}, which analyze general system event sequences, can only indirectly infer inter-file relationships and do not capture any intra-file details. 
More recent provenance-based IDS~\cite{cheng2024kairos, rehman2024flash, jia2024magic} successfully model inter-file I/O by analyzing graphs from kernel audit logs. 
Nevertheless, they face two fundamental limitations in the HPC context: first, the significant performance overhead of their required logging tools (e.g., auditd, Falco, CamFlow) makes them impractical for production systems~\cite{her2025depth, dai2017lightweight}; second, they still lack visibility into the intra-file level, such as the specific offsets and sizes of I/O operations. 
In contrast, \ScaleMon distinguishes itself from these previous studies by providing deep, multi-level I/O monitoring at both the inter-file and intra-file levels, while being specifically designed to meet the stringent performance demands of production HPC systems.


In this paper, we present \ScaleMon, a scalable monitoring framework that introduces a hierarchical inspection methodology to detect a wide spectrum of attacks in HPC systems. Our work is driven by three primary goals: 1) Comprehensive Threat Coverage 2) Exascale Scalability 3) Zero-Day Readiness To achieve these goals, \ScaleMon 1) employs a hierarchical architecture consisting of an \IdentityVerifier, \InterMon, and \IntraMon, with each module focusing on a different scale of I/O behavior; 2) utilizes lightweight data representations derived from a lightweight Darshan-based profiling pipeline, including execution profile vectors and I/O Images, to drastically compress raw I/O traces while preserving the spatial-temporal access patterns necessary for detection; and 3) builds upon one-class anomaly detection models trained exclusively on benign data. Our evaluation on widely used scientific applications on a production HPC system demonstrates the effectiveness and practicality of this approach. \ScaleMon successfully detects all diverse range of attacks, ranging from blatant masquerading to stealthy data tampering, while inducing minimal overhead acceptable for production environments (e.g., less than 0.0001\% of application runtime). As our models are trained solely on benign data, these high-performance results also validate \ScaleMon’s Zero-Day Readiness. We have open-sourced the code for \ScaleMon at \url{https://github.com/ScaleMon/ScaleMon.git} and the dataset at \url{https://doi.org/10.5281/zenodo.XXXXXX}.



% ScaleMon distinguishes itself from these previous studies by introducing a scalable and practical HIDS framework specifically designed for leadership-scale HPC systems. Whereas existing provenance-based HIDS~\cite{cheng2024kairos, rehman2024flash} construct and analyze massive, computationally expensive graphs, ScaleMon introduces a novel I/O-to-image transformation that converts voluminous, fine-grained I/O logs into compact, fixed-size 2D image tensors. This approach preserves critical spatio-temporal access patterns (offset, time, size) while drastically reducing data volume and computational complexity, directly addressing the scalability and practicality challenges. Furthermore, instead of relying on scarce real-world attack data, ScaleMon employs a Synthetic Anomaly Generator (SAG). This generator systematically perturbs benign log images to create a diverse and balanced two-class training dataset, enabling a robust model that learns the subtle boundary between normal and malicious I/O behaviors. Our design avoids the overhead of real-time graph construction and the limitations of one-class models, enabling scalable, high-fidelity attack detection tailored to the unique I/O characteristics of HPC workloads.

% In this paper, we present \ScaleMon, a scalable monitoring framework that introduces a hierarchical inspection methodology to detect a wide spectrum of attacks in HPC systems. Our work is driven by three primary goals: 1) Comprehensive Threat Coverage, by capturing hard-to-conceal I/O behaviors at both inter-file and intra-file levels; 2) Exascale Scalability, by processing voluminous, fine-grained logs with low overhead; and 3) Zero-Day Readiness, by training exclusively on benign data to remain independent of attack signatures. To achieve these goals, \ScaleMon 1) employs a hierarchical architecture consisting of an \IdentityVerifier, \InterMon, and \IntraMon, with each module focusing on a different scale of I/O behavior; 2) utilizes lightweight data representations, including execution profile vectors and the I/O Image, to drastically compress raw data while preserving the spatial-temporal access patterns necessary for detection; and 3) builds upon one-class anomaly detection models trained exclusively on benign data. Our evaluation on widely-used scientific applications on a production HPC system demonstrates the effectiveness and practicality of this approach. \ScaleMon successfully detects a diverse range of attacks, from blatant masquerading to stealthy data tampering, with a low XX.X\% False Positive Rate (FPR), while inducing a minimal overhead acceptable for production environments (e.g., less than 0.0001\% of the application's runtime). As our models are trained only on benign data, these high-performance results also validate \ScaleMon's capability to detect threats that are, from the model's perspective, previously unseen. We have open-sourced the code for \ScaleMon at the following link: \url{https://github.com/ScaleMon/ScaleMon.git}.

% In this paper, we present ScaleMon, a scalable system and application monitoring framework for attack detection in secure HPC systems. Our work integrates three key techniques to achieve high-fidelity detection with low overhead. The goal of ScaleMon is to 1) efficiently represent voluminous, fine-grained I/O logs in a compact and analyzable format, 2) enable robust detection that overcomes the limitations of one-class models which fail against subtle attacks, and 3) provide a practical multi-stage pipeline that balances detection granularity with efficiency. To achieve these goals, ScaleMon 1) introduces a novel I/O trace-to-image transformation that converts raw DXT logs into fixed-size 3D image tensors. This representation preserves critical I/O dimensions (offset, time, size, and request type) while drastically reducing data volume by up to XXX, 2) employs a Synthetic Anomaly Generator (SAG) to systematically perturb benign log images. This creates a diverse, balanced dataset that enables a two-class model to learn the subtle boundary between normal and malicious behavior, a significant advantage over existing one-class baselines, and 3) adopts a lightweight Stage 1 detector for rapid macroscopic filtering and a powerful Stage 2 detector for in-depth microscopic pattern analysis. Our evaluation on a leadership-scale HPC system demonstrates the effectiveness of our two-stage pipeline: its lightweight Stage 1 detector identifies macroscopic contextual anomalies (e.g., unusual file types and locations) with near-perfect accuracy (Recall >99.9\% at <0.01\% FPR), while the complete framework effectively detects a range of injected threats mimicking resource exploitation, ransomware, and data exfiltration, achieving a final Recall of XX.X\% with a False Positive Rate (FPR) of X.X\%, and significantly outperforming four baseline one-class models in all test cases.


% \begin{table*}[!t] % Use table* for a two-column spanning table if needed, or table for single column
% \caption{A Landscape of Modern Intrusion Detection Systems and Their Core Contributions. The checklist highlights each system's focus in relation to ScaleMon's value propositions.}
% \label{tab:ids_landscape_final}
% \centering
% \begin{tabular}{@{}ll p{3.2cm} l cccc@{}}
% \toprule
% \textbf{Study} & \textbf{Domain} & \textbf{Data Source} & \textbf{Venue} & \textbf{\rotatebox[origin=c]{90}{Fine-grained}} & \textbf{\rotatebox[origin=c]{90}{Scalable}} & \textbf{\rotatebox[origin=c]{90}{Data-Efficient}} & \textbf{\rotatebox[origin=c]{90}{Practical}} \\
% \midrule
% Bro~\cite{paxson1999bro} & NIDS & Network Packets & Comp. Networks '99 & & \checkmark & & \checkmark \\
% Cerberus~\cite{zhou2024cerberus} & NIDS & Network Packets & IEEE S\&P '24 & & \checkmark & & \\
% \midrule
% DeepLog~\cite{du2017deeplog} & HIDS & System Logs & ACM CCS '17 & & & & \checkmark \\
% Kairos~\cite{cheng2024kairos} & HIDS & Provenance Graph & IEEE S\&P '24 & \checkmark & & & \\
% Flash~\cite{rehman2024flash} & HIDS & Provenance Graph & IEEE S\&P '24 & \checkmark & & & \\
% ORTHRUS~\cite{jiang2025orthrus} & HIDS & Provenance Graph & USENIX Sec. '25 & \checkmark & & & \checkmark \\
% VELOX~\cite{bilot2025sometimes} & HIDS & Provenance Graph & USENIX Sec. '25 & \checkmark & & & \checkmark \\
% Jbeil~\cite{khoury2024jbeil} & HIDS & Authentication Logs & IEEE S\&P '24 & & & \checkmark & \\
% \midrule
% \textbf{ScaleMon (Ours)} & HIDS & \textbf{Fine-grained I/O Logs} & - & \checkmark & \checkmark & \checkmark & \checkmark \\
% \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table}[!t]
% \caption{A Landscape of Modern Intrusion Detection Systems and Their Addressed Challenges. The checklist indicates the primary challenges each study aims to solve.}
% \label{tab:ids_landscape}
% \centering
% % \resizebox를 사용하여 테이블 전체를 컬럼 너비에 맞춥니다.
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}llcccc@{}}
% \toprule
% \textbf{Study} & \textbf{Domain} & \textbf{\begin{tabular}[c]{@{}c@{}}Zero-day\\ Attack\end{tabular}} & \textbf{Scalability} & \textbf{\begin{tabular}[c]{@{}c@{}}Data\\ Scarcity\end{tabular}} & \textbf{Practicality} \\
% \midrule
% \multicolumn{6}{l}{\textit{Data Source: Network Packets}} \\
% Bro~\cite{paxson1999bro} & NIDS & & & & \\
% Cerberus~\cite{wang2024cerberus} & NIDS & & \checkmark & & \\
% \midrule
% \multicolumn{6}{l}{\textit{Data Source: System Logs / Provenance Graph}} \\
% DeepLog~\cite{du2017deeplog} & HIDS & \checkmark & & & \\
% Kairos/Flash~\cite{cheng2024kairos,rehman2024flash} & HIDS & \checkmark & & & \\
% ORTHRUS/Simpler...~\cite{lee2024orthrus,ahn2024sometimes} & HIDS & & & & \checkmark \\
% Jbeil~\cite{khoury2024jbeil} & HIDS & \checkmark & & \checkmark & \\
% \midrule
% \multicolumn{6}{l}{\textit{Data Source: Fine-grained I/O Logs}} \\
% \textbf{ScaleMon} & HIDS & \checkmark & \checkmark & \checkmark & \checkmark \\
% \bottomrule
% \end{tabular}%
% }
% \end{table}

% related work
% TEE, fire wall, full packet inspection, simplmpi pattern(unsufficient)

% but Scale Mon, is concentrate on I/O
% \begin{table}[H]
% \footnotesize
% \caption{Representative attack scenarios and expected I/O behavior changes in HPC systems.}
% \label{intro_table}
% \centering
% {\rmfamily
% \begin{tabularx}{\linewidth}{X|X}
% \toprule
% \textbf{Attack Type} & \textbf{Expected Change in I/O Behavior} \\
% \midrule
% \textbf{Data exfiltration} &unusual access to file that don't have to access if it is was benign. and do some read operation \\
% \midrule
% \textbf{Data poisoning / code alteration} & Irregular write operations to input datasets or executables and source code; increased.\\
% \midrule
% \textbf{Ransomware} &unusual access to file that don't have to access if it is was benign. and do some write operation\\
% \midrule
% \textbf{Misuse of computing cycles (cryptomining, unauthorized jobs)} & deviation from typical job I/O ratios; I/O activity observed outside normal workload windows. \\
% \midrule
% \textbf{Disruptions or denial-of-service} & Abnormally high I/O contention; excessive open/close requests; delayed or incomplete write operations causing queue buildup. \\
% \bottomrule
% \end{tabularx}
% }
% \end{table}

% \begin{table}[!t]
% \footnotesize
% \caption{\changjong{KCJ: You need to add related works...}}
% \label{intro_table}
% \centering
% {\rmfamily
% \begin{tabularx}{\linewidth}{l|X|c|c|c|c}
% \toprule
% \textbf{Study} & \textbf{for what} & \textbf{network} & \textbf{computation resource} & \textbf{I/O} & \textbf{IU} \\
% \midrule
% Shanbhag \textit{et al.}~\cite{shanbhag2020study} & Relational DB          & Single      & \checkmark &            & \checkmark \\
% Sirin \textit{et al.}~\cite{sirin2021performance} & HTAP DB                & Single      &            &            & \checkmark \\
% Fuchs \textit{et al.}~\cite{fuchs2022sortledton}  & GraphDB               & Single      &            & \checkmark & \checkmark \\
% Rui \textit{et al.}~\cite{rui2020efficient}       & Relational DB          & Single      & \checkmark &            & \checkmark \\
% Zhang \textit{et al.}~\cite{zhang2023scalable}    & OpenMLDB              & Single      &            &            & \checkmark \\
% Cai \textit{et al.}~\cite{caibonsaikv}            & Key-value DB          & Single      & \checkmark & \checkmark &            \\
% Cheng \textit{et al.}~\cite{chengmammoths}        & GraphDB               & Single      &            & \checkmark &            \\
% Kim \textit{et al.}~\cite{kim2019border}          & Key-value DB          & Single      &            & \checkmark & \checkmark \\
% Zhi \textit{et al.}~\cite{zhicorograph}           & GraphDB               & Single      &            & \checkmark &            \\
% Lee \textit{et al.}~\cite{lee2023lru}             & Storage engine in RDB & Single      &            & \checkmark & \checkmark \\
% An \textit{et al.}~\cite{an2022avoiding}          & Storage engine in RDB & Single      &            & \checkmark & \checkmark \\
% %Lim \textit{et al.}~\cite{lim2017cicada}                         & Engine for OLTP          & Single & \checkmark & \checkmark & \checkmark \\
% Kallman \textit{et al.}~\cite{kallman2008h}                             & Relational DB          & Distributed & \checkmark &            &            \\
% Michael \textit{et al.}~\cite{stonebraker2013voltdb}              & Relational DB          & Distributed & \checkmark &            &            \\
% \midrule
% \textbf{ScaleMon}                                & Relational DB          & Single      & \checkmark & \checkmark & \checkmark \\
% \bottomrule
% \end{tabularx}
% }
% \end{table}

% none of them use i/o log for security purpose.
% attack -> change in I/O behaviour


% \skim{Need to add table}
% Many previous studies, as shown in Table 1, aimed to defense system against multiple attack type by multiple way. there are some protection scheme that can be applied to HPC system.



% have focused on optimizing quantum circuit
%  simulation from different architecture and algorithm perspectives.
 
% There have been may studies that aim to defense hpc system against attacks. there can be divided to protection mechanism and defense mechanism. 
% protection mechanism alone is not sufficient.
% defese mechanism 
% There have been many studies,,, Many previous works focues on the HPC security heavely rely on incomming network such as improving firewall. 


% In addition, other papers focuses on the regular resource utilization in HPC but utilize the information to improve the peformance and analyze the user behavior.
% Kim et al .......
% While some studies suggest the benefit of regularity in terms of security breach detection, they only focus on possible oppertunity and do not implement and verify the scheme's benfit in large-scale production systems.

% Enabling Refinable Cross-Host Attack Investigation with Efficient Data Flow Tagging and Tracking



% \changjong{

% In this paper, we propose \ScaleMon, a novel security monitoring framework for Leadership-scale HPC systems. Our key idea is to leverage an I/O trace-to-image transformation design, realizing large-scale I/O log data compression and robust, scalable anomaly detection. To achieve this, \ScaleMon introduces three key mechanisms. First, its \textit{novel data representation} transforms voluminous, fine-grained logs for efficient, parallel processing by ML models. The framework parses raw DXT logs into fixed-size 3D image tensors (e.g., capturing request type, offset, time, and size), which reduces data volume (by up to XXX)  while preserving spatial-temporal access patterns necessary for detection. Second, a \textit{Synthetic Anomaly Generator (SAG)} mechanism maintains high detection accuracy against subtle attacks. In contrast to relying only on normal data (e.g., which fails to detect subtle anomalies), \ScaleMon systematically perturbs benign log images to create a diverse set of synthetic anomalies. This two-class approach enables the model to learn the subtle boundary between normal and malicious behavior, overcoming the limitations of existing one-class baselines. Finally, a \textit{flexible ML-based detection pipeline} enables both per-application modeling and generalized detection. \ScaleMon can model the I/O behavior of specific applications (Case 1) or generalize across workloads, enabling effective detection of attacks on applications unseen during training (Case 3).

% }





% \taebin{pids imposssible and unreluctant yet because of logging overhead of linux auditd, camFlow,eBPF based Falco .. ids that only using HPC naitive logging tool needed}
% Many previous studies, as shown in Table~\ref{tab:ids_landscape}, have advanced intrusion detection from various perspectives. Network-based Intrusion Detection Systems (NIDS), pioneered by Bro (Zeek)\cite{paxson1999bro} and modernized by scalable in-network architectures like Cerberus\cite{zhou2024cerberus}, form the first line of defense. However, to capture host-internal threats, Host-based Intrusion Detection Systems (HIDS) have become crucial. Early ML-based HIDS like DeepLog\cite{du2017deeplog} modeled system logs as sequences but lacked fine-grained context. To address this, a dominant research trend has emerged using provenance graphs, where systems leverage Graph Neural Networks (GNNs) to achieve high detection accuracy~\cite{cheng2024kairos, rehman2024flash}. However, recent studies within this domain, such as ORTHRUS\cite{jiang2025orthrus} and VELOX\cite{bilot2025sometimes}, critically point out that the extreme computational overhead and alert complexity of these GNN-based approaches hinder their practical deployment—a problem severely amplified in HPC environments. Furthermore, while the fundamental challenge of data scarcity is being addressed by data augmentation techniques as seen in Jbeil\cite{khoury2024jbeil}, it remains an acute problem in specialized domains.