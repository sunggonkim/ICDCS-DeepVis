\section{\DASCALE Design} \label{design}

We propose \DASCALE, a scalable relational database for manycore systems that adopts a single-master, multi-slave architecture with a shared-nothing design on a shared-memory single node to overcome lock contention and I/O stalls. 
It overcomes lock contention by simplifying ordering and synchronization via a single master and leverages transaction distribution and parallel processing across database instances to reduce I/O stalls.

\DASCALE is designed using the following three key strategies:

\begin{itemize}

    \item \textbf{Strategy \#1:} \textit{\DASCALE adopts a shared-nothing architecture on shared memory and handles transactions using a single-master, multiple slave instances within a single manycore server. Each database instance is pinned to dedicated CPU cores and memory regions to minimize cross-instance interference.}

    \item \textbf{Strategy \#2:} \textit{\DASCALE addresses lock contention by employing a single master to simplify ordering and synchronization while partitioning transactions for independent processing by slave instances. During data modification, instances acquire row-level locks scoped to their own data, preventing global lock contention.}
    
    \item \textbf{Strategy \#3:} \textit{\DASCALE maintains separate WALs for distributed transaction data within each instance, alleviating I/O stalls through parallel I/O. To ensure consistency, the master routinely verifies the persistent state of all distributed instances and exclusively creates the global metadata.}

\end{itemize}


Together, these strategies address key scalability bottlenecks in manycore systems. 
Unlike approaches that optimize individual components, such as \textit{WAR}~\cite{an2022avoiding} for logging or \textit{LRU-C}~\cite{lee2023lru} for eviction, \DASCALE integrates architectural, concurrency, and I/O optimizations into a unified design. 
In addition, it avoids deployment complexities faced by fully distributed databases such as \textit{VoltDB}~\cite{stonebraker2013voltdb}, which encounter challenges in data partitioning, transaction coordination, and network overhead, making them ill-suited for single manycore nodes. 
By leveraging a single-node, shared-memory architecture, \DASCALE balances scalability with operational simplicity, making it well-suited for modern manycore hardware.

\begin{figure}[!t]
    \centering
    \includegraphics[width=8.5cm]{figures/DASCALE_Procedure.pdf}
    %%%%%%\vspace{-0.4cm}
    \caption{Overall Architecture of \DASCALE.}
    \label{lcp_procedure}
    %%%%%%\vspace{-.3cm}
\end{figure}





\subsection{Overall Architecture}\label{section_transcation_processing}

Figure~\ref{lcp_procedure} shows the architecture of \DASCALE. As shown in the figure, it consists of a single master and multiple slave instances.


\noindent\textbf{Master Instance:} The master exclusively handles transaction distribution, coordination, ordering, and global database synchronization. \DASCALE chooses single coordination over multiprocessing coordination in traditional RDBs or multiple coordination strategies used in distributed databases. Multiprocessing coordination is inherently complex, involving multiple processes contending for shared resources such as flush and buffer lists, and scales only within limited single components. 
Likewise, multiple coordination relying on heavyweight distributed consensus faces scalability ceilings. 
In contrast, \DASCALE employs a lightweight single-master coordinator coupled with a two-level asynchronous checkpointing scheme on shared storage. 
Despite using single-master coordination, \DASCALE sacrifices the simplicity of a fully shared global state to achieve massive parallelism through architectural partitioning. This practical trade-off balances scalability and complexity by avoiding challenges inherent in both multiprocessing coordination and heavyweight distributed consensus. Decoupling coordination overhead from I/O stalls is critical to maintaining high throughput, as it frees transaction processing from being blocked by synchronous I/O operations.




The master comprises three key components: \textit{Instance Manager}, \textit{Tx Coordinator}, and \textit{Global Sync Handler}.
First, \textit{Instance Manager} maintains metadata for both the master and slave instances, including identifiers (Instance ID, Data File ID, Log File ID) and instance status (e.g., active or passive).
Second, \textit{Tx Coordinator}, composed of \textit{Task Scheduler} and \textit{Global Sync Handler}, schedules transaction processing across active instances and manages global synchronization.
\textit{Task Scheduler} maps transactions to instances using a hash function (\textit{f(x)}) on requested pages, identifies the responsible slaves, and records detailed entries for each transaction in \texttt{Tx Table}. 
This entry contains the transaction ID, execution status, and participating active instance IDs with their associated locks (e.g., Page-Row ID) to enforce concurrency control.
The master sends processing requests to all involved slaves and waits for acknowledgments before finalizing the transaction by updating its status to committed in \texttt{Tx Table}.
Third, \textit{Global Sync Handler} regularly monitors instance states and creates global checkpoints (GCPs) representing the persistent database state. A GCP aggregates metadata from Local Checkpoints (LCPs) of all slaves, which track pages processed by transactions. 
After the GCP is created, \texttt{Global Version Table} is updated to map each GCP ID to the latest LCP ID across instances.


\noindent\textbf{Slave Instance: } Each slave processes transactions assigned by the master via WAL and LCP writes. Because transactions are pre-coordinated by the master, slaves execute them independently and in parallel. Each slave consists of \textit{Local Sync Handler} and \textit{Tx Executor} coordinating transaction processing with the master. \textit{Local Sync Handler} receives assigned transactions, creates Local Checkpoints (LCPs), appends log records to WAL buffer, and flushes WAL to ensure durability. \textit{Tx Executor} performs parallel writes to in-memory buffer pages. After both WAL flush and in-memory writes complete, the slave sends an acknowledgment to the master confirming LCP finalization. Dirty pages are then asynchronously flushed to storage.


\begin{figure}[!t]
    \centering
    \includegraphics[width=8.5cm]{figures/Global checkpoints.pdf}
    %%%%%%\vspace{-0.4cm}
    \caption{Timeline of Local and Global checkpoints (S\#: Slave ID, GCP: Global Checkpoint, LCP: Local Checkpoint, WAL: WAL Flush, TX: Transaction Data Write).} %\changjong{need change global version table format!!}} 
    \label{global checkpoint}
    %%%%%%\vspace{-0.3cm}
\end{figure}




\begin{figure*}[t]
    \centering
    \subfloat[read.]{\includegraphics[width=4.27cm]{figures/read.pdf}\label{read}}%
    \hfill % a와 b 사이 간격
    \subfloat[write.]{\includegraphics[width=6.9cm]{figures/write.pdf}\label{write}}%
    \hfill % b와 c 사이 간격
    \subfloat[read-modify-write.]{\includegraphics[width=6.17cm]{figures/rmw.pdf}\label{rmw}}%
    %\hfill
  \caption{Procedure of Concurrency Control with Different Operations: read, write, read-modify-write.} 
  \label{concurrency_control}

\end{figure*} 

\subsection{Scalability with Two-Level Asynchronous Checkpointing}~\label{section_checkpointing}

In \DASCALE, transaction consistency and scalability are ensured through a two-level checkpointing mechanism. 
Each slave creates a local checkpoint (LCP) to persist its part of a transaction, while the master periodically creates a global checkpoint (GCP) that aggregates the most recent LCPs from all instances to capture the entire database state.




\noindent\textbf{Decentralized LCP Management:} As shown in Figure~\ref{global checkpoint}, the master initiates a transaction by selecting a single slave to perform the required WAL and transaction data writes. 
This avoids broadcasting to all slaves, which would be inefficient on shared storage due to multiple writes of the same data adding overhead without improving reliability. 
The designated slave then processes the transaction in two phases: \textit{Local Sync Handler} first handles the WAL flush, followed by \textit{Tx Executor} applying the requested transaction data per local checkpoint (i.e., LCP 1 - Slave 1, LCP N - Slave N). Once the local checkpoint is completed, the slave sends an acknowledgment (Ack) to the master, finalizing the local checkpoint process.




\DASCALE introduces three main contributions through the local checkpoint (LCP) mechanism. 1) It writes LCPs only once via a designated slave instead of broadcasting to all instances, reducing redundant I/O overhead on shared storage and eliminating unnecessary coordination while preserving crash consistency. 2) It enables pipelined LCP processing for increased parallelism by distributing independent LCPs to designated slaves, allowing the master to coordinate subsequent transaction requests without waiting for prior completions and letting slaves process their LCPs concurrently without synchronization. 3) It writes LCPs to shared storage, which allows universal access for all instances and lays a foundation for robust crash recovery, enabling seamless master failover by promoting another slave and reassignment of failed slave workloads to other instances.

\begin{comment}
    \DASCALE introduces three main contributions through the local checkpoint (LCP) mechanism: first, it writes LCPs only once via a designated slave instead of broadcasting to all instances, reducing redundant I/O overhead on shared storage and eliminating unnecessary coordination while preserving crash consistency; second, it enables pipelined LCP processing for increased parallelism by distributing independent LCPs to designated slaves, allowing the master to coordinate subsequent transaction requests without waiting for prior completions and letting slaves process their LCPs concurrently without synchronization; third, it writes LCPs to shared storage, which allows universal access for all instances and lays a foundation for robust crash recovery, enabling seamless master failover by promoting another slave and reassignment of failed slave workloads to other instances.
\end{comment}


    
\noindent\textbf{Global Persistence with GCP:}~\label{section_GCP} In \DASCALE, the master periodically (e.g., every 2000 milliseconds) creates a global checkpoint (GCP) to ensure consistency across local checkpoints.
This process extends the persistent state of all slaves, represented by their local checkpoints (LCPs), into a unified global database snapshot (the GCP) by tracking the latest state of each instance and persisting their combined metadata.
As shown in Figure~\ref{global checkpoint}, the master first creates an initial global checkpoint (GCP 1) using the existing local checkpoint data (LCP 1). Subsequent global checkpoints (e.g., GCP 2) aggregate completed local checkpoints (LCP 1, LCP 2, LCP 3, etc.) to maintain a consistent state across the system. Once all transactions are complete, a final global checkpoint (GCP N) consolidates information from all persistent local checkpoints (LCP 1 through LCP N).



\DASCALE introduces two main contributions through the global checkpoint (GCP) mechanism. 1) In\DASCALE, it increases GCP efficiency by having a single slave write each LCP instead of broadcasting to all slaves, which eliminates redundant data and reduces the number of checkpoints the GCP must manage. 2) It enables non-blocking slave progress by allowing the master to create the GCP independently. The master reads LCPs directly from shared storage rather than requesting them from slaves.


\begin{comment}
    
\DASCALE introduces two main contributions through GCP. 


\begin{itemize}
    \item \DASCALE increases GCP efficiency by having a single slave write each LCP instead of broadcasting to all slaves. This eliminates redundant data and ultimately reduces the number of checkpoints the GCP must manage. 
    \item \DASCALE enables non-blocking slave progress by having the master create a GCP independently. The master can do this because it reads LCPs directly from shared storage instead of requesting them from the slaves.
\end{itemize}

\end{comment}







\subsection{Transaction Processing with Concurrency Control}~\label{3.2}

Figure~\ref{concurrency_control} shows the procedures for each transaction operation type. 
For read transactions, \DASCALE employs a lock-free mechanism over global snapshots (GCPs), allowing non-blocking retrieval of up-to-date data. For write and read-modify-write transactions, it enforces exclusive row-level locks on all target rows until commit or abort to ensure atomicity and isolation.


\noindent\textbf{Read Transaction:} As shown in Figure~\ref{read}, \textit{Task Scheduler} in the master first hashes (\textit{f(x)}) the requested pages to locate the relevant slaves (e.g., S1 and S4). 
The master then references the \texttt{Global Version Table} to find the most recent data version among the identified slaves (e.g., S1 - LCP 28, S4 - LCP 31) (\ding{182}). 
This process ensures that read operations use a globally consistent snapshot, because the latest GCP in the table points to the correct LCPs.
In addition, to maintain this consistency without blocking, the system uses a lock-free update mechanism for the table. 
When the master updates the \texttt{Global Version Table}, it forks the table and atomically swaps in the new version. 
This method prevents partial updates and allows only selected slaves to respond to read requests without contention.


To perform an actual read operation, the master sends the read request to the slaves that have the required LCPs for the current GCP (e.g., GCP 51 requires S1 - LCP 28 and S4 - LCP 31) (\ding{183}). 
Upon receiving a request from the master, each selected slave refers to its local LCP sequence and then reads the target data from the corresponding LCP file location. 
Then, the requested data is sent to the master, which combines the partial results into a globally consistent snapshot and returns it to the client (\ding{184}).




\noindent\textbf{Write Transaction:} As shown in Figure~\ref{write}, for a write transaction (e.g., Tx 100), \textit{Task Scheduler} in the master first hashes (\textit{f(x)}) the target pages to identify the owner slaves (e.g., S1 and S4) (\ding{182}). 
Then, it records the transaction entry, containing the identified instances and pages, into \texttt{Tx Table} to track its progress (\ding{183}). 
The master then sends a request to these designated slaves to process the transaction.


To perform an actual write operation, the master sends the write request to the slaves. 
If a write transaction spans multiple slaves (i.e., Tx 100 on S1 and S4), the master instructs each relevant slave to create a new LCP (e.g., S1 - LCP 571, S4 - LCP 261). 
Then, each designated slave processes its portion of the transaction in parallel, which includes flushing its WAL and updating the in-memory pages under a row-level lock (e.g., Slave 1 – LCP 571 for Page 63, Slave 4 – LCP 261 for Page 594) (\ding{184}).
After receiving acknowledgments from all participating slaves, the master updates \texttt{Global Version Table} to finalize the new GCP (e.g., GCP 581) and commits the transaction (\ding{185}). 
Concurrently, updated dirty pages are persisted to each slave LCP File asynchronously (\ding{186}). 
LCP files are managed in an append-only manner, with a background garbage collection process that periodically removes outdated pages.






\noindent\textbf{Read-Modify-Write Transaction:} As shown in Figure~\ref{rmw}, when a client requests a read-modify-write transaction (e.g., Tx 30), the master records it in \texttt{Tx Table}: transaction ID, target pages, operation type, and assigned instances. It references \texttt{Global Version Table} to find the slave holding the source data (e.g., Slave 1 with Page 63) and sends the read request (\ding{182}).


The execution proceeds in two phases. 
In the read phase, the designated slave (e.g., Slave 1) acquires a row-level lock on the source page, reads the data, and returns it to the master (\ding{183}). Upon receiving the data, the master updates \texttt{Tx Table} to mark the transaction status as waiting for the modify-write phase. 
The slave (e.g., Slave 1) holds a row-level lock on the source page to prevent large table-level serialization (\ding{184}). 
To ensure consistency, this lock is not released until the transaction is committed for consistency. 
In the modify-write phase, the master sends the write request with the read source data to the target slave (e.g., Slave 4). 
The slave then locks its target page (e.g., Page 214), writes the data, and sends an acknowledgment (\ding{185}). 
After receiving the acknowledgment for the write, the master commits the transaction, updates \texttt{Global Version Table} with the new LCP (i.e., LCP 15), and instructs both slaves (e.g., Slave 1, Slave 4) to release their row-level lock of the committed page (\ding{186}).
This design, where each instance manages its own locks, avoids the bottleneck of a centralized lock manager found in monolithic systems.


\subsection{\DASCALE implementation}


We implemented \DASCALE based on the NDB engine of MySQL NDB Cluster 7.4.10, a widely used distributed database for managing distributed nodes and storage~\cite{Mysql:NDB_Cluster}. Modifications were primarily concentrated in the Distributed Data Handler (Dbdih) module, responsible for deploying database instances and managing global and local checkpoints. Although the code changes were minimal (fewer than 300 lines), they fundamentally transformed the central coordination model of \DASCALE into a shared-nothing on shared-memory design.
This was achieved by reorganizing the responsibilities of the Dbdih module from centralized control to distributed instance orchestration and redesigning checkpoint handling to support independent transaction processing under a lightweight global coordinator.
The three primary modifications are as follows: 1) The transaction distribution algorithm is modified to make the master a central coordinator. A new data structure allows the master to assign transactions to specific slaves, avoiding inefficient broadcasts. 2) Transaction processing is updated to support concurrent, independent local checkpoints (LCPs). Each slave now generates and flushes its own LCP, enabling parallelism and reducing latency. 3) The communication protocol is changed to a direct request-acknowledgment model. The master now communicates only with assigned slaves and waits for their specific responses, simplifying recovery.