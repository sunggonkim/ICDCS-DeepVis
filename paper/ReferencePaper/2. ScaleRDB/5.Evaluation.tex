%\vspace{-0.3cm}
\section{Evaluation} \label{eval}
%\vspace{-0.3cm}

\begin{comment}
\mbox{}\\
Our evaluation of \DASCALE focuses on answering these questions:



\begin{itemize}
    \item Can \DASCALE scale to increase throughput and reduce latency?(Section~\ref{eval_throughput},\ref{eval_latency})
    \item How does \DASCALE perform across various read/write (R/W) ratios? (Section~\ref{eval_latency})
    \item Does \DASCALE alleviate lock contention and I/O stalls? (Section~\ref{systemoverhead})
    \item How does \DASCALE scale with increasing instances and threads to utilize manycore CPUs effectively? (Section~\ref{scale_eval},~\ref{CPU_util})
    \item Does the master sustain performance, and how does recovery time change, as the Global Checkpoint interval grows? (Section~\ref{GCP_evel})
\end{itemize}
    
\end{comment}


\begin{comment}

\begin{table}[t]
\centering
\captionsetup{font=small, textfont=sc, labelsep=period, justification=centering}
\caption{Server Specifications.}
\label{tab:servers}
%\vspace{-7pt}
\renewcommand{\arraystretch}{1.1}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{>{\centering\arraybackslash}p{0.2\columnwidth} >{\raggedright\arraybackslash}p{0.75\columnwidth}}
  \hline
  \rowcolor{gray!20} \multicolumn{2}{c}{\textbf{Server A}} \\ \hline
  \footnotesize \textbf{CPU}                & \footnotesize Intel i9-12900k (16 cores / 24 threads @ 5.2GHz) \\ \hline
  \footnotesize \textbf{Memory}             & \footnotesize DDR4 45GB                                   \\ \hline
  \footnotesize \textbf{Storage}            & \footnotesize Samsung 870 Pro SATA SSD 500GB                               \\ \hline 
  \footnotesize \textbf{OS}                 & \footnotesize Ubuntu 22.04.4 LTS, Linux 6.5.0-25-generic          \\ \hline
  \multicolumn{2}{l}{}  \\ \hline
  \rowcolor{gray!10} \multicolumn{2}{c}{\textbf{Server B}} \\ \hline
  \footnotesize \textbf{CPU}                & \footnotesize AMD EPYC 7713 (64 cores / 128 threads @ 2.0GHz)            \\ \hline
  \footnotesize \textbf{Memory}             & \footnotesize DDR4 130GB                                  \\ \hline
  \footnotesize \textbf{Storage}            & \footnotesize Seagate FireCuda 530 NVMe SSD 2TB                             \\ \hline
  \footnotesize \textbf{OS}                 & \footnotesize Ubuntu 22.04.3 LTS, Linux 6.6.2                     \\ \hline
\end{tabular}
}
%\vspace{-0.3cm}
\end{table}    
\end{comment}

\begin{table}[!t]
  \captionsetup{font=small, textfont=sc, labelsep=period, justification=centering}
  \caption{List of various workloads.}\label{workload_table}
  \centering
  \footnotesize
  \begin{tabular}{|>{\centering\arraybackslash}p{0.23\columnwidth}|
                  >{\centering\arraybackslash}p{0.23\columnwidth}|
                  >{\centering\arraybackslash}p{0.15\columnwidth}|
                  >{\centering\arraybackslash}p{0.23\columnwidth}|}
    \hline
    \textbf{Workload} & \textbf{R/W Ratio(\%)} & \textbf{Complexity} & \textbf{Skewness} \\ \hline \hline
    TPC-C & 92/8 & High & Realistic \\ \hline
    YCSB-Workload A  & 50/50, 20/80, 80/20 & Low & Realistic, Extreme \\ \hline
    YCSB-Workload B  & 95/5 & Low & Realistic, Extreme \\ \hline
    YCSB-Workload F  & 50/50 (RMW) & High & Realistic, Extreme \\ \hline
  \end{tabular}
\end{table}



\begin{table}[!t]
  \captionsetup{font=small, textfont=sc, labelsep=period, justification=centering}
  \caption{List of evaluated database systems.}\label{db_table}
  \centering
  \footnotesize
  \begin{tabular}{|>{\centering\arraybackslash}p{0.25\columnwidth}|
                  >{\centering\arraybackslash}p{0.09\columnwidth}|
                  >{\centering\arraybackslash}p{0.20\columnwidth}|
                  >{\centering\arraybackslash}p{0.14\columnwidth}|
                  >{\centering\arraybackslash}p{0.11\columnwidth}|}
    \hline
    \textbf{Database} & \textbf{Type} & \textbf{Architecture} & \textbf{Operation} & \textbf{Version} \\ \hline \hline
    MySQL~\cite{Mysql} & Relation & Monolithic & Disk-based & 5.6 \\ \hline
    \textit{WAR}~\cite{an2022avoiding} (MySQL) & Relation & Monolithic & Disk-based & 5.7 \\ \hline
    \textit{LRU-C}~\cite{lee2023lru} (MySQL) & Relation & Monolithic & Disk-based & 5.6 \\ \hline
    PostgreSQL~\cite{Postgre} & Relation & Monolithic & Disk-based & 16.2 \\ \hline
    \textit{MongoDB}~\cite{MongoDB} & NoSQL & Non-monolithic & Disk-based & 7.0 \\ \hline
    \textit{VoltDB}~\cite{stonebraker2013voltdb} & Relation & Non-monolithic & In-memory & 14.4 \\ \hline
    \DASCALE & Relation & Monolithic & Disk-based & Prototype  \\ \hline
  \end{tabular}
\end{table}






\begin{comment}
  \begin{table}[]
\footnotesize
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|}
\hline
\multicolumn{2}{|c|}{\textbf{Server A}} \\ \hline
\textbf{CPU}                & Intel i9-12900k (16 cores / 24 threads @ 5.2GHz) \\ \hline
\textbf{Memory}             & DDR4 80GB                                   \\ \hline
\textbf{Storage}            & Samsung 870 Pro SATA SSD 500GB                               \\ \hline 
\textbf{OS}                 & Ubuntu 22.04.4 LTS, Linux 6.5.0-25-generic          \\ \hline
\multicolumn{2}{l}{}  \\ \hline

\multicolumn{2}{|c|}{\textbf{Server B}} \\ \hline
\textbf{CPU}                & AMD EPYC 7713 (64 cores / 128 threads @ 2.0GHz)            \\ \hline
\textbf{Memory}             & DDR4 130GB                                  \\ \hline
\textbf{Storage}            & Seagate FireCuda 530 NVMe SSD 2TB                             \\ \hline
\textbf{OS}                 & Ubuntu 22.04.3 LTS, Linux 6.6.2                     \\ \hline
\end{tabular}%     
}
\caption{Server specifications.} %\skim{Let's change the server name to server A and B.}}
\label{tab:servers}
\end{table}
  
\end{comment}



\subsection{Experimental setup}~\label{eval_setup}

The evaluation runs on a server with AMD EPYC 7713 (64 cores, 128 threads, 2.0GHz), 45 GB DDR4 memory, a 500 GB Samsung 870 Pro SATA SSD, and Ubuntu 22.04.1 LTS (Linux 6.5.0-25-generic).

\noindent\textbf{Benchmarks:} We employ TPC-C~\cite{TPC-C} (25 GB database, 250 warehouses) for OLTP and YCSB~\cite{cooper2010benchmarking} workloads A, B, and F. Skew is evaluated from realistic to extreme: realistic skew via TPC-C (New-Order and Payment dominant) and YCSB using Zipfian from \textit{ZipfianGenerator} ($\theta \approx 0.99$). Extreme skew via YCSB hotspot by tuning the hot data fraction and the hot access fraction. A summary appears in Table~\ref{workload_table}.



\noindent\textbf{Evaluated Databases: }For comparison, we use MySQL 5.6.28~\cite{Mysql} with tuned settings for a fair baseline (In Section~\ref{motivation}) and PostgreSQL~\cite{Postgre}, a monolithic RDBMS employing MVCC and group commit for high concurrency. 
For state-of-the-art, we evaluate \textit{WAR}~\cite{an2022avoiding} and \textit{LRU-C}~\cite{lee2023lru}, which target lock contention and I/O stalls in buffer management: \textit{LRU-C} reduces contention by splitting the global buffer list mutex, while \textit{WAR} mitigates I/O stalls via a temporary buffer that writes dirty pages (\textit{pwrite}) asynchronously to decouple reads from slow flushes (\textit{fsync}). These monolithic schemes form strong baselines against \DASCALE, as all address lock contention and I/O stalls, but at the component level. We also include non-monolithic systems: \textit{MongoDB}~\cite{MongoDB}, a sharded NoSQL store, and \textit{VoltDB}~\cite{stonebraker2013voltdb}, an in-memory OLTP engine. Section~\ref{variousdb} analyzes these systems, and Table~\ref{db_table} summarizes all databases.





%\vspace{-0.3cm}
\subsection{Throughput and Latency of Monolithic Database}

%\changjong{monolithic -> need to add postgreSQL???}

\subsubsection{Throughput}~\label{eval_throughput}
%\vspace{-0.3cm}
\begin{figure}[t]

    % TPS 결과 그림 (Server A 단독)
    \includegraphics[width=8.5cm]{figures/Smallserver_TPS.png}

    %\vspace{-0.3cm}
    \caption{TPS using TPC-C.}
    \label{all_tps}
    %\vspace{-0.3cm}
\end{figure}


\begin{comment}
% Server B Data included.

%\vspace{-0.3cm}
\subsection{Throughput}~\label{eval_throughput}
%\vspace{-0.3cm}
\begin{figure}[t]
    \centering
    % Legend figure
    \includegraphics[width=6.5cm]{figures/lagend_tps.png}
    %\vspace{-0.2cm}

    % TPS 결과 그림 2개
    \subfloat[Server A.]{%
        \includegraphics[width=4.15cm]{figures/Smallserver_TPS.png}
        \label{Smallserver_TPS}
    }%
    \subfloat[Server B.]{%
        \includegraphics[width=4.20cm]{figures/TPCC_Server B.png}
        \label{Largeserver_TPS}
    }%

    %\vspace{-0.3cm}
    \caption{TPS using TPC-C.}
    \label{all_tps}
    %\vspace{-0.3cm}
\end{figure}    
\end{comment}



\begin{comment}
 \begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{6.5cm}
        \centering
        \includegraphics[width=6.5cm]{figures/TPCC_Server A.png}
        \caption{Server A.}
        \label{Smallserver_TPS}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{6.5cm}
        \centering
        \includegraphics[width=6.5cm]{figures/TPCC_Server B.png}
        \caption{Server B.}
        \label{Largeserver_TPS}
    \end{subfigure}
    \caption{Transactions per second (TPS) performance comparison of \DASCALE and MySQL on Server A and Server B.}
\end{figure}   
\end{comment}

\begin{figure*}[t]
  \centering
  % Legend (독립적으로 넣기)
  \includegraphics[width=8.5cm]{figures/lagend_tps.png}
  %\vspace{-0.2cm} % legend와 subfigure 사이 간격

  % 실제 그래프들
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_9505.png}
    \caption{R/W:95/5\%.}
    \label{ycsb_9505_small}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_8020.png}
    \caption{R/W:80/20\%.}
    \label{ycsb_8020_small}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_5050.png}
    \caption{R/W:50/50\%.}
    \label{ycsb_5050_small}
  \end{subfigure}
  \begin{subfigure}[b]{3.40cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_2080.png}
    \caption{R/W:20/80\%.}
    \label{ycsb_2080_small}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_workloadF.png}
    \caption{R-M/W:50/50\%.}
    \label{ycsb_f_small}
  \end{subfigure}

  %\vspace{-0.3cm}
  \caption{OPS using YCSB.}
  \label{ops_small}
  %\vspace{-0.3cm}
\end{figure*}


\begin{figure*}[t]
  \centering

  % Legend (독립적으로 삽입)
  \includegraphics[width=8.5cm]{figures/lagend_tps.png}
  %\vspace{-0.3cm}

  % 실제 그래프들
  \begin{subfigure}[b]{3.23cm}
    \includegraphics[width=\textwidth]{figures/Latency_thread8_Small_server.png}
    \caption{Thread: 8.}
    \label{thread8_small}
  \end{subfigure}
  \begin{subfigure}[b]{3.32cm}
    \includegraphics[width=\textwidth]{figures/Latency_thread16_Small_server.png}
    \caption{Thread: 16.}
    \label{thread16_small}
  \end{subfigure}
  \begin{subfigure}[b]{3.32cm}
    \includegraphics[width=\textwidth]{figures/Latency_thread32_Small_server.png}
    \caption{Thread: 32.}
    \label{thread32_small}
  \end{subfigure}
  \begin{subfigure}[b]{3.32cm}
    \includegraphics[width=\textwidth]{figures/Latency_thread64_Small_server.png}
    \caption{Thread: 64.}
    \label{thread64_small}
  \end{subfigure}
  \begin{subfigure}[b]{3.35cm}
    \includegraphics[width=\textwidth]{figures/Latency_thread128_Small_server.png}
    \caption{Thread: 128.}
    \label{thread128_small}
  \end{subfigure}

  %\vspace{-0.3cm}
  \caption{99\% Latency (ms) using TPC-C.}
  \label{latency_small}
  %\vspace{-0.3cm}
\end{figure*}

\begin{figure*}[t]
  \centering

  % Legend (독립적으로 삽입)
  \includegraphics[width=8.5cm]{figures/lagend_tps.png}
  %\vspace{-0.3cm}

  % 실제 그래프들
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_9505_latency.png}
    \caption{R/W:95/5\%.}
    \label{ycsb_9505_latency_A}
  \end{subfigure}
  \begin{subfigure}[b]{3.40cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_8020_latency.png}
    \caption{R/W:80/20\%.}
    \label{ycsb_8020_latency_A}
  \end{subfigure}
  \begin{subfigure}[b]{3.40cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_5050_latency.png}
    \caption{R/W:50/50\%.}
    \label{ycsb_5050_latency_A}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_2080_latency.png}
    \caption{R/W:20/80\%.}
    \label{ycsb_2080_latency_A}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerA_workloadF_latency.png}
    \caption{R-M/W:50/50\%.}
    \label{ycsb_f_latency_A}
  \end{subfigure}

  %\vspace{-0.3cm}
  \caption{99\% Latency (µs) using YCSB.}
  \label{ops_small_latency}
  %\vspace{-0.3cm}
\end{figure*}



\begin{comment}

% Server B Data.


\begin{figure*}[t]
  \centering
  % 첫 번째 행
    \begin{subfigure}[b]{17.9cm}
        \includegraphics[width=\textwidth]{figures/lagend_largeserver.png}
        %\vspace{-0.5cm}
    \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_9505.png}
    \caption{R/W:95/5\%.}
    \label{ycsb_9505}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_8020.png}
    \caption{R/W:80/20\%.}
    \label{ycsb_8020}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_2080.png}
    \caption{R/W:20/80\%.}
    \label{ycsb_2080}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_5050.png}
    \caption{R/W:50/50\%.}
    \label{ycsb_5050}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_workloadF.png}
    \caption{R-M/W:50/50\%.}
    \label{ycsb_f}
  \end{subfigure}
      %\vspace{-0.3cm}
    \caption{Operation per second (OPS) performance comparison of \DASCALE and MySQL on Server B using YCSB.}
    \label{ops_large}
        %\vspace{-0.3cm}
\end{figure*}


\end{comment}





\begin{comment}
    
\end{comment}

%\skim{Can we add SOTA in this part (Throughput and Latency)??? keep talking about mysql doesn't look too good.}

Figure~\ref{all_tps} shows the throughput of MySQL, PostgreSQL, SOTA (\textit{WAR}, \textit{LRU-C}), and \DASCALE with TPC-C and YCSB. 
We configured \DASCALE with its optimal setup of 12 instances (1 Master and 11 Slaves), and 64 concurrent threads were distributed among these instances and pinned to dedicated cores.

\noindent\textbf{Comparison with MySQL and PostgreSQL: }Figure~\ref{all_tps} shows that MySQL TPS decreases with more concurrent threads due to contention, while \DASCALE improves TPS by 3.57$\times$–28.06$\times$, maintaining scalability. PostgreSQL outperforms MySQL at all thread counts (e.g., 721 TPS vs. 36.17 TPS at 64 threads) thanks to better concurrency control, yet \DASCALE still surpasses PostgreSQL by 1.32$\times$ (64 threads) and 1.40$\times$ (32 threads). 
This pattern also appears in YCSB (Figure~\ref{ops_small}), where \DASCALE achieves OPS improvements of up to 110.91$\times$ and consistently outperforms PostgreSQL with 1.20$\times$–1.88$\times$ gains across workloads.

\begin{comment}
As shown in Figure~\ref{all_tps}, with TPC-C benchmark, MySQL TPS decreases as the number of concurrent threads increases, demonstrating limited scalability due to increased contention. In contrast, \DASCALE demonstrates TPS improvements of up to 28.06$\times$ (at 32 threads), a minimum of 3.57$\times$ (at 8 threads) compared to MySQL, and maintains high TPS as the number of concurrent threads increases. In contrast, PostgreSQL exhibits higher TPS than MySQL across all thread counts (e.g., 721 TPS at 64 threads vs. 36.17 TPS for MySQL) due to its more efficient concurrency control and WAL processing. However, \DASCALE still significantly outperforms PostgreSQL, achieving 1.32$\times$ higher TPS at 64 threads and 1.40$\times$ at 32 threads. This trend is consistent when demonstrating OPS with YCSB benchmark across different workloads. As shown in Figure~\ref{ops_small}, \DASCALE demonstrates OPS improvements of up to 10.89$\times$ (at 128 threads, RW:95/05), 25.43$\times$ (at 16 threads, RW:80/20), 110.91$\times$ (at 16 threads, RW:20/80), 20.34$\times$ (at 64 threads, RW:50/50), and 36.12$\times$ (at 64 threads, RMW). When compared with PostgreSQL, \DASCALE still shows clear performance advantages. It achieves 1.20$\times$ higher OPS in a read-heavy workload (RW:95/05), 1.88$\times$ in a write-intensive workload (RW:20/80), and 1.52$\times$ in the RMW workload. 
    
\end{comment}


These results highlight that MySQL suffers from centralized lock contention and synchronous WAL flushes, whereas PostgreSQL alleviates these bottlenecks through MVCC-based concurrency control and more efficient WAL handling, which explains its consistently higher throughput than MySQL. However, it still faces global contention in shared components (e.g., buffer manager, WAL writer, and transaction ID management via the ProcArray), limiting its scalability under high concurrency. The benefits of \DASCALE vary by workload. The gains are greatest in write-intensive workloads (RW:20/80 and RMW) because the asynchronous flush and distributed logging design avoids the severe I/O stalls that bottleneck both MySQL and PostgreSQL. In read-heavy workloads (RW:95/05), the improvements are smaller but still significant, as the system eliminates global contention in shared structures and enables slaves to process reads independently, thereby improving concurrency.




%These results highlight that MySQL suffers from centralized lock contention and synchronous WAL flushes, whereas \DASCALE distributes lock management and performs data flush asynchronously. 
 
%The gains are greatest in write-intensive workloads (RW:20/80 and RMW) because the asynchronous flush and distributed logging design avoids the severe I/O stalls that bottleneck MySQL. 



\noindent\textbf{Comparison with \textit{WAR}, and \textit{LRU-C}: }As shown in Figure~\ref{all_tps}, \DASCALE achieves 950.43 TPS at 64 threads, outperforming \textit{WAR} and \textit{LRU-C} by 1.70$\times$ and 1.20$\times$, respectively. At 128 threads, it maintains 899.22 TPS, while \textit{WAR} and \textit{LRU-C} drop to 455.31 and 590.49 TPS, resulting in 1.98$\times$ and 1.52$\times$ improvements.
In YCSB (Figure~\ref{ops_small}), \DASCALE surpasses \textit{WAR} and \textit{LRU-C} by 2.97$\times$ and 2.53$\times$ in write-heavy workloads (RW:20/80), 2.14$\times$ and 1.30$\times$ in balanced workloads (RW:50/50), 5.43$\times$ and 1.44$\times$ in read-modify-write (RMW), and 2.10$\times$ and 1.15$\times$ in read-heavy workloads (RW:95/05).

\begin{comment}

As shown in Figure~\ref{all_tps}, at 64 threads \DASCALE achieves 950.43 TPS, outperforming \textit{WAR} (557.95 TPS) and \textit{LRU-C} (791.7 TPS) by 1.70$\times$ and 1.20$\times$, respectively. 
At 128 threads, \DASCALE sustains 899.22 TPS, while \textit{WAR} and \textit{LRU-C} drop to 455.31 TPS and 590.49 TPS, demonstrating 1.98$\times$ and 1.52$\times$ improvements. A similar trend is observed for OPS in Figure~\ref{ops_small}. 
In write-dominant workloads (RW:20/80), \DASCALE outperforms \textit{WAR} and \textit{LRU-C} by 2.97$\times$ and 2.53$\times$. At a balanced workload (RW:50/50), it outperforms \textit{WAR} and \textit{LRU-C} by 2.14$\times$ and 1.30$\times$. For read-modify-write (RMW), \DASCALE achieves 5.43$\times$ and 1.44$\times$ improvements over \textit{WAR} and \textit{LRU-C}. In read-dominant workloads (RW:80/20), it improves over \textit{WAR} and \textit{LRU-C} by 2.20$\times$ and 1.51$\times$, and in (RW:95/05) by 2.10$\times$ and 1.15$\times$.

    
\end{comment}


These results show the structural efficiency of \DASCALE.
This efficiency is enabled by distributing transactions across multiple database instances, which allows parallel execution without global contention. 
In contrast, \textit{LRU-C} enhances I/O parallelism by prioritizing clean pages and dividing locks to mitigate contention. 
However, this design reduces hit ratio and still relies on a shared buffer, which leads to new bottleneck points (e.g., flush bursts and mutex serialization). Similarly, \textit{WAR} alleviates read stalls by reversing the read-write order but still suffers from limited scalability because slow flush operations eventually accumulate, increasing latency and reducing throughput under a high number of threads.



%Thus, as the number of threads increases, both LRU-C and WAR encounter memory pressure and I/O stalls, ultimately degrading performance. 
%However, \DASCALE mitigates these issues by removing shared buffer dependencies entirely and isolating I/O operations within each database instance, which sustains higher throughput as concurrency scales.  




%\noindent\textbf{TPC-C:} Figure~\ref{all_tps} shows the throughput of MySQL, \DASCALE, and SOTA (LRU-C, WAR) with TPC-C benchmark on Server A and Server B. 
%We configured \DASCALE to its optimal instance setup: 12 instances (1 Master, 11 Slaves) on Server A and 16 instances (1 Master, 15 Slaves) on Server B.

%\skim{Should be seperate flow based on MySQL first (data + Reason) // SOTA second (Data + Reason)// rather than Server A and B??}


%On Server A, as shown in Figure~\ref{Smallserver_TPS}, MySQL performance decreases as the number of concurrent threads increases, demonstrating limited scalability due to increased contention. 
%In contrast, \DASCALE demonstrates TPS improvements of up to 28.06$\times$ (for 32 threads), a minimum of 3.57$\times$ (for 8 threads) compared to MySQL, and maintains high throughput as the number of concurrent threads increases. 
%At 64 threads, the SOTA databases LRU-C and WAR %achieve 791.7 TPS and 557.95 TPS, respectively. Both are outperformed by \DASCALE, which reaches 950.43 TPS, representing a 1.20$\times$ and 1.70$\times$ improvement.
%At 128 threads, LRU-C achieves 590.49 TPS and WAR achieves 455.31 TPS, which are still lower than the \DASCALE throughput of 899.22 TPS, demonstrating 1.52$\times$ and 1.98$\times$ improvements, respectively. 

%\textbf{For Server B need contents. }

%These results show the structural efficiency of \DASCALE.
%This efficiency is enabled by distributing transactions across multiple database instances, which allows parallel execution without global buffer contention. 
%In contrast, \textit{WAR} alleviates read stalls by reversing the read-write order but still suffers from limited scalability because slow flush operations eventually accumulate, increasing latency and reducing throughput under a high number of concurrent threads. Similarly, \textit{LRU-C} enhances I/O parallelism by prioritizing clean pages and dividing locks to mitigate contention. 
%However, this design reduces hit ratio and still relies on a shared buffer, which leads to new bottlenecks of global contention, such as flush bursts and mutex serialization. 
%Thus, as the number of threads increases, both LRU-C and WAR encounter memory pressure and I/O bottlenecks, ultimately degrading performance. 
%However, \DASCALE mitigates these issues by removing shared buffer dependencies entirely and isolating I/O operations within each database instance, which sustains higher throughput as concurrency scales. 




\begin{comment}
 %As shown in the figure, 
MySQL exhibits lower TPS than \DASCALE across all number of concurrent threads on both servers.
For Server A, as shown in Figure~\ref{Smallserver_TPS}, MySQL performance decreases as the number of concurrent threads increases, demonstrating limited scalability due to increased contention.
In contrast, \DASCALE demonstrates TPS improvements of up to 23.74$\times$ (for 64 threads), a minimum of 2.92$\times$ (for 8 threads), and an average of 15.14$\times$ when compared to MySQL on Server A. Additionally, \DASCALE's performance improves as the number of slave instances increases. When comparing TPS from 4 instances to 12 instances, the performance is enhanced by a maximum of 1.76$\times$ (for 32 threads), a minimum of 1.12$\times$ (for 8 threads), and an average of 1.46$\times$. 
This shows that \DASCALE effectively utilizes multiple database instances across CPU cores by adopting a distributed architecture. 
However, \DASCALE also shows limited scalability as the number of concurrent threads and instances increases beyond a certain point, with TPS peaking between 32 and 64 threads but decreasing at 128 threads due to contention among threads.
%In the case of MySQL, performance consistently declines after 8 threads. Initially, performance improves as the number of instances increases, but after reaching peak TPS with 12 instances, performance declines. 
This is because all instances share limited memory, and as more instances are added, the memory allocated to each decreases, leading to increased contention and lower TPS.  


For Server B, as shown in Figure~\ref{Largeserver_TPS}, \DASCALE records a maximum TPS of 1271.07 (6.65x that of MySQL) and a minimum of 187.24 (1.38x). In contrast, MySQL does not show TPS improvement as the number of concurrent threads increases, with a maximum TPS of 191.05 and a minimum of 135.53.
Compared to the results on Server A, the highest TPS on Server B is achieved with 400 threads. 
This is due to Server B's lower clock speed, which requires more threads to reach peak TPS. 
However, increasing the number of instances does not improve performance due to the increased memory contention. 
The highest TPS is reached with 16 instances, and performance declines with more than 24 instances. This shows that while a higher number of cores and lower clock speeds benefit from more threads, memory limitations restrict the scalability of instances.

\end{comment}

\begin{comment}

% Server B Data.
\begin{figure*}[t]
  \centering
  % 첫 번째 행
    \begin{subfigure}[b]{17.9cm}
        \includegraphics[width=\textwidth]{figures/lagend_largeserver.png}
           %\vspace{-0.5cm}
    \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_9505_latency.png}
    \caption{R/W:95/05\%.}
    \label{ycsb_9505_latency_B}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_8020_latency.png}
    \caption{R/W:80/20\%.}
    \label{ycsb_8020_latency_B}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_2080_latency.png}
    \caption{R/W:20/80\%.}
    \label{ycsb_2080_latency_B}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_5050_latency.png}
    \caption{R/W:50/50\%.}
    \label{ycsb_5050_latency_B}
  \end{subfigure}
  \begin{subfigure}[b]{3.45cm}
    \includegraphics[width=\textwidth]{figures/YCSB_ServerB_workloadF_latency.png}
    \caption{R-M/W:50/50\%.}
    \label{ycsb_f_latency_B}
  \end{subfigure}
      %\vspace{-0.3cm}
    \caption{99\% Latency (us) for \DASCALE and MySQL on per concurrent thread on Server B using YCSB.}
    \label{ops_large_latency}
        %\vspace{-0.3cm}
\end{figure*}

    
\end{comment}








%\noindent\textbf{YCSB:} Figures~\ref{ops_small} and~\ref{ops_large} show the throughput with YCSB benchmark. 
%As shown in both figures, \DASCALE demonstrates superior OPS across all R/W ratios, following a trend similar to TPS. 
%Specifically, for Server A, in read-dominant operations of 95/5\% and 80/20\%, \DASCALE reached the maximum of 144,784 OPS and 118,899 OPS (both at 64 threads), demonstrating improvements of 2.10$\times$ and 1.15$\times$ over WAR and LRU-C for 95/5\%, and 2.20$\times$ and 1.51$\times$ for 80/20\%, respectively. 
%In write-dominant operations of 20/80\%, \DASCALE achieved a maximum of 94,144 OPS (for 64 threads), showing 2.97$\times$ and 2.53$\times$ higher performance compared to WAR and LRU-C. 
%At a balanced 50/50\% R/W ratio, \DASCALE achieved a maximum of 102,378 OPS (for 64 threads), surpassing WAR and LRU-C by 2.14$\times$ and 1.30$\times$, respectively. 
%Finally, at a 50/50\% R-M/W ratio, \DASCALE achieved 69,077 OPS (for 64 threads), outperforming WAR and LRU-C by 5.43$\times$ and 1.44$\times$, respectively. 



%\changjong{ 
%\textbf{For server B need contents. }

%}


%\skim{Reasoning?? focusing on YCSB being more complex benchmark and compare the performace when using TPC-C? I think many previous reviews from VLDB and EuroSys mentioned this.}


\begin{comment}
 Figures~\ref{ops_small} and~\ref{ops_large} show the throughput of MySQL and \DASCALE on Server A and Server B. 
As shown in both figures, \DASCALE demonstrates superior OPS compared to MySQL across all R/W ratios, following a trend similar to TPS.
Specifically, for Server A, in read-dominant operations of 95/5\% and 80/20\%, \DASCALE reached the maximum of 131,157.24 OPS (for 64 threads) and 126,620.74 OPS (for 64 threads), respectively, demonstrating performance improvements of up to 11.15$\times$ and 74.11$\times$ compared to MySQL. In write-dominant operations of 20/80\%, \DASCALE achieved a maximum of 94,144.22 OPS (for 64 threads), resulting in a performance improvement of 264.17$\times$ compared to MySQL. 
At a 50/50\% R/W ratio, \DASCALE achieved a maximum of 102,378.24 OPS(for 64 threads), showing performance improvements of up to 47.95$\times$ and a minimum of 2.12$\times$ compared to MySQL. At a 50/50\% R-M/W ratio, it reached a maximum of 74,036.78 OPS (for 64 threads), showing improvements of up to 139.9$\times$ and a minimum of 2.15$\times$ over MySQL.   
\end{comment}


\begin{comment}
 For Server B, in read-dominant operations of 95/5\% and 80/20\%, \DASCALE reached the maximum of 71,336.34 OPS (for 400 threads) and 71,069.16 OPS (for 400 threads), respectively, demonstrating performance improvements of up to 10.07$\times$ and 5.44$\times$ compared to MySQL. In write-dominant operations of 20/80\%, it achieved a maximum of 67,225.98 OPS (for 400 threads), resulting in a performance improvement of 18.57$\times$ over MySQL. At a 50/50\% R/W ratio, \DASCALE reached a maximum of 68,650.19 OPS (for 400 threads), showing performance improvements of up to 12.86$\times$ and a minimum of 1.49$\times$ compared to MySQL. At a 50/50\% R-M/W ratio, it achieved a maximum of 39,531.16 OPS (for 128 threads), with a performance improvement of up to 4.80$\times$ over MySQL.   
\end{comment}




% Server B - YCSB OPS

\subsubsection{Latency}~\label{eval_latency}

Figure~\ref{latency_small} shows the 99\% latency of MySQL, PostgreSQL, SOTA (\textit{WAR}, \textit{LRU-C}), and \DASCALE with TPC-C and YCSB. 
We configured \DASCALE to its optimal instance setup: 12 instances (1 Master, 11 Slaves).

\noindent\textbf{Compared to MySQL and PostgreSQL: } Figure~\ref{latency_small} shows MySQL latency sharply rises with concurrency, reaching 3,522 ms at 64 threads and 4,398 ms at 128 threads. PostgreSQL lowers this to 690 ms and 631 ms, thanks to MVCC and efficient WAL processing. \DASCALE further reduces latency to 135 ms (96.16\% lower than MySQL, 80.40\% lower than PostgreSQL) at 64 threads and 180 ms at 128 threads, maintaining low latency. 
In YCSB, it shows similar trends: \DASCALE reduces latency by over 99.87\% vs. MySQL and over 94.90\% vs. PostgreSQL in write- and read-heavy workloads and maintains more than 85.04\% latency reduction across workloads even at 64 threads.

\begin{comment}
    
\changjong{As shown in Figure~\ref{latency_small}, MySQL latency increases sharply as concurrency grows, reaching 3,522.04 ms at 64 threads and 4,398.23 ms at 128 threads. PostgreSQL significantly lowers latency compared to MySQL (690.66 ms and 631.30 ms at the same scales) due to MVCC-based concurrency control and more efficient WAL processing. However, \DASCALE further reduces latency to 135.08 ms (96.16\% reduction from MySQL and 80.40\% from PostgreSQL) and 179.92 ms (95.91\% and 71.50\% reduction, respectively), demonstrating its ability to sustain low latency. A similar trend is observed in the YCSB benchmark. At 128 threads, \DASCALE reduces latency by 99.12\% compared to MySQL and by 96.90\% compared to PostgreSQL in write-intensive workloads (20/80), and by 99.87\% and 94.90\%, respectively, in read-heavy workloads (95/05). Even at 64 threads, it consistently achieves more than 97.20\% lower latency than MySQL and over 85.04\% lower latency than PostgreSQL across all workloads.}
\end{comment}


MySQL shows sharp latency growth as threads increase due to serialization on centralized resources and synchronous WAL flush–induced I/O stalls. PostgreSQL lowers latency via MVCC and group commit, but still spikes under high contention because WAL writing and buffer eviction serialize in the background, causing periodic I/O stalls. In contrast, \DASCALE contains tail latency by distributing requests across independent slaves and overlapping background flush with parallel transactions, yielding a non-blocking execution model that avoids such spikes.



\noindent\textbf{Compared to \textit{WAR}, and \textit{LRU-C}: } Figure~\ref{latency_small} shows \DASCALE at 135.08 ms with 64 threads versus \textit{WAR} at 260.48 ms and \textit{LRU-C} at 1,423.93 ms; at 128 threads, \DASCALE holds 179.92 ms while \textit{WAR} and \textit{LRU-C} rise to 677.06 ms and 1,512.46 ms, indicating robust latency under high contention. In \textit{WAR}, the temporary dirty page buffer that prevents read stalls saturates under heavy writes and stalls foreground threads until flushing completes; in \textit{LRU-C}, frequent clean to dirty transitions cause pointer update overhead and many small fragmented flushes, both elevating latency. Even at 8 threads, \textit{WAR} and \textit{LRU-C} (288.27 ms, 161.48 ms) exceed \DASCALE (55.91 ms) and MySQL (85.10 ms) because their added mechanisms help mainly under contention. \DASCALE uses single master coordination without complex shared structures, keeping low thread latency close to MySQL. In YCSB at 128 threads, \DASCALE achieves its largest latency gains in read heavy 95/5, remains competitive in 80/20, 50/50, and RMW, and also improves 20/80, consistently outperforming \textit{WAR} and \textit{LRU-C} with fewer spikes.




\subsection{Performance Under Data Skew}~\label{skew_evel}

\begin{figure}[!t]
    \centering
    \includegraphics[width=8.5cm]{figures/skewness.png}
    %\vspace{-0.7cm}
    \caption{OPS across different read/write ratios and data skew (Legend: Hotpot data fraction, Peak: OPS with a non-skewed workload).}    
    \label{skewness}
\end{figure}




Figure~\ref{skewness} evaluates skewed YCSB via hotspot distributions, varying (\textit{hotspotdatafraction}, \textit{hotspotopnfraction}) as (0.05, 0.95) and (0.01, 0.99). The Zipfian default (“Peak”) is the baseline in Section~\ref{eval_throughput}.

\noindent\textbf{Read-Intensive:} Under 95/5 with extreme skew (0.99), \DASCALE reaches 84,615 OPS, 2.35$\times$ over \textit{WAR} (35,965) and 1.46$\times$ over \textit{LRU-C} (58,085); at 80/20, it sustains 81,412 OPS, 2.18$\times$ and 2.70$\times$ over \textit{WAR} and \textit{LRU-C}. \DASCALE routes reads via the master \textit{task scheduler} and a global version table, so only relevant slaves serve the single correct snapshot, avoiding global contention, whereas monolithic systems contend on shared buffer metadata.

\noindent\textbf{Write-Intensive:} At 20/80, \DASCALE delivers 73,728 OPS, 3.55$\times$ and 3.33$\times$ over \textit{WAR} and \textit{LRU-C}, because monolithic designs serialize commits on a centralized WAL and contend on the shared buffer; \DASCALE dispatches long I/O asynchronously across slaves, each flushing its own WAL.

\noindent\textbf{Balanced-RW:} At 50/50 with skew 0.99, \DASCALE achieves 80,182 OPS, exceeding \textit{WAR} (30,965) and \textit{LRU-C} (38,230) by 2.59$\times$ and 2.10$\times$; in the RMW case, \DASCALE sustains 49,501 OPS (5.04$\times$ over \textit{WAR}’s 9,812 and 1.89$\times$ over \textit{LRU-C}’s 26,185), retaining over 71\% of its Peak (69,077 to 49,501), while \textit{LRU-C} drops to 55\% (47,791 to 26,185). By localizing locks within slaves and eliminating global serialization, \DASCALE outperforms component-level SOTA under hotspots.


\begin{comment}


Figure~\ref{skewness} shows the impact of skewed YCSB workloads on SOTA (\textit{WAR}, \textit{LRU-C}) and \DASCALE.
To simulate skewness, the hotspot distribution is used, where the fraction of data items in the hot set and the fraction of operations targeting the hot set are tuned.
Specifically, we vary (\textit{hotspotdatafraction}, \textit{hotspotopnfraction}) as (0.05, 0.95) and (0.01, 0.99). 
The baseline is the Zipfian distribution (default), marked as "Peak", whose performances have already been presented in Section~\ref{eval_throughput}. 


\noindent\textbf{Read-Intensive:} In the 95/5 workload with extreme skew (0.99), \DASCALE achieves 84,615 OPS, which is 2.35$\times$ and 1.46$\times$ that of \textit{WAR} (35,965 OPS) and \textit{LRU-C} (58,085 OPS), respectively. A similar trend appears in the 80/20 workload, where \DASCALE sustains 81,412 OPS, reaching 2.18$\times$ and 2.70$\times$ that of \textit{WAR} and \textit{LRU-C}. This is because \DASCALE processes reads across independent instances, and the master routes requests through \textit{task scheduler} and resolves them via the global version table, allowing only the relevant slaves to respond with the correct snapshot without global contention. In contrast, monolithic systems are bottlenecked by contention on shared buffer metadata, as even read operations require locking for buffer management.

\noindent\textbf{Write-Intensive:} The performance gap widens significantly in the write-intensive 20/80 workload. \DASCALE achieves 73,728 OPS, which is 3.55$\times$ and 3.33$\times$ that of \textit{WAR} and \textit{LRU-C}, respectively. This is because monolithic systems rely on a centralized WAL that serializes all commit operations, causing global contention on the shared buffer.
These results show that the architectural approach of \DASCALE can outperform component-level optimizations that target specific shared data structures or operations.
By using a single master to distribute long I/O tasks for asynchronous, parallel processing across slaves, each of which flushes its own WAL, the system achieves superior performance.

\noindent\textbf{Balanced-RW:} Even in the balanced 50/50 workload, \DASCALE shows 80,182 OPS under extreme skew (0.99), surpassing \textit{WAR} (30,965 OPS) and \textit{LRU-C} (38,230 OPS) by 2.59$\times$ and 2.10$\times$, respectively. The difference is most pronounced in high-contention scenario, RMW workload, where data lock contention and WAL overhead are the most dominant. 

\DASCALE achieves a throughput of 49,501 OPS under extreme skew (0.99), which is 5.04$\times$ and 1.89$\times$ that of \textit{WAR} (9,812 OPS) and \textit{LRU-C} (26,185 OPS), respectively. Notably, \DASCALE maintains over 71\% of its "Peak" performance (dropping from 69,077 to 49,501 OPS). In contrast, \textit{LRU-C}'s performance collapses to just 55\% of its peak (from 47,791 to 26,185 OPS), highlighting the severe impact of hotspots on monolithic systems. \DASCALE localizes data lock contention within individual slaves, avoiding the global serialization that limits a monolithic system.
    
\end{comment}




%Even in a balanced 50/50 workload with extreme skew (0.99), \DASCALE shows 80,182 OPS, surpassing surpassing \textit{WAR} (30,965 OPS) and \textit{LRU-C} (38,230 OPS) by 2.59$\times$ and 2.10$\times$, respectively. 
%This advantage is most significant in the RMW workload, where lock contention is highest.
%\DASCALE achieves 49,501 OPS under extreme skew (0.99), which is 5.04$\times$ and 1.89$\times$ higher than \textit{WAR} (9,812 OPS) and \textit{LRU-C} (26,185 OPS).  
%Notably, \DASCALE maintains over 71\% of its peak performance (with less skewed zipfian workload) under extreme skew. In contrast, the performance of \textit{LRU-C} collapses to just 55\% of its peak (from 47,791 to 26,185 OPS), which highlights the severe impact of hotspots on monolithic systems. \DASCALE avoids this by localizing data lock contention within individual slaves, which prevents the global serialization that limits a monolithic system.







\subsection{Lock Contention and I/O Stall}~\label{systemoverhead}
%%\vspace{-0.1cm}

Figures~\ref{overhead_lock} and~\ref{overhead_write} analyze MySQL and \DASCALE under TPC-C (64 threads, fixed 100s) to show how \DASCALE mitigates the Section~\ref{background} bottlenecks, with MySQL using a 100\% buffer pool and \DASCALE using a 12‑instance setup.



\noindent\textbf{Lock Contention:} As shown in Figure~\ref{lockcount}, \DASCALE issues far fewer locks (5.77$\times$ fewer at 8 threads, 3.43$\times$ fewer at 128 threads) and holds them for less time (103.06s vs 34.85s total at 128 threads), with shorter per-lock time (2.01$\times$ at 8 threads, 1.55$\times$ at 128 threads). MySQL contends on globally shared structures (e.g., Flush list, LRU list) guarded by a single mutex; \DASCALE uses \texttt{Tx Table} to route to target instances, where slaves execute independently with per-instance WAL flush and row-level locks, confining contention to the touched rows and avoiding global bottlenecks.


\begin{comment}
\noindent\textbf{Lock Contention:} As shown in Figure~\ref{lockcount}, \DASCALE dramatically reduces the total number of lock attempts compared to MySQL, using 5.77$\times$ fewer locks at 8 threads and 3.43$\times$ fewer at 128 threads.
This efficiency extends to lock duration. 
For total lock time (Figure~\ref{locktime}), MySQL holds locks for 103.06 seconds at 128 threads, while \DASCALE requires only 34.85 seconds. 
The average time per lock is also significantly shorter (Figure~\ref{perlocktime}), with \DASCALE achieving a 2.01$\times$ improvement at 8 threads and a 1.55$\times$ improvement at 128 threads.    
\end{comment}



These results show the structural efficiency of \DASCALE. 
MySQL struggles under high concurrency because all threads need to compete when accessing globally shared structures, such as the Flush list and LRU buffer list, which is protected by a single mutex.
In contrast, \DASCALE avoids this bottleneck by using \texttt{Tx Table} to locate target instances. Each slave then executes transactions independently, managing its own WAL flushes and row-level locks. 
Although locking is still required, it is confined to the target row of the page within each instance, preventing global contention.



\begin{figure}[t]
    \centering
    % Legend figure
    \includegraphics[width=8.5cm]{figures/legend_lock.png}
    \vspace{-0.8cm}

    %  데이터 3개
    \subfloat[Total lock count.]{%
        \includegraphics[width=2.75cm]{figures/lock count.png}
        \label{lockcount}
    }%
    \subfloat[Total lock time.]{%
        \includegraphics[width=2.7cm]{figures/total lock.png}
        \label{locktime}
    }%
    \subfloat[Per-lock time (log-scale).]{%
        \includegraphics[width=2.80cm]{figures/per-lock.png}
        \label{perlocktime}
    }%

    \caption{Lock Overhead of MySQL and \DASCALE.}
    \label{overhead_lock}
    %\vspace{-0.3cm}
\end{figure}



\begin{figure}[t]
    \centering
    %  Legend figure
    \includegraphics[width=4.5cm]{figures/lagend_overhead.png}
    %\vspace{-0.3cm}

    %  데이터 3개
    \subfloat[Total write count.]{%
        \includegraphics[width=2.75cm]{figures/write count.png}
        \label{writecount}
    }%
    \subfloat[Total write size.]{%
        \includegraphics[width=2.75cm]{figures/write size.png}
        \label{writesize}
    }%
    \subfloat[Total fsync count.]{%
        \includegraphics[width=2.75cm]{figures/fsync count.png}
        \label{fsynccount}
    }%

    \caption{I/O Stall of MySQL and \DASCALE.}
    \label{overhead_write}
    %\vspace{-0.3cm}
\end{figure}




\noindent\textbf{I/O Stall:} As shown in Figure~\ref{writecount}, \DASCALE issues fewer writes than MySQL at 128 threads (11,479 vs 13,674), and each call carries more data, reducing stall frequency. For example, at 64 threads the total write size is 8462.29 MB for \DASCALE versus 2738.87 MB for MySQL (3.09$\times$), reflecting higher throughput with fewer calls (Figure~\ref{writesize}). 
For durability, MySQL performs 25,662 fsync calls at 128 threads while \DASCALE requires 2,585, nearly 10$\times$ fewer (Figure~\ref{fsynccount}). These results indicate that \DASCALE sustains higher throughput by avoiding frequent small writes and fsync calls that trigger I/O stalls under high concurrency.


\begin{comment}
 \noindent\textbf{I/O Stall:} As shown in Figure~\ref{writecount}, \DASCALE issues fewer write calls than MySQL, using 11,479 calls compared to 13,674 at 128 threads.
This efficiency in write calls alleviates I/O stalls, as each call carries a larger amount of data. 
For example, the total write size (Figure~\ref{writesize}) shows that \DASCALE writes 8462.29 MB compared to 2738.87 MB in MySQL at 64 threads, a 3.09$\times$ increase that reflects higher throughput, as \DASCALE shows a larger volume of data being written with fewer calls.
For durability (Figure~\ref{fsynccount}), MySQL performs 25,662 fsync calls at 128 threads, while \DASCALE requires only 2,585, nearly 10$\times$ fewer.
Therefore, these results show that \DASCALE sustains higher throughput by reducing frequent write and fsync operations that trigger I/O stalls, thereby lowering overhead under high concurrency.   
\end{comment}




\begin{figure}[t]
    \centering
    \includegraphics[width=8.5cm]{figures/smallserver_instance.png}
    \caption{TPS of \DASCALE with varying number of instances (1 Master and all slaves).}
    \label{overall-tps}
\end{figure}


\subsection{Scalability}~\label{scale_eval}

\noindent\textbf{Threads and Instance Scalability: } Figure~\ref{overall-tps} reports \DASCALE throughput on TPC-C, peaking at 950.02 TPS with 12 instances (a 1.59$\times$ gain over 4 instances) and exhibiting diminishing returns thereafter. 
TPS also scales with threads up to between 32 and 64 but falls at 128 for all instance counts, reflecting intra‑instance contention when multiple threads share an instance. Increasing instances improves performance until 12, beyond which shared memory limits reduce per‑instance capacity and amplify master coordination overhead, yielding flat or reduced TPS.





\begin{figure}[!t]
    \centering
    % --- Legend 그림 추가 ---
    \includegraphics[width=8.5cm]{figures/legend_cpu.png} % ← 범례 전용 그림 (따로 저장해둔 파일)
    \vspace{-0.4cm}
    
    % --- 첫 번째 그림 ---
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/16util.png}
        \caption{Threads: 16.}
        \label{CPU_Utilization_1}
    \end{subfigure}
    \hfill
    % --- 두 번째 그림 ---
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/64util.png}
        \caption{Threads: 64.}
        \label{CPU_Utilization_2}
    \end{subfigure}
    
    %\vspace{-0.3cm}
    \caption{CPU utilization of 12 Instances (1 Master and 11 Slaves) running TPC-C.}
    \label{CPU_Utilization}
\end{figure}






\noindent\textbf{CPU Utilization: } Figure~\ref{CPU_Utilization} reports CPU usage for \DASCALE (master and 11 slaves) on TPC-C with 16 and 64 threads. At 16 threads (Figure~\ref{CPU_Utilization_1}), slaves average 43.18\% and the master 62.4\%, indicating underutilization; at 64 threads (Figure~\ref{CPU_Utilization_2}), slaves rise to 67.22\% as the master effectively distributes work, but the master averages 78.61\% with peaks near 98\%, revealing a coordination bottleneck. This pattern shows the strengths and limits of \DASCALE design: slaves parallelize well, boosting throughput and CPU use, but a single coordinator caps scalability once its CPU saturates. 
However, we also think that moving to multi-master or multiprocessing would add substantial complexity per Section~\ref{motivation} without guaranteeing relief; \DASCALE therefore opts for a simple single-master to remove lock contention on a single manycore node. Future work will explore partitioning the master role by key ranges or distributing coordination with consensus algorithms.






\subsection{Overhead of Global Checkpoints}~\label{GCP_evel}


\begin{figure}[!t]
    \centering

    % --- 첫 번째 그림 ---
    \begin{subfigure}[t]{4cm} % ✅ cm 단위
        \centering
        \includegraphics[width=\linewidth]{figures/GCP_creation_TPS.png}
        \caption{GCP Interval.}
        \label{GCP_creation_1}
    \end{subfigure}
    % --- 두 번째 그림 ---
    \begin{subfigure}[t]{4.2cm} % ✅ cm 단위
        \centering
        \includegraphics[width=\linewidth]{figures/GCP_recovery.png}
        \caption{Takeover and Recovery.}
        \label{GCP_creation_2}
    \end{subfigure}
    
    \caption{Overhead of global checkpoints.}
    \label{GCP_creation}
\end{figure}


Figure~\ref{GCP_creation} shows the trade-off between throughput and recovery cost as the global checkpoint interval increases from 2,000 ms to 8,000 ms. Experiments were run after the system generated over 3,000 GCPs to ensure stable performance without warm-up effects.

\noindent\textbf{Impact of GCP Interval:} As shown in Figure~\ref{GCP_creation_1}, extending the GCP interval from 2000 ms to 4000 ms slightly improves throughput, reaching a peak of 982.18 TPS at 64 threads, a 1.03$\times$ increase over 950.43 TPS at 2000 ms due to reduced checkpoint frequency. However, increasing the interval further to 8000 ms degrades performance to 884.82 TPS, a 10\% drop from the peak. This decline results from the accumulation of more dirty pages and metadata, causing heavier disk flushes and longer serialization, which increase contention with transaction processing.


\noindent\textbf{Impact of Recovery:} Figure~\ref{GCP_creation_2} shows that longer GCP intervals significantly increase takeover and recovery times. Takeover, the process of switching from a failed master to an active slave instance, includes slave promotion, coordination, and data recovery, and its runtime increases 1.59$\times$ (from 22.78s at 2000 ms to 36.21s at 8000 ms). Recovery restores a single logical checkpoint (LCP) failure by replaying all transactions since the last checkpoint, causing runtime to grow 2.84$\times$ (from 13.08s to 37.14s) due to increased volume of WAL logs and LCPs, which raises both I/O (log scanning, page loading) and CPU (log parsing, redo) overhead. While takeover time increases as the interval grows, it rises more slowly than recovery time. At 8000 ms, recovery dominates the total takeover runtime, making coordination overhead negligible.



\begin{comment}
 \changjong{Figure~\ref{GCP_creation_2} shows that longer GCP intervals significantly increase both takeover and recovery times. The takeover runtime increases by about 1.59$\times$ from 22.78s at 2000 ms to 36.21s at 8000 ms, and recovery time grows even more sharply by about 2.84$\times$ from 13.08s to 37.14s. This is because the recovery process must replay all transactions that occurred since the last checkpoint. With longer intervals, the volume of WAL logs and LCPs to process grows proportionally, increasing both I/O overhead (e.g., log scanning and page loading) and CPU overhead (e.g., log parsing and redo). While the takeover runtime, which includes slave promotion, coordination, and data recovery, also increases as the GCP interval becomes longer, it shows a slower growth compared to the recovery time. At the 8000 ms interval, the recovery phase dominates the total takeover runtime, rendering the additional coordination overheads relatively negligible.   
\end{comment}









\subsection{Monolithic vs. Multi-Instance Performance}~\label{variousdb}


\begin{figure}[!t]
    \centering

    % ---------- Legend (상단 전체 폭) ----------
    \includegraphics[width=8.5cm]{figures/legend_various.png}
    \vspace{-0.4cm}

    % ---------- 왼쪽 서브그림 ----------
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/various_database.png}
        \caption{TPS.}
        \label{varioustps}
    \end{subfigure}
    \hfill
    % ---------- 오른쪽 서브그림 ----------
    \begin{subfigure}[t]{0.50\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/interval_avg_latency.png}
        \caption{Latency by time interval (128 threads).}
        \label{variouslatency}
    \end{subfigure}

    \vspace{-0.3cm}
    \caption{TPS and Latency (Avg) of various databases.}
    \label{various_db}
\end{figure}




Figure~\ref{various_db} compares throughput (TPS) and average latency at 128 threads between monolithic and non-monolithic databases, highlighting how centralized resource sharing and decentralized data partitioning affect scalability on manycore systems.


\noindent\textbf{Monolithic: }Figure~\ref{varioustps} shows that monolithic systems have limited scalability due to contention for centralized resources. For example, MySQL performance collapses beyond 8 threads because of lock contention in a shared-memory model. SOTA optimizations (\textit{WAR} and \textit{LRU-C}) improve this by addressing lock contention and I/O stalls, but still eventually decline, indicating inherent limits in centralized designs. PostgreSQL scales better than MySQL, peaking at 799.75 TPS, but contention on the buffer cache and background writer limits further growth.




\noindent\textbf{Non-Monolithic: }In contrast, non-monolithic systems reduce contention through distributed transaction processing, yielding superior scalability. \textit{MongoDB} scales horizontally via sharding and document-level locking, reaching 501.15 TPS with multiple instances on a single node before hitting its limits.

\textit{VoltDB}, an in-memory database, uses a single-threaded partition model to eliminate intra-partition locking, reaching 178,203 TPS on a single thread—two orders of magnitude faster by avoiding disk I/O. 
However, throughput collapses under multi-threading, falling to 923 TPS at 128 threads due to costly coordination for multi-partition transactions and poor scaling on synchronization-heavy queries; latency also degrades (Figure~\ref{variouslatency}), exceeding 3200 ms at 128 threads, 15.7$\times$ higher than \DASCALE (203 ms) and 1.7$\times$ higher than PostgreSQL (1842 ms). 
In contrast, \DASCALE is a disk-based, shared-nothing system on shared memory that partitions transactions across independent instances with single-master coordination, thereby avoiding global lock contention; although its peak is 950.43 TPS, it sustains superior single-node scalability with disk flushes, in contrast to \textit{VoltDB}’s in-memory-only operation.


\begin{comment}
\changjong{
\textit{VoltDB}, an in-memory database, uses a single-threaded partition model to eliminate intra-partition locking. 
This design allows it to achieve 178,203 TPS with a single thread, two orders of magnitude faster than other databases.
This is expected because, as an in-memory database, \textit{VoltDB} bypasses the performance overhead from writing to slow, persistent storage. 
However, its performance collapses under multi-threaded workloads, dropping to 923 TPS at 128 threads. 
This decline stems from costly coordination for multi-partition transactions, and because it is optimized for scale-out environments, \textit{VoltDB} scales poorly on complex queries that require coordination across partitions and frequent synchronization. This degradation is also reflected in latency as shown in Figure~\ref{variouslatency}: at 128 threads, the latency of \textit{VoltDB} exceeds 3200 ms (at 30-40 sec), which is 15.7$\times$ higher than that of \DASCALE (203 ms) and 1.7$\times$ higher than that of PostgreSQL (1842 ms), demonstrating that coordination overhead severely impacts tail performance at scale. 

In contrast, \DASCALE is a disk-based system with a shared-nothing on shared-memory architecture. It partitions transactions across independent instances under single-master coordination, bypassing the global lock contention that limits monolithic systems. 
Thus, while its peak throughput of 950.43 TPS is lower than that of \textit{VoltDB}, \DASCALE provides superior and sustained single-node scalability. This scalability is achieved with disk flushes, which contrasts with the in-memory-only operations of \textit{VoltDB}.
}    
\end{comment}





