\section{Background} \label{background}


\subsection{Bottlenecks in Database I/O Path}~\label{background_buffer}

\begin{figure}[!t]
    \centering
    \includegraphics[width=9cm]{figures/buffer_design.pdf}
    \caption{Buffer Management and I/O Operation. }
    \label{db_buffer}
\end{figure}


Since the essence of a database lies in storing and fetching data, the efficient utilization of memory tiers such as DRAM and storage is critical.
To achieve this, database systems employ buffer caches to reduce the overhead of disk access by keeping recently used data in limited, high-speed memory. The buffer cache is typically managed by a Least Recently Used (LRU) list. Because every transaction references this list, it requires lock-based synchronization, which creates a major bottleneck under concurrent access from multiple threads.


Figure~\ref{db_buffer} shows the buffer management architecture in MySQL. As depicted, the buffer cache maintains pages in the free list, flush list, and main LRU list. Each list is protected by a global mutex, and under high concurrency, these mutexes can cause contention, serialization, and reduced throughput~\cite{an2022avoiding, freitag2022memory, leis2018leanstore, johnson2009shore, yuan2025twcache, lee2023lru, lee2025boosting, wu2019multiple, an2023nv}.
Consequently, this design introduces three primary bottlenecks: (1) serialized access to the centralized LRU list, (2) contention on the flush list, and (3) stalls caused by Write-Ahead Logging (WAL) dependencies.


\noindent\textbf{Centralized LRU List Access:} As shown in the figure, searching for a clean page requires a full iteration of the LRU list (\ding{183}). 
This operation acquires the global mutex protecting the centralized LRU list to prevent concurrent access.
Additionally, moving or removing a page also requires acquiring the same mutex. Consequently, all threads contend for this lock, creating a serialized execution path that limits scalability under high concurrency.

Prior studies have addressed this issue by exploring fine-grained or partitioned LRU lists and lock-free data structures to reduce contention~\cite{lee2023lru, an2022avoiding, wu2019multiple, lee2025boosting, an2023nv, an2022your}. However, these methods present drawbacks: they increase maintenance overhead, do not fully resolve the complexity of synchronizing a large shared structure, and still require coordination for global operations such as page promotion and eviction.
 

\noindent\textbf{Flush List Contention:} As shown in the figure, when no clean page is available (\ding{183}), the database selects a dirty page from the tail of the LRU list and initiates a flush operation via the flush list. 
The associated WAL records are first written to the WAL log area to guarantee durability (\ding{184}\textbf{-1}), followed by flushing the dirty page to the transaction data area (\ding{184}\textbf{-2}). 
While the LRU list manages candidates for page replacement, the flush list separately tracks dirty pages pending writeback to storage. Although disk I/O for flushing individual pages can occur in parallel, the flush list is guarded by a global mutex. This lock serializes all modifications on the flush list, creating a bottleneck that impairs overall parallelism. The resulting serialization delays allocation of clean pages and stalls foreground transactions under write-heavy workloads (\ding{185}).



To alleviate this bottleneck, prior works have proposed various solutions spanning the system stack, including I/O path parallelization within the buffer manager~\cite{lee2023lru, an2022avoiding, an2022your}, novel hardware I/O commands to reduce flush latency, and offloading flush operations to remote memory tiers to remove slow I/O from critical paths~\cite{zhang2021towards, ruan2023persistent, park2017new, sun2014sea, lee2025boosting, an2023nv}. 
Nevertheless, these approaches often require specialized hardware support or introduce considerable architectural complexity.


\noindent\textbf{WAL-Dependent Eviction Delays:} As shown in the figure, before a dirty page can be evicted, the corresponding Write-Ahead Log (WAL) records up to the LSN of the page must be flushed to persistent storage to ensure durability (\ding{184}\textbf{-1}). Page eviction depends on WAL persistence, so any delay in flushing the WAL, such as \textit{fsync} contention on the WAL file, stalls eviction until all necessary log records are safely persisted (\ding{185}). Once the WAL file is persisted, the dirty page is written to the operating system page cache using \textit{pwrite}, and the kernel subsequently flushes this data to disk asynchronously.

To address this flush overhead, some research eliminates the need to persist log buffers directly in non-volatile memory~\cite{an2023nv, lee2025boosting, koutsoukos2021use, katzburg2018nvdimm, dulong2021nvcache}. Other approaches reduce synchronous I/O and improve multicore scalability by employing lock-free logging mechanisms that mitigate contention in centralized log buffers~\cite{huang2022removing, kim2019border, wang2024lavastore, nguyen2025moving, xia2020taurus, oh2020demeter}.
However, these solutions often require specialized hardware support and focus narrowly on optimizing the logging component, leaving other database subsystems unoptimized and limiting holistic performance gains.



\subsection{Database for Manycore System}~\label{background_manycore}


Due to hardware limitations, server systems have evolved from single-core to multicore and now to manycore architectures~\cite{cerrolaza2020multi, vajda2011multi, manferdelli2008challenges, gupta2021mapping, markall2013finite, peccerillo2022survey}. Manycore systems contain a large number of distributed CPUs across multiple physical sockets, enabling extreme parallelism~\cite{orenes2022tiny, bang2022full, zhang2020optimizing, fang2020parallel, cho2022dopia}. This architectural complexity demands new software designs that can efficiently exploit parallelism while managing inter-core communication and synchronization overhead. Existing software, however, faces substantial challenges in parallelization and synchronization, limiting scalability on modern manycore hardware. For example, conventional relational databases rely on coarse-grained locking to protect shared data and critical sections. This blocks concurrent access to shared data structures by multiple processes running on different CPUs. As a result, it causes severe lock contention, which has become a major bottleneck in database performance on manycore systems~\cite{kim2019border, caibonsaikv, fuchs2022sortledton, 264834}.


Prior work has primarily targeted optimizing the internals of a database system on manycore hardware within a single node. Techniques such as fine-grained locking, lock-free data structures~\cite{an2022avoiding, lee2023lru, an2022your, kim2019border, caibonsaikv, fuchs2022sortledton, wu2019multiple, lee2025boosting}, and NUMA-aware optimizations~\cite{leis2018leanstore, li2022numa, liu2023zen+, wagle2015numa, hong2019scaling} aim to reduce lock contention and minimize global synchronization. While these methods improve scalability, shared resources still necessitate centralized coordination, which limits scalability. Other efforts have explored scaling databases across multiple nodes~\cite{yang2023oceanbase, 6313694, loesing2015design, zhang2024towards, arnold2019high, chen2024tdsql, stonebraker2013voltdb, kallman2008h, lam2024accelerating}. By distributing transactions across nodes, they reduce contention within a single server and improve aggregate throughput. However, distributed architectures introduce added complexity in data partitioning, transaction coordination, and ensuring consistency under high concurrency.



\renewcommand{\arraystretch}{1.3} % 줄 높이 통일 (1.2~1.4 권장)
\begin{table}[!t]
  \captionsetup{font=small, textfont=sc, labelsep=period, justification=centering}
  \caption{Adjusted MySQL Configuration Parameters.}\label{innodb_params}
  \centering
  \footnotesize
  \begin{tabular}{|>{\centering\arraybackslash}p{0.19\columnwidth}|
                  >{\centering\arraybackslash}p{0.42\columnwidth}|
                  >{\centering\arraybackslash}p{0.10\columnwidth}|
                  >{\centering\arraybackslash}p{0.11\columnwidth}|}

    \hline
    \textbf{Category} & \textbf{Parameter} & \textbf{Default} & \textbf{Modified} \\ \hline \hline

    \multirow{6}{*}{\textbf{Concurrency}} 
    & \texttt{innodb\_thread\_concurrency} & 0 & 32 \\ \cline{2-4}
    & \texttt{innodb\_buffer\_pool\_instances} & 8 & 16 \\ \cline{2-4}
    & \texttt{innodb\_read\_io\_threads} & 4 & 8 \\ \cline{2-4}
    & \texttt{innodb\_write\_io\_threads} & 4 & 8 \\ \cline{2-4}
    & \texttt{innodb\_lru\_scan\_depth} & 1024 & 4096 \\ \hline

    \multirow{6}{*}{\textbf{Flush efficiency}} 
    & \texttt{innodb\_flush\_neighbors} & 1 & 0 \\ \cline{2-4}
    & \texttt{innodb\_io\_capacity} & 200 & 2000 \\ \cline{2-4}
    & \texttt{innodb\_io\_capacity\_max} & 2000 & 6000 \\ \cline{2-4}
    & \texttt{innodb\_max\_dirty\_pages\_pct} & 30 & 75 \\ \cline{2-4}
    & \texttt{innodb\_max\_dirty\_pages\_pct\_lwm} & 0 & 10 \\ \hline


  \end{tabular}
\end{table}


\section{Motivational Evaluation}~\label{motivation}

To identify the performance bottlenecks discussed in Sections~\ref{background_buffer} and~\ref{background_manycore}, we conduct a motivational evaluation using MySQL running the TPC-C benchmark on a manycore system, with the experimental setup described in Section~\ref{eval}. 
Since previous works have demonstrated that MySQL with its default configuration exhibits extremely low performance, we modified several InnoDB parameters for a fair evaluation. 
We use this modified configuration throughout the paper, following similar practices as the studies for \textit{WAR}~\cite{an2022avoiding} and \textit{LRU-C}~\cite{lee2023lru}.
The modified configurations include parameters for thread concurrency and flush efficiency, as detailed in Table~\ref{innodb_params}.

\subsection{Limited Scalability}




\begin{figure}[t]
    \centering
    \subfloat[TPS.]{\includegraphics[width=4.35cm]{figures/TPS_mysql_background.png}\label{TPS_mysql}}%
    \centering
    \subfloat[Latency-MySQL(Configured).]{\includegraphics[width=4.60cm]{figures/latency_mysql_background.png}\label{Latency_mysql}}%
  \caption{Limited scalability of MySQL.} %\skim{maybe break this two different figures?}
  \label{Manycorearchitecture}
\end{figure} 

Figure~\ref{Manycorearchitecture} shows the throughput per second (TPS) and 99\% tail latency as the number of threads increases. As shown in Figure~\ref{TPS_mysql}, the modified configuration maintains higher TPS with increasing concurrent threads. 
While the default configuration performs better at 8 threads due to lower contention, from 16 threads onward the modified configuration consistently outperforms it, with improvements ranging from 1.33$\times$ to 2.63$\times$. 
In both configurations, performance peaks at a low thread count and declines as thread count rises. Limited scalability is also evident in latency, as seen in Figure~\ref{Latency_mysql}. Although TPS drops less sharply for the modified configuration, 99\% latency rises steeply with thread count, reaching a maximum of 12,924.72 ms at 128 threads compared to 51.21 ms with a single thread. These results highlight the complex synchronization in database operations and confirm that the current database is not optimized to scale on manycore systems.


\subsection{Lock Contention and I/O Stall}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/lagend_buf.png}

    \subfloat[Buf 5\%.]{%
        \includegraphics[width=4.2cm]{figures/buf5.png}
        \label{lockio_buf5}
    }%
    \subfloat[Buf 50\%.]{%
        \includegraphics[width=4.2cm]{figures/buf50.png}
        \label{lockio_buf50}
    }%

    \caption{Lock contention and I/O operation breakdown under different buffer pool sizes in MySQL.} 
    \label{fig:lockio_mysql}
\end{figure}


Figure~\ref{fig:lockio_mysql} presents the runtime breakdown for different buffer pool sizes, set to 5\% and 50\% of the total database size, as the number of concurrent threads increases. We analyze the relative time spent in four key components: \textit{Flush lock}, \textit{Buffer Lock}, \textit{Fsync}, and \textit{Pwrite}.


\noindent\textbf{Lock Contention:} For the 5\% buffer pool (Figure~\ref{lockio_buf5}), overhead from the \textit{Buffer Lock} increases sharply as the number of threads grows, rising from 3.16\% of total runtime with 1 thread to 63.85\% at 64 threads. This increase results from frequent page replacements caused by the limited buffer pool size, which creates contention on the global LRU mutex. 
The contention intensifies when the victim page is dirty, as it must be added to the flush list while holding the global LRU mutex, thereby extending the lock duration. 
In contrast, with the 50\% buffer pool (Figure~\ref{lockio_buf50}), \textit{Buffer Lock} overhead remains low, while \textit{Flush lock} overhead increases with more threads. 
A larger buffer pool holds more clean pages, reducing replacements. 
However, dirty pages accumulate before flushing, resulting in higher contention on the global flush mutex. The \textit{Flush lock} overhead grows from 13.10\% to 32.76\% at 64 threads, indicating that global flush mutex contention becomes higher than that of the global LRU mutex.



\noindent\textbf{I/O Stall: } The two key I/O operations described in Section~\ref{background_buffer}, \textit{Fsync} (for WAL persistence) and \textit{Pwrite} (for dirty page writes), exhibit distinct behaviors. The time spent in \textit{Fsync} increases sharply with the number of threads, as synchronous flushes to storage become a bottleneck that blocks more threads under higher concurrency. At 128 threads, \textit{Fsync} accounts for 41.04\% of total runtime in the 5\% buffer pool case and 52.16\% in the 50\% buffer pool case, making it the dominant bottleneck. 
In contrast, \textit{Pwrite} writes data to the OS page cache and is handled asynchronously by kernel threads, resulting in stable overhead regardless of thread count or buffer pool size.

In summary, a small buffer pool size causes frequent evictions that increase contention on the global LRU mutex, while a large buffer pool size accumulates more dirty pages, leading to higher contention on the global flush mutex and increased I/O synchronization overhead.
