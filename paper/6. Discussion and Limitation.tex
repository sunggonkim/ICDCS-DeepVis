\section{Discussion and Limitation}
\label{DiscussionAndLimitation}
\label{Discussion}

We critically analyze the security properties, limitations, and potential evasion strategies of \DeepVis. Following the principles of adversarial machine learning, we explicitly evaluate robustness against adaptive attackers and analyze the operational constraints in hyperscale environments.

\subsection{Robustness Against Adaptive Attackers}

We assume a white-box adversary who possesses knowledge of the hash mapping function, the RGB encoding scheme, and the CAE architecture.

\subsubsection{Attack 1: Low-Entropy Mimicry}
An attacker might attempt to reduce the entropy of a rootkit to evade detection by the Red channel.

\noindent\textbf{Attack Vector.} The attacker pads the malicious binary with null bytes, English text, or NOP sleds. This lowers the Shannon entropy from the typical packed range ($S \approx 7.8$) to the benign range ($S \approx 5.5$).

\noindent\textbf{Defense Analysis.} While this strategy evades the Red channel, it forces the attacker into a \textit{Trilemma} where minimizing one detection signal inevitably amplifies another.
\begin{enumerate}
    \item \textbf{Size Inflation (Green Channel).} Padding increases the file size substantially. To reduce entropy from 7.8 to 5.5, the file size must increase by approximately $4\times$. This deviation triggers the Green channel which monitors file size anomalies.
    \item \textbf{Permission Anomaly (Blue Channel).} The malicious file still requires execution permissions. A large, low-entropy binary with SUID bits in a system directory such as \texttt{/lib/modules/} triggers the Blue channel.
\end{enumerate}
We formally model this as a constrained optimization problem. The attacker cannot simultaneously minimize entropy, file size, and permission risk below their respective detection thresholds while maintaining executable utility.

\subsubsection{Attack 2: Chameleon Attack (Hash Collision)}
An attacker might craft a malicious filename such that its hash coordinates collide with a high-churn benign file.

\noindent\textbf{Attack Vector.} The attacker computes a path $p^*$ such that $\Phi(p^*) = \Phi(\texttt{/var/log/syslog})$. They aim to hide the rootkit signal within the noise of frequent log updates.

\noindent\textbf{Defense Analysis.} \DeepVis mitigates this through two mechanisms.
\begin{enumerate}
    \item \textbf{Pre-image Resistance.} Finding a functional path in a target directory that hashes to a specific coordinate requires $2^{64}$ operations. This is computationally prohibitive for run-time attacks.
    \item \textbf{Max-Risk Pooling.} Even if a collision occurs, \DeepVis utilizes a Max-Priority collision resolution strategy as defined in Section~\ref{design_3}. If a packed rootkit ($S=7.8$) maps to the same pixel as a log file ($S=4.2$), the pixel retains the maximum value of 7.8. Therefore, the attack signal is preserved regardless of the background noise.
\end{enumerate}

\subsection{Operational Analysis: The SNR Advantage}
\label{snr_analysis}

System administrators understand that checking the integrity of a petabyte-scale file system requires granularity. A single global checksum is useless because it changes with every log write. The ``MSE Paradox'' we identified in Section~\ref{Background} is the statistical equivalent of this problem. We demonstrate why \DeepVis succeeds where global metrics fail using Signal-to-Noise Ratio (SNR) analysis.

\subsubsection{The Needle in the Haystack Problem}
Let $N$ be the total number of files and $k$ be the number of compromised files.
\begin{itemize}
    \item \textbf{Benign Updates (Diffuse Noise):} An upgrade modifies $N_{up} \approx 1000$ files with small variance $\sigma^2$.
    \item \textbf{Rootkit (Sparse Signal):} An attack modifies $k \approx 1$ file with large deviation $\delta$.
\end{itemize}

When using Global MSE ($L_2$), the attack signal is diluted by the system size $N$.
\begin{equation}
    SNR_{Global} \propto \frac{k}{N} \cdot \delta
\end{equation}
As $N \to \infty$ in hyperscale storage, $SNR \to 0$. The rootkit becomes statistically invisible against the background noise of legitimate churn.

\subsubsection{The Local Max Solution ($L_\infty$)}
By using the Local Maximum ($L_\infty = \max_i |D_i|$), \DeepVis functions as a parallelized difference operation. We isolate the single worst violation regardless of the file system size.
\begin{equation}
    SNR_{Local} \propto \delta
\end{equation}
This property is critical for systems scaling. It means that the sensitivity of \DeepVis does not degrade as the file system grows to millions of files. This contrasts with global statistical models which lose precision at scale.

\subsection{Limitations}

\subsubsection{Memory-Only Rootkits}
Rootkits that reside solely in RAM, such as those injected via \texttt{ptrace} or reflective DLL injection, leave no persistent footprint on the disk. Since \DeepVis operates on file system snapshots, it cannot detect these volatile threats. To address this, we recommend deploying \DeepVis alongside memory forensics tools such as Volatility or LKRG.

\subsubsection{Low-Entropy Malware}
While rare, some malware utilizes low-entropy payloads such as ASCII-encoded shellcode or polymorphic engines to evade entropy-based detection. In these cases, the Red channel (Entropy) may fail. However, the Blue channel (Permissions) and Green channel (Size/API Density) provide secondary detection signals.

\subsubsection{Collision Density at Hyperscale}
For extremely large file systems exceeding 10 million files, the collision density in a $128 \times 128$ tensor increases. This may cause information loss where multiple benign files mask the features of a lower-risk anomaly. To mitigate this, we recommend increasing the tensor resolution to $256 \times 256$ or employing a 3D tensor mapping strategy with secondary hashing for conflict resolution.

\subsection{Deployment Considerations}

\subsubsection{Poisoned Baseline Defense}
A critical security concern is preventing an attacker from poisoning the baseline tensor or trained model. We address this through:

\begin{enumerate}
    \item \textbf{Golden Image Attestation.} The baseline is generated from a cryptographically verified golden image (e.g., signed Docker image or AMI). The image hash is recorded in an immutable audit log.
    \item \textbf{Model Provenance.} The trained CAE model is stored in a read-only artifact repository (e.g., OCI registry) with content-addressable hashing. Any modification invalidates the hash.
    \item \textbf{Trusted Analysis Environment.} During training and detection, the \DeepVis process runs in a TEE (Trusted Execution Environment) such as Intel SGX or AWS Nitro Enclave, isolating it from potentially compromised host kernels.
\end{enumerate}

\noindent\textbf{Trusted Computing Base (TCB).} The TCB for \DeepVis consists of: (1) the snapshot engine (read-only mount), (2) the CAE inference runtime (ONNX), and (3) the hash verification logic. This is significantly smaller than provenance systems requiring kernel instrumentation.

\subsubsection{Agentless Architecture}
To further minimize TCB concerns, \DeepVis supports an agentless architecture. The system snapshots the target disk (e.g., AWS EBS or LVM volume) and mounts it read-only on a trusted analysis instance. This ensures that the monitoring process cannot be tampered with by a compromised kernel on the target host.

\subsubsection{Parallel and Incremental Architecture}
To scale beyond one million files, sequential scanning is insufficient. We propose a \textbf{Parallel Asynchronous Architecture} for future work.
\begin{enumerate}
    \item \textbf{Sharded Metadata Collection.} File system traversal is parallelized across $K$ worker threads. Each thread handles a distinct directory shard determined by $\text{Hash}(path) \pmod K$.
    \item \textbf{Incremental Visual Update.} Instead of regenerating the entire image $I_t$, we optimize the update cost. Since the baseline comparison yields a sparse set of changes $\Delta$, we directly update only the affected pixels:
    \begin{equation}
        I_{t}[\Phi(f)] \leftarrow \text{MaxRisk}(\text{Feature}(f)) \quad \forall f \in \Delta
    \end{equation}
    This reduces the update complexity from $O(N)$ to $O(|\Delta|)$. This optimization makes real-time monitoring feasible even for high-performance computing storage systems such as Lustre or GPFS.
\end{enumerate}

\subsubsection{Resource-Constrained Environments}
For edge devices or legacy servers without GPUs, we recommend deployment via ONNX Runtime with Int8 Dynamic Quantization. As demonstrated in our evaluation, this reduces the model size by $4\times$ and inference latency by $3\times$ compared to standard FP32 execution. This enables \DeepVis to run effectively on low-power hardware with less than 1\% CPU utilization.