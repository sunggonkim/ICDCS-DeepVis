\section{Security Analysis and Limitations}
\label{sec:discussion}

We analyze the security robustness of \DeepVis against adaptive evasion, detail the operational triage workflow, and discuss system boundaries.

\noindent\textbf{Defense against Adaptive Adversaries (Feature Trilemma). }
We simulated an advanced adaptive attacker attempting to evade detection by (1) moving the rootkit to a compliant path (e.g., \texttt{/usr/bin}) to minimize Context ($G$) and Structure ($B$) penalties, and (2) injecting zero-padding to reduce header entropy ($R$) to 6.0.
Experimental results demonstrate that \DeepVis maintains robust detection (Recall 100\%) even under this combined evasion scenario ($L_\infty$ Score 0.40 $> \tau=0.31$). The investigation reveals a detection boundary at $H < 4.0$ bits, implying that an attacker must replace over 50\% of the ELF header with null bytes to evade detection, a constraint that structurally corrupts the binary format. 

Regarding Targeted Hash Collisions, \DeepVis employs a high-entropy secret key $K$ (managed via TEE/HSM) to randomize spatial mapping. Evaluation across 50 random key rotations confirms threshold $\tau$ stability (KS-test $p \approx 1.0$). This stability holds because the CAE learns the global manifold of valid file features rather than overfitting to specific tensor coordinates; since a file's intrinsic features (e.g., entropy, header) remain invariant regardless of its mapped grid position, the overall reconstruction error distribution remains statistically identical, enabling seamless key rotation without retraining.

\noindent\textbf{Operational Triage: The Filter-then-Verify Workflow. }
A primary concern in fixed-size hashing is the cost of investigating alerts when multiple files collide. We propose a Two-Stage Filter-then-Verify model. Instead of relying on \DeepVis for absolute forensic attribution, it serves as a high-speed filter.
\begin{enumerate}[]
    \item \textbf{Stage 1 (DeepVis Filter):} The system identifies anomalous pixels within seconds. Empirically, even at maximum saturation ($\approx$610 collisions/pixel with 10M files and 128x128 resolution), our experiments confirm 100\% recall for injected rootkits (see Section~\ref{eval_saturation}), restricting the search space from $10^7$ files to a bounded bucket of $\approx 600$ files.
    \item \textbf{Stage 2 (Targeted Verification):} Security operators trigger expensive scanners (e.g., YARA, full-hash) only on the flagged bucket.
\end{enumerate}
This reduces the investigation search space by approximately 16,000$\times$, making continuous integrity monitoring operationally viable without the alert fatigue of traditional FIMs.


\noindent\textbf{Scope, Limitations, and Complementarity. }
\DeepVis prioritizes hyperscale throughput via header-only sampling (first 4KB).
\begin{itemize}[]
    \item Blind Spots: It excels at detecting structural anomalies in binaries (ELF/LKM) but inherently misses deep-payload injections in script-based attacks (e.g., Python polyglots) or data-only corruption.
    \item OS-Centricity: While achieving 97.1\% recall on Linux, effectiveness drops on Windows due to the higher variance of PE headers. Future work will incorporate OS-specific feature engineering.
    \item Statelessness: Unlike event-based monitors (e.g., \texttt{auditd}) that require continuous operation, \DeepVis provides stateless, point-in-time verification. This makes it robust against agent-killing attacks, serving as a definitive ground-truth auditor to complement real-time event streams.
\end{itemize}