\section{Evaluation}
\label{sec:evaluation}
We evaluate \DeepVis on a production Google Cloud Platform (GCP) infrastructure using real compiled rootkits and realistic attack scenarios. Our evaluation answers whether the multi-modal RGB encoding distinguishes high-entropy packed malware (RQ1), scales to millions of files (RQ2), tolerates legitimate system churn (RQ3), compares favorably against runtime monitors and legacy scanners (RQ4), and resists hash collisions at hyperscale (RQ5).
%=====================================================================
\subsection{Evaluation Setup}
\label{eval_setup}
%=====================================================================
\noindent\textbf{Testbed Environment.}
We conduct experiments on three distinct GCP configurations to represent a spectrum of cloud instances: \textbf{Low} (e2-micro, 2 vCPU, 1GB RAM, HDD), \textbf{Mid} (e2-standard-2, 2 vCPU, 8GB RAM, SSD), and \textbf{High} (c2-standard-4, 4 vCPU, 16GB RAM, NVMe SSD). The primary evaluation uses the High tier to demonstrate performance on modern NVMe storage. To simulate a production environment, we populated the file system with a diverse set of benign artifacts, including system binaries (e.g., \texttt{nginx}, \texttt{gcc}), configuration files, and Python scripts, scaling up to 50 million files for stress testing.

\noindent\textbf{Target Datasets.} We curated two datasets for evaluation: (1) a \textbf{Benign Baseline} of 667 legitimate system binaries (e.g., \texttt{python3}) from Ubuntu 20.04 to measure false positives; and (2) a \textbf{Malware Corpus} of 37,387 files sourced from open repositories~\cite{vxunderground, themalwarerepo}. This corpus spans Linux (305 files including rootkits~\cite{diamorphine,azazel}), Windows (35k+ files), and Web artifacts. For our primary evaluation, we isolate 68 active Linux ELF binaries to rigorously test detection recall.

%=====================================================================
\subsection{Detection Accuracy and Feature Orthogonality (RQ1)}
\label{eval_accuracy}
%=====================================================================

\noindent\textbf{Baseline and Model Configuration. }Reference tools utilized both default and tuned configurations to represent realistic deployment scenarios. ClamAV employed the standard virus database, while YARA used a minimal rule set matching common malware keywords and ELF headers. For DeepVis, the CAE was trained exclusively on benign artifacts to establish a normality baseline, yielding a calibrated anomaly detection threshold of $\tau_{L_{\infty}} = 0.52$. To isolate the architectural benefit of this learned representation, the evaluation includes DeepVis (Entropy), a heuristic baseline that applies a static threshold ($\tau_R=0.75$) solely to the R-channel without multi-modal fusion.

%---------------------------------------------------------------------
% TABLE III: Unified Performance (Extended)
%---------------------------------------------------------------------
\begin{table}[t]
\centering
\caption{\textbf{Unified Detection Performance.} Comparison against tunned baselines. DeepVis (Full) achieves superior recall on Linux/Mobile threats via unsupervised learning.}
\label{tab:unified_detection}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l cccc c}
\toprule
\multirow{2}{*}{\textbf{System}} & \multicolumn{4}{c}{\textbf{Recall by Platform}} & \textbf{False Pos.} \\
& \textbf{Linux} & \textbf{Win.} & \textbf{Web} & \textbf{Mob.} & \textbf{(Benign)} \\
\midrule
ClamAV (Standard) & 33.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
ClamAV (Tuned)$^\dagger$ & 95.4\% & \textbf{96.6\%} & 82.6\% & 50.0\% & \textbf{0.0\%} \\
YARA (Standard) & 100.0\% & 1.9\% & 24.3\% & 0.0\% & 45.0\% \\
YARA (Tuned)$^\ddagger$ & 24.6\% & 2.6\% & 68.1\% & 79.2\% & 31.0\% \\
AIDE & \textbf{100.0\%} & \textbf{100.0\%} & \textbf{100.0\%} & \textbf{100.0\%} & 100.0\% \\
Set-AE & 40.0\% & 10.0\% & 6.9\% & 91.7\% & 5.0\% \\
\midrule
DeepVis (Entropy) & 25.0\% & 14.2\% & 5.6\% & 91.7\% & 10.2\% \\
\rowcolor{gray!10} 
\textbf{DeepVis (Full)} & \textbf{97.1\%} & \textbf{15.8\%} & \textbf{89.6\%} & \textbf{100.0\%} & \textbf{0.3\%} \\
\bottomrule
\multicolumn{6}{l}{\scriptsize $^\dagger$Using custom DB of known hashes. $^\ddagger$Using custom rules for LKM/Webshells.}
\end{tabular}%
}
\end{table}

\noindent\textbf{Platform-Specific Generalization. }Table~\ref{tab:unified_detection} presents detection recall across four dominant platforms. ClamAV (Standard) fails to detect most samples (0--33\%), whereas the Tuned version achieves high recall (95.4--96.6\%). However, this performance is solely due to the use of a custom database containing the exact hashes of our dataset—an ex-post-facto "cheating" configuration that is inherently blind to zero-day variants. YARA (Tuned) improves detection on Web (68.1\%) and Mobile (79.2\%) but incurs a high false positive rate (31.0\%) due to aggressive heuristic rules. In contrast, \DeepVis demonstrates exceptional generalization without prior knowledge, achieving 97.1\% Recall on Linux and 100.0\% on Mobile. Furthermore, \DeepVis achieves an integrated detection rate of approximately 16.8\% across the entire malware corpus ($\approx$37k samples). While seemingly modest, this significantly outperforms platform-agnostic baselines such as YARA ($\approx$3\%) and Set-AE ($\approx$10\%), demonstrating robust structural anomaly detection even for non-target binaries (Windows/Web). Crucially, \DeepVis maintains a negligible False Positive rate of 0.3\%, confirming that the learned normality model is far more robust to benign variations than static heuristics or single-channel baselines (10.2\%).

\noindent\textbf{Architectural Advantage over Set-based Learning. }Benchmarking against a Set-based Autoencoder (Set-AE) baseline evaluates the structural necessity of the Hash-Grid. As shown in Table~\ref{tab:unified_detection}, Set-AE fails to isolate sparse threats, achieving only 40\% recall on rootkits. This performance degradation stems from the global feature pooling mechanism, which dilutes the signal of a single malicious file ($N=1$) against the variance of thousands of benign system files. In contrast, DeepVis projects files onto a fixed Spatial Grid and employs $L_\infty$ pooling, ensuring that sparse anomalies remain locally distinct spikes rather than being averaged out globally.

%---------------------------------------------------------------------
% TABLE IV: Detailed Breakdown (Micro View)
%---------------------------------------------------------------------
\begin{table}[t]
\centering
\caption{\textbf{Detailed Detection Analysis.} Multi-modal RGB features catch threats that single metrics miss. The table presents representative samples from our Benign (N=667) and Malware (N=68) datasets. The "Miss" cases highlight limitations against threats that mimic benign header statistics.}
\label{tab:detailed_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c ccc c}
\toprule
\textbf{Type} & \textbf{Name} & \textbf{R} & \textbf{G} & \textbf{B} & \textbf{Status} \\
\midrule
\multicolumn{6}{l}{\textit{Detected Active Threats (Multi-Platform)}} \\
LKM Rootkit & \texttt{Diamorphine} & 0.55 & \textbf{0.81} & \textbf{1.00} & Det. \\
Windows Spy & \texttt{FormGrab.exe} & 0.75 & 0.57 & \textbf{0.90} & Det. \\
Android Mal & \texttt{DEX Dropper} & \textbf{1.00} & 0.57 & \textbf{1.00} & Det. \\
Webshell & \texttt{TDshell.php} & 0.69 & 0.72 & \textbf{0.60} & Det. \\
Encrypted RK & \texttt{azazel\_enc} & \textbf{1.00} & \textbf{0.90} & \textbf{0.80} & Det. \\
\midrule
\multicolumn{6}{l}{\textit{Undetected (Limitations)}} \\
Obfuscated & \texttt{libc\_fake.so} & 0.61 & 0.00 & 0.00 & Miss \\
Mimicry Script & \texttt{setup.sh} & 0.58 & 0.00 & 0.00 & Miss \\
\midrule
\multicolumn{6}{l}{\textit{Benign Baselines (Clean)}} \\
Interpreter & \texttt{python3} & 0.78 & 0.96 & 0.10 & Clean \\
Library & \texttt{libc.so.6} & 0.79 & 0.90 & 0.10 & Clean \\
\multicolumn{6}{l}{\textit{False Positives (High Entropy)}} \\
Admin Tool & \texttt{snap} & \textbf{0.75} & \textbf{1.00} & 0.10 & False Pos. \\
Config Gen & \texttt{cloud-init} & 0.72 & 0.85 & 0.30 & False Pos. \\
\bottomrule
\end{tabular}
}
\end{table}

\noindent\textbf{Micro View Analysis.} 
Table~\ref{tab:detailed_breakdown} provides a granular analysis of detection capabilities. \DeepVis detects evasive threats through feature orthogonality. For instance, the rootkit \texttt{Diamorphine} evaded the Entropy channel ($R=0.55$) but was detected by the Context ($G=0.81$) and Structure ($B=1.00$) channels. Similarly, the Windows binary \texttt{FormGrab.exe} was flagged due to its alien PE structure ($B=0.90$) in a Linux environment. However, the header-only approach exhibits intrinsic blind spots. \DeepVis failed to detect the script \texttt{setup.sh} and the obfuscated library \texttt{libc\_fake.so}, as they resided in structurally valid paths and lacked binary packing anomalies. This confirms that \DeepVis operates as a high-speed first-line defense for binary integrity rather than a full-content forensic scanner.


%=====================================================================
\subsection{Scalability and Performance Analysis (RQ2)}
\label{eval_scalability}
%=====================================================================

The primary architectural claim of \DeepVis is the decoupling of verification latency from file system size. We validate this through two distinct lenses: processing throughput (micro-benchmark) and service interference (macro-benchmark).

\begin{figure}[t]
    \centering
    \subfloat[Throughput]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_final_throughput.pdf}
        \label{fig:perf_throughput}
    } \hfill
    \subfloat[Interference]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_final_latency.pdf}
        \label{fig:perf_latency}
    }
    \caption{\textbf{Comprehensive Performance Analysis.} (a) \textbf{Throughput}: DeepVis achieves hyperscale speeds ($\approx$40k files/s) via asynchronous I/O, outperforming synchronous baselines. (b) \textbf{Interference}: Despite its speed, \DeepVis maintains negligible latency overhead (+2\%) compared to massive spikes caused by AIDE (+291\%) and YARA (+547\%).}
    \label{fig:perf_analysis}
\end{figure}

\subsubsection{Micro-benchmark}
\noindent\textbf{Scan Throughput. }Figure~\ref{fig:perf_analysis}(a) compares DeepVis against AIDE to demonstrate operational feasibility. AIDE performs full-file cryptographic hashing, providing strong integrity guarantees but incurring $O(N \times Size)$ I/O complexity. This heavy I/O load often forces operators to restrict scanning to weekly maintenance windows. On a GCP High tier (c2-standard-4), \DeepVis achieves a 7.7$\times$ speedup over standard AIDE. Even against an optimized Partial-Hash AIDE baseline that reads only the first 128 bytes, \DeepVis maintains a 5.4$\times$ throughput advantage. This comparison highlights the architectural benefit of the \textit{Header-Sampling paradigm} over traditional full-content hashing. The goal is not merely to compare I/O speeds under identical workloads, but to demonstrate that \DeepVis's design choice enables hyperscale feasibility—transforming FIM from a heavy, periodic task into a lightweight, continuous monitoring service. This represents a system-level trade-off between theoretical completeness and operational scalability.

\noindent\textbf{Comparison with Commercial Scanners. }Benchmarking against fuzzy hashing (ssdeep) and signature scanners (ClamAV, YARA) on the full \texttt{/usr} directory (240,827 files) reveals that traditional tools are bottlenecked by synchronous content reads (127--1,004 files/s). In contrast, \DeepVis achieves 39,993 files/s, representing a 40$\times$ to 215$\times$ speedup over the baselines. This throughput demonstrates the efficiency of the asynchronous snapshot engine in hyperscale environments.

\subsubsection{Macro-benchmark}
\noindent\textbf{Service Interference. }Figure~\ref{fig:perf_analysis}(b) illustrates the P99 latency of a co-located NGINX web server during a full system scan. While raw throughput is critical, interference defines the operational constraint. Traditional tools severely impact system responsiveness; YARA and Heuristic engines cause degradation of +546\% and +324\% respectively due to CPU-intensive pattern matching. AIDE induces a +291\% latency spike (12.1ms) due to blocking I/O operations. In contrast, \DeepVis maintains a P99 latency of 3,162$\mu$s, reflecting a negligible +2.0\% overhead compared to the baseline (3,100$\mu$s). This confirms that the spatial hashing and asynchronous design allow the system to operate transparently in the background.

\noindent\textbf{CPU Resource Profile. }Resource contention analysis explains the latency results. Legacy FIMs and scanners such as Osquery and AIDE saturate the Global CPU at near 100\%, forcing the OS scheduler to throttle the web server. DeepVis, however, maintains a CPU profile of 11.2\%, nearly identical to the baseline (9.8\%). Unlike runtime monitors (e.g., Falco) which incur constant context-switching overhead (+58.3\% latency degradation), \DeepVis utilizes lightweight SIMD optimizations to ensure security monitoring remains strictly orthogonal to the primary service performance.


%=====================================================================
\subsection{Impact of Spatial Dimension and Hash Saturation (RQ3, RQ6)}
\label{eval_saturation}
%=====================================================================


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/comparison_dilution.pdf} 
    \caption{Visualizing Signal Preservation. (Top) \DeepVis maintains spatial locality, isolating the malware as a distinct red peak ($L_\infty$ Spike). (Bottom) Set-AE averages the features into a single global vector, causing the attack signal to dilute into the background noise (Signal Dilution), resulting in detection failure.}
    \label{fig:signal_comparison}
\end{figure}

\noindent\textbf{Set-based vs. Hash-Grid Architectures.}Figure~\ref{fig:signal_comparison} compares the internal representations under an active attack scenario. The top panel demonstrates that the 2D Hash-Grid architecture maintains the spatial isolation of anomalies, manifesting injected malware as sharp, localized peaks against diffuse background noise. In contrast, the bottom panel shows that reducing the dimension to a single global vector (Set-AE) aggregates sparse attack signals with thousands of benign signals, washing out the anomaly. Quantitatively, measurements during a live system update confirm this observation. The global pooling approach fails to distinguish the attack from update noise because the background noise floor rises ($\mu_{noise}=0.35$), masking the attack signal and resulting in a negligible Signal-to-Noise Ratio (SNR) of 1.09. Conversely, the spatial isolation of \DeepVis effectively filters out diffuse noise, preserving the sharp attack spike ($0.95$) against the background ($0.35$). This yields a superior SNR of 2.71, ensuring robust detection even during high churn.


\noindent\textbf{Resilience to Hash Saturation. }To validate the stability of the hash mapping as the file count ($N$) exceeds the grid capacity ($W \times H$), we stress-tested the system by injecting up to 204,000 files into the $128 \times 128$ grid. Table~\ref{tab:hyperscale_saturation} shows that even at 99.99\% saturation (high collision state), the system maintains stability. Unlike traditional hash tables where collisions degrade performance to $O(N)$, our Max-Risk Pooling strategy ($\text{Grid}[h] = \max(\text{Grid}[h], s)$) ensures that tensor construction remains strictly $O(1)$. Crucially, even under extreme saturation (e.g., 100 files per pixel), a single high-risk anomaly dominates the pixel value due to the \texttt{max()} operation. This design makes \DeepVis inherently resistant to \textit{Decoy Attacks}—an attempt to flood the grid with benign files to mask a threat—as low-score benign files cannot suppress the signal of a malicious file. Any information loss during saturation applies exclusively to benign details, while the attack signal is preserved as the dominant feature. Collisions do not increase computational overhead; they merely aggregate risk scores, ensuring that detection latency remains constant regardless of file density.

\begin{table}[t]
\centering
\caption{Hash Saturation Analysis. High collision rates do not impact processing overhead due to $O(1)$ Max-Risk Pooling.}
\label{tab:hyperscale_saturation}
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{r c c}
\toprule
Files ($N$) & Grid Saturation & Avg. Collisions \\
\midrule
10,000 & 45.47\% & 0.61 \\
50,000 & 95.21\% & 3.05 \\
100,000 & 99.87\% & 6.10 \\
\rowcolor{gray!10} 
204,000 & 99.99\% & 12.45 \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Component Overhead. }Component analysis at scale (500K files) confirms that the hashing and mapping process is computationally efficient. The \texttt{io\_uring} based file reading consumes 90.1\% of the total scan time, while hashing, tensor mapping, and CAE inference account for negligible overhead ($<3\%$). This validates that the Hash-Grid architecture effectively decouples detection complexity from file system size without introducing computational bottlenecks.



%=====================================================================
\subsection{Fleet-Scale Scalability (RQ7)}
\label{eval_fleet}
%=====================================================================

A key requirement for distributed systems is demonstrating scalability across a fleet of nodes under realistic network conditions. We evaluate the capability of \DeepVis to verify a large distributed cluster by deploying 100 concurrent nodes in a public cloud environment.

\noindent\textbf{Experimental Setup and Orchestration at Scale. }Deploying 100 concurrent nodes presents significant orchestration challenges, including API rate limits and network saturation. To mitigate these, we distributed the fleet across three geographically distant GCP regions: \texttt{us-central1} (Iowa), \texttt{us-east1} (South Carolina), and \texttt{us-west1} (Oregon). A hierarchical orchestration architecture was employed where a single bastion node (\texttt{deepvis-mid}) located in \texttt{asia-northeast3} (Seoul) coordinated the entire US-based fleet via GCP's internal VPC network. This cross-region control plane demonstrates effective management of global deployments without co-location. Each e2-micro node, provisioned with a custom Golden Image containing the Rust-based \DeepVis scanner, performed a full scan of local directories upon activation. The resulting $128 \times 128 \times 3$ RGB tensors were transmitted to the aggregator via the asynchronous protocol.

\begin{figure}[t]
  \centering
  \subfloat[Linear Scalability]{
    \includegraphics[width=0.8\linewidth]{Figures/fig_fleet_scalability.pdf}
    \label{fig:fleet_scale}
  } \\
  \subfloat[Geo-Stability]{
    \includegraphics[width=0.8\linewidth]{Figures/fig_fleet_vis.pdf}
    \label{fig:fleet_heatmap}
  }
  \caption{\textbf{Fleet-Scale Performance.} (a) \textbf{Linear Scalability:} Throughput increases linearly with fleet size, reaching $\approx$206k files/s at 100 nodes. (b) \textbf{Geo-Stability:} The latency heatmap across 100 nodes shows consistent performance (avg 4.29s) across three US regions, confirming resilience against network variance.}
  \label{fig:fleet_perf}
\end{figure}

\noindent\textbf{Linear Scalability. }Figure~\ref{fig:fleet_perf}(a) demonstrates the linear scaling capability of \DeepVis. As the fleet size scales from 1 to 100 nodes, the aggregate verification throughput grows proportionally, reaching \textbf{206,611 files/sec}. This confirms that the stateless nature of the Hash-Grid architecture effectively decouples the processing load, eliminating central bottlenecks.

\noindent\textbf{Global Stability. }Figure~\ref{fig:fleet_perf}(b) visualizes the scan latency distribution across the 100 nodes. Despite the geographical dispersion, \DeepVis maintains a tight latency bound with an average of 4.29s and a maximum of 6.0s. Crucially, the aggregation overhead for the entire fleet was merely 548ms. This validates that \DeepVis ensures consistent performance SLAs even in "noisy neighbor" public cloud environments.

\noindent\textbf{Network Efficiency. }Tensor-based verification drastically reduces bandwidth consumption. Each node transmits a fixed-size 49KB tensor regardless of file count, totaling only 4.9MB for the 100-node fleet. This represents a 100$\times$ reduction in network overhead compared to provenance-based systems, which can exceed 500MB under similar workloads.

\begin{table}[h]
\centering
\caption{\textbf{Component Time Breakdown (Cold Cache).} I/O dominates ($\approx$90\%), confirming that \\DeepVis computational overhead (Hashing, Entropy, Tensor Update) is negligible.}
\label{tab:overhead_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l rrrr}
\toprule
\textbf{Component} & \textbf{10K} & \textbf{100K} & \textbf{200K} & \textbf{500K} \\
\midrule
Traversal & 127ms (14\%) & 1.74s (11\%) & 3.93s (9.5\%) & 8.88s (7.0\%) \\
\textbf{I/O (Header Read)} & \textbf{675ms (77\%)} & \textbf{12.77s (84\%)} & \textbf{36.09s (87\%)} & \textbf{113.2s (89.9\%)} \\
Hashing & 24ms (2.8\%) & 209ms (1.4\%) & 418ms (1.0\%) & 1.31s (1.0\%) \\
Entropy Calc & 30ms (3.4\%) & 322ms (2.1\%) & 557ms (1.3\%) & 1.86s (1.5\%) \\
Tensor Update & 21ms (2.4\%) & 132ms (0.9\%) & 321ms (0.8\%) & 746ms (0.6\%) \\
\midrule
\textbf{Total Time} & \textbf{877ms} & \textbf{15.17s} & \textbf{41.31s} & \textbf{126.0s} \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Computational Overhead Analysis. }Table~\ref{tab:overhead_breakdown} presents the execution time breakdown across varying file counts ranging from 10K to 500K under a cold cache scenario. I/O operations dominate the runtime, consuming approximately 90\% of the total execution time at scale (500K files). Conversely, computational tasks, including hashing, entropy calculation, and tensor updates, collectively account for less than 3.1\% of the total time. This indicates that the computational overhead of the hash-grid mapping and feature extraction remains negligible, confirming that the performance of \DeepVis is limited effectively by storage bandwidth rather than CPU resources.



%=====================================================================
\subsection{Ablation Study: Component Contribution Analysis}
\label{eval_ablation}
%=====================================================================

To quantify the contribution of each architectural decision, we evaluated system performance by selectively disabling key components. Table~\ref{tab:ablation} separates \textbf{Performance-critical} components (affecting throughput) from \textbf{Accuracy-critical} components (affecting detection).

\begin{table}[t]
\centering
\caption{Component-wise Ablation Study. Performance components (I/O, Sampling) determine throughput; Accuracy components (Hash-Grid, RGB) determine detection capability.}
\label{tab:ablation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l l | c c}
\toprule
Category & Configuration & Rate (files/s) & F1-Score \\
\midrule
\multirow{3}{*}{\textbf{Performance}} 
    & Baseline (Sync + Full Read) & 1,455 & -- \\
    & + Async I/O (\texttt{io\_uring}) & 2,910 & -- \\
    & + Header Sampling (128B) & 39,993 & -- \\
\midrule
\multirow{3}{*}{\textbf{Accuracy}} 
    & Entropy Only (R-channel) & -- & 0.25 \\
    & + Hash-Grid ($L_\infty$) & -- & 0.35 \\
    & + RGB Fusion (Full DeepVis) & -- & \textbf{0.96} \\
\midrule
\rowcolor{gray!15}
\multicolumn{2}{l|}{\textbf{DeepVis (All Components)}} & \textbf{39,993} & \textbf{0.96} \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Performance Optimization. }Replacing synchronous operations with asynchronous I/O doubles the throughput from 1,455 files/s to 2,910 files/s. This indicates that eliminating kernel-user context switching overhead improves efficiency even when reading entire files. The most significant gain stems from header sampling. Limiting the read scope to the first 128 bytes increases the scan rate to 39,993 files/s, representing a 27$\times$ speedup. This confirms that minimizing I/O volume is the primary factor for achieving hyperscale monitoring capabilities.

\noindent\textbf{Detection Fidelity. }Relying solely on entropy results in a poor F1-score of 0.25 due to false positives from benign compressed files. Applying the hash-grid architecture with local max pooling raises the score to 0.35, as it preserves sparse signals that are otherwise diluted by global pooling. However, the definitive improvement is achieved through multi-modal fusion. Integrating context and structure channels with entropy enables combinatorial reasoning, increasing the F1-score to 0.96. This demonstrates that architectural isolation must be paired with orthogonal feature sets to distinguish malicious artifacts from legitimate system noise.






