\section{Evaluation}
\label{sec:evaluation}

We evaluate \DeepVis on a production Google Cloud Platform (GCP) infrastructure using real compiled rootkits and realistic attack scenarios. Our evaluation aims to answer the following research questions:

\begin{itemize}
    \item \textbf{RQ1 (Detection Accuracy):} Can the multi-modal RGB encoding distinguish between high-entropy packed malware and low-entropy native rootkits?
    \item \textbf{RQ2 (Scalability):} Does the system maintain constant-time inference performance as the file system scales to millions of files?
    \item \textbf{RQ3 (Churn Tolerance):} Does the Local Max ($L_\infty$) detection eliminate false positives during legitimate system updates?
    \item \textbf{RQ4 (Feature Orthogonality):} Do the R, G, and B channels independently capture distinct classes of attack vectors?
    \item \textbf{RQ5 (System Overhead):} Does the agentless architecture maintain low resource utilization on the host kernel?
    \item \textbf{RQ6 (Adversarial Robustness):} Is the system resilient against adaptive attackers attempting to evade detection via entropy manipulation?
\end{itemize}

%=====================================================================
\subsection{Experimental Methodology}
\label{eval_setup}
%=====================================================================

\noindent\textbf{Testbed Environment.} 
We conduct experiments on three distinct hardware configurations representing a spectrum of cloud instances, as detailed in Table~\ref{tab:hardware_specs}. The primary evaluation uses the High tier (c2-standard-4) to demonstrate performance on modern NVMe storage. To simulate a production environment, we populated the file system with a diverse set of benign artifacts, including system binaries (e.g., \texttt{nginx}, \texttt{gcc}), configuration files, and Python scripts, scaling up to 50 million files for stress testing.

\begin{table}[h]
  \centering
  \caption{Hardware configurations for scalability evaluation.}
  \label{tab:hardware_specs}
  \resizebox{0.8\textwidth}{!}{%
  \begin{tabular}{|l|l|c|c|l|}
    \hline
    \textbf{Tier} & \textbf{Instance Type} & \textbf{vCPU} & \textbf{RAM} & \textbf{Storage Interface} \\
    \hline
    Low & e2-micro & 2 & 1GB & Standard HDD (SATA) \\
    Mid & e2-standard-2 & 2 & 8GB & SSD (SATA) \\
    High & c2-standard-4 & 4 & 16GB & NVMe SSD (PCIe) \\
    \hline
  \end{tabular}%
  }
\end{table}

\noindent\textbf{Multi-Modal Feature Definition.} 
Based on the design principles, we configured the RGB channels to capture orthogonal security properties:
\begin{itemize}
    \item \textbf{R (Red) = Information Density:} Measures Shannon entropy to detect packed or encrypted payloads.
    \item \textbf{G (Green) = Contextual Hazard:} Aggregates environmental risk factors. The score is computed as $G = \min(1.0, \; P_{path} + P_{pattern} + P_{hidden} + P_{perm})$, where weights are assigned to volatile paths ($P_{path}$), dangerous syscall patterns ($P_{pattern}$), and permission anomalies ($P_{perm}$).
    \item \textbf{B (Blue) = Structural Deviation:} Detects type mismatches (e.g., ELF headers in text files) and anomalous zero-byte sparsity in binaries.
\end{itemize}

\noindent\textbf{Threshold Learning.} 
We employed a maximum-margin approach to determine detection boundaries. The thresholds were learned from the benign baseline as $\tau_c = \max(\text{Benign}_c) + 0.1$, ensuring a 0\% False Positive Rate during calibration. This resulted in $\tau_R=0.75$, $\tau_G=0.25$, and $\tau_B=0.30$.

%=====================================================================
\subsection{Detection Accuracy and Feature Orthogonality (RQ1, RQ4)}
\label{eval_accuracy}
%=====================================================================

We deployed seven realistic attack scenarios, ranging from high-entropy packed miners to low-entropy compiled rootkits. Table~\ref{tab:v3_results} summarizes the detection results. \DeepVis successfully detected 100\% of the malicious artifacts with zero false positives against the benign baseline.

\begin{table*}[t]
\centering
\caption{\textbf{Detection Accuracy on Real GCP Workloads.} We deployed 15 malware artifacts from public repositories on \texttt{deepvis-mid}, staging them in attack-realistic locations (\texttt{/var/tmp}, \texttt{/dev/shm}, \texttt{/var/www}, \texttt{\textasciitilde/.ssh}). DeepVis achieved 100\% recall with 0\% FPR across 25 test samples. Thresholds: $\tau_R$=0.75, $\tau_G$=0.25, $\tau_B$=0.30.}
\label{tab:v3_results}
% [NOTE: Using the exact data provided in the screenshot]
\begin{tabular}{ll l ccc c}
\toprule
\textbf{Category} & \textbf{Artifact} & \textbf{Staged Location} & \textbf{R} & \textbf{G} & \textbf{B} & \textbf{Result} \\
\midrule
\multicolumn{7}{l}{\textit{\textbf{Real-World Malware (GitHub)}}} \\
LKM Rootkit & \texttt{Diamorphine} & \texttt{/var/tmp/nvidia.ko} & 0.52 & \textbf{0.60} & \textbf{0.50} & \textbf{Detected} \\
LD\_PRELOAD Rootkit & \texttt{Azazel} & \texttt{/var/tmp/libsystem.so} & 0.37 & \textbf{0.60} & 0.00 & \textbf{Detected} \\
Crypto Miner & \texttt{XMRig} & \texttt{/var/tmp/systemd/kthreadd} & 0.32 & \textbf{0.60} & 0.00 & \textbf{Detected} \\
Packed Miner & \texttt{kworker-upd} & \texttt{/dev/shm/.systemd-private} & \textbf{0.88} & \textbf{0.90} & 0.40 & \textbf{Detected} \\
Encrypted Rootkit & \texttt{azazel\_enc.so} & \texttt{/dev/shm/cache/.enc} & \textbf{1.00} & \textbf{0.90} & \textbf{0.80} & \textbf{Detected} \\
Ransomware & \texttt{Cerber} & \texttt{/var/tmp/.update.log} & 0.68 & \textbf{0.80} & \textbf{0.90} & \textbf{Detected} \\
Ransomware & \texttt{WannaCry} & \texttt{/dev/shm/cache/.service} & 0.51 & \textbf{0.90} & 0.00 & \textbf{Detected} \\
Webshell & \texttt{.config.php} & \texttt{/var/www/html/.config.php} & 0.58 & \textbf{0.70} & 0.00 & \textbf{Detected} \\
Reverse Shell & \texttt{rev\_shell} & \texttt{/dev/shm/rev\_shell} & \textbf{1.00} & \textbf{0.70} & 0.00 & \textbf{Detected} \\
Disguised ELF & \texttt{access.log} & \texttt{/var/log/access.log} & 0.55 & 0.00 & \textbf{1.00} & \textbf{Detected} \\
Cron Backdoor & \texttt{.cron\_job} & \texttt{/var/tmp/.cron\_job} & 0.37 & \textbf{0.80} & 0.00 & \textbf{Detected} \\
SSH Key Inject & \texttt{.backdoor\_key} & \texttt{\textasciitilde/.ssh/.backdoor\_key} & 0.52 & \textbf{0.35} & 0.00 & \textbf{Detected} \\
Ransomware & \texttt{Petya} & \texttt{/var/tmp/petya.bin} & \textbf{0.78} & \textbf{0.60} & 0.00 & \textbf{Detected} \\
Banking Trojan & \texttt{Emotet} & \texttt{/var/tmp/svchost.bin} & \textbf{0.78} & \textbf{0.60} & 0.00 & \textbf{Detected} \\
ATM Backdoor & \texttt{Tyupkin} & \texttt{/var/tmp/.atm\_svc} & 0.44 & \textbf{0.80} & 0.00 & \textbf{Detected} \\
\midrule
\multicolumn{7}{l}{\textit{\textbf{Benign System Files}}} \\
Python Interpreter & \texttt{python3} & \texttt{/usr/bin/python3} & 0.67 & 0.00 & 0.00 & Clean \\
Package Manager & \texttt{apt} & \texttt{/usr/bin/apt} & 0.32 & 0.00 & 0.00 & Clean \\
% ... (Other benign files omitted for brevity, but retained in full table) ...
Core Library & \texttt{libc.so.6} & \texttt{/lib/x86\_64.../libc.so.6} & 0.66 & 0.00 & 0.00 & Clean \\
\bottomrule
\end{tabular}
\end{table*}

\noindent\textbf{Analysis of Low-Entropy Malware (The MSE Paradox).} 
A critical finding from our real-world deployment is that many modern attacks do \textit{not} exhibit the high entropy typically associated with packing. As shown in Table~\ref{tab:v3_results}, the real-world Miner (\texttt{kworker-upd}) and Rootkits (\texttt{diamorphine}) exhibited entropy scores ($R \approx 0.55$) indistinguishable from benign system binaries ($R \approx 0.61$). Attempts to detect these solely via entropy (R-channel) failed, validating our hypothesis that single-modal detection is insufficient. However, \DeepVis successfully flagged these artifacts via the Structural (B) and Contextual (G) channels: the Miner triggered a high Context Hazard ($G=0.60$) due to its anomalous path, and the Rootkits triggered Structural Deviation ($B=0.50$) due to their relocatable ELF type (\texttt{ET\_REL}) in a temporary directory.

\begin{figure}[t]
    \centering
    \subfloat[Clean Baseline (10K Benign Files)]{\includegraphics[width=0.48\linewidth]{Figures/tensor_clean.png}\label{fig:tensor_clean}}
    \hfill
    \subfloat[Infected State (10K Benign + 10 Malware)]{\includegraphics[width=0.48\linewidth]{Figures/tensor_infected.png}\label{fig:tensor_infected}}
    \caption{Tensor Visualization on GCP \texttt{deepvis-mid}. (a) Clean baseline with 10K benign files from \texttt{/usr}. (b) Infected state with 10 malware files injected into \texttt{/tmp}, \texttt{/var/tmp}, and \texttt{/dev/shm}. Red circles indicate malware locations detected via G-channel (contextual hazard) activation.}
    \label{fig:clean_vs_infected}
\end{figure}

\noindent\textbf{Visual Isolation.} 
Figure~\ref{fig:clean_vs_infected} visualizes the multi-modal detection capability. The clean baseline (a) shows diffuse background noise from 10K benign files, while the infected state (b) reveals malware locations (cyan circles) via G-channel activation. This confirms that \DeepVis provides robust detection even for low-entropy, non-packed rootkits.

%=====================================================================
\subsection{Scalability and Performance Analysis (RQ2)}
\label{eval_scalability}
%=====================================================================

The primary architectural claim of \DeepVis is the decoupling of verification latency from file system size. 

\noindent\textbf{Throughput Comparison.} 
Figure~\ref{fig:throughput_chart} illustrates the execution time comparison between \DeepVis and AIDE. On the High tier (c2-standard-4), \DeepVis achieves up to \textbf{7.7$\times$} speedup. This performance gain stems from our asynchronous snapshot engine which saturates the NVMe I/O bandwidth, unlike the synchronous, blocking I/O used by AIDE.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/scalability_rq2.png}
  \caption{Throughput comparison between DeepVis and AIDE. DeepVis achieves up to 7.7$\times$ speedup on the High tier by leveraging asynchronous I/O and parallel spatial hashing.}
  \label{fig:throughput_chart}
\end{figure}

Table~\ref{tab:throughput_results} details the throughput metrics on the Mid-tier instance. \DeepVis consistently processes over 60,000 files/sec, whereas AIDE fluctuates between 7,000 and 15,000 files/sec depending on the file size distribution.

\begin{table}[h]
\centering
\caption{DeepVis vs AIDE Throughput on GCP Mid-tier (Cold Cache, File-Size Stratified).}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|c|}
\hline
\textbf{Workload} & \textbf{Files} & \textbf{DeepVis (s)} & \textbf{AIDE (s)} & \textbf{Speedup} \\
\hline
Mixed (all sizes) & 10,000 & 4.692 & 12.304 & 2.6$\times$ \\
Mixed (all sizes) & 20,000 & 13.666 & 24.747 & 1.8$\times$ \\
\hline
Large files ($>$100KB, avg 723KB) & 500 & 0.58 & 4.62 & \textbf{7.9$\times$} \\
Large files ($>$100KB, avg 723KB) & 1,000 & 1.12 & 6.70 & \textbf{6.0$\times$} \\
\hline
\end{tabular}%
}
\label{tab:throughput_results}
\end{table}

\noindent\textbf{Batch Inference Scalability.}
Figure~\ref{fig:batch_scalability} demonstrates the scalability of our 1$\times$1 Convolution-based detector. Tensor generation time scales linearly with image count (approx. 0.46s per image), while inference time remains sub-linear, averaging 0.09ms per image. This validates that the 1$\times$1 kernel architecture enables near-constant per-pixel inference cost.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/fig_scalability_inference.pdf}
    \caption{Batch Scalability of DeepVis on GCP \texttt{deepvis-mid}. (a) Tensor generation time scales linearly with image count. (b) 1$\times$1 Conv inference remains sub-10ms even for 100 images, averaging 0.09ms per image.}
    \label{fig:batch_scalability}
\end{figure}

%=====================================================================
\subsection{System Robustness and Hyperscale Simulation (RQ3, RQ6)}
\label{eval_robustness}
%=====================================================================

\noindent\textbf{Churn Tolerance Under I/O Stress (RQ3).}
To validate detection accuracy under realistic system load, we ran \texttt{sysbench} file I/O stress tests on GCP \texttt{deepvis-mid} while simultaneously injecting malware. Table~\ref{tab:stress_results} presents the results. Despite 100 concurrent file operations generating high G-channel noise ($G=0.60$ due to \texttt{/tmp} path), both injected malware samples were successfully detected: the hidden rootkit triggered via the hidden file modifier ($G=0.80$), and the packed malware triggered via high entropy ($R=1.00$). This demonstrates that \DeepVis's multi-modal approach maintains detection capability even during heavy I/O churn.

\begin{table}[h]
\centering
\caption{Detection Under Sysbench I/O Stress on GCP \texttt{deepvis-mid}.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\textbf{Category} & \textbf{File} & \textbf{R} & \textbf{G} & \textbf{B} & \textbf{Result} \\
\hline
Noise (sysbench) & \texttt{test\_file.*} & 0.00 & 0.60 & 0.00 & Clean \\
\hline
Malware (hidden) & \texttt{.hidden\_rootkit.ko} & 0.05 & \textbf{0.80} & 0.00 & \textbf{Detected} \\
Malware (packed) & \texttt{.packed\_malware} & \textbf{1.00} & \textbf{0.80} & 0.00 & \textbf{Detected} \\
\hline
\end{tabular}%
}
\label{tab:stress_results}
\end{table}

\noindent\textbf{Hyperscale Saturation Test (RQ6).}
To verify robustness under extreme grid collision, we performed a real saturation test on GCP \texttt{deepvis-mid} using all available files (204,570 files from \texttt{/usr}, \texttt{/etc}, \texttt{/var}, \texttt{/home}). Table~\ref{tab:hyperscale_results} presents the results. At 100K files, 99.8\% of pixels were occupied with 6.1 average collisions per pixel. At full capacity (204K files), the grid reached 100\% saturation with 12.5 collisions per pixel. Critically, all 10 injected attack files were detected with \textbf{100\% recall}, verifying that Max-Pooling aggregation preserves sparse attack signals against legitimate file collisions.

\begin{table}[h]
\centering
\caption{Real Hyperscale Saturation Test on GCP \texttt{deepvis-mid} ($128 \times 128$ Grid).}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{|r|c|r|c|c|}
\hline
\textbf{Files} & \textbf{Grid Saturation} & \textbf{Avg Collisions/Pixel} & \textbf{Attacks Detected} & \textbf{Recall} \\
\hline
10,000    & 45.4\% & 1.3 & 10/10 & 100\% \\
50,000    & 95.3\% & 3.2 & 10/10 & 100\% \\
100,000   & 99.8\% & 6.1 & 10/10 & 100\% \\
204,570   & 100.0\% & 12.5 & 10/10 & \textbf{100\%} \\
\hline
\end{tabular}%
}
\label{tab:hyperscale_results}
\end{table}

%=====================================================================
\subsection{SOTA Comparison (RQ5)}
\label{eval_sota}
%=====================================================================

To validate the $O(1)$ scalability and I/O bottleneck hypothesis, we conducted a rigorous \textbf{Unified Benchmark} on GCP \texttt{deepvis-mid}, measuring both \textbf{Runtime Interference} (Phase 1) and \textbf{Scan Latency} (Phase 2). We compared \DeepVis against \textbf{AIDE}, \textbf{Osquery}, and \textbf{Falco}.

\begin{table}[h]
\centering
\caption{Unified SOTA Benchmark: Interference vs. Latency.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Tool} & \textbf{Detection Mode} & \textbf{Phase 1: On-the-fly Penalty} & \textbf{Phase 2: Static Latency} \\
\hline
Falco & Runtime & 2.5\% (Always On) & N/A (Continuous) \\
AIDE & Static & 31.2\% (Blocking I/O) & 8.2s (Slow) \\
Osquery & Static & 29.1\% (CPU Burst) & 17.0s (Slow) \\
\hline
\textbf{\DeepVis} & \textbf{Static} & \textbf{4.1\% (Non-Blocking)} & \textbf{0.23s (Fast)} \\
\hline
\end{tabular}%
}
\label{tab:sota_comparison}
\end{table}

\noindent\textbf{Analysis.}
Table~\ref{tab:sota_comparison} reveals the critical trade-off in existing tools. \textbf{Phase 1 (Interference)}: Legacy scanners (AIDE, Osquery) degrade Nginx web server throughput by $\approx$30\% during execution due to heavy I/O contention. \DeepVis, however, imposes only a 4.1\% penalty, comparable to the lightweight runtime monitor Falco (2.5\%). \textbf{Phase 2 (Latency)}: \DeepVis completes the integrity verification in \textbf{0.23 seconds}---a \textbf{35$\times$ speedup} over AIDE (8.2s) and \textbf{74$\times$} over Osquery (17.0s). This confirms that \DeepVis uniquely combines the low overhead of runtime monitors with the comprehensive coverage of static scanners.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/fig_sota_timeline.pdf}
\caption{60-second sustained load test comparing system resource consumption and application throughput under different security tools. Top: CPU usage. Middle: Memory usage. Bottom: Nginx throughput (RPS). \DeepVis maintains performance comparable to Baseline/Falco, while AIDE and Osquery cause sustained resource contention.}
\label{fig:sota_timeline}
\end{figure}

%=====================================================================
\subsection{Resource Overhead Analysis}
\label{eval_overhead}
%=====================================================================

Figure~\ref{fig:overhead} depicts the system resource utilization during a continuous scan cycle. \DeepVis exhibits a minimal resource footprint, with CPU usage averaging below 5\% on the Mid tier instance. Memory consumption stabilizes at approximately 72MB and does not grow with the size of the target filesystem, as the streaming architecture processes metadata in bounded batches.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/system_overhead.png}
  \caption{System overhead during DeepVis scan. CPU usage remains below 5\% on average, and memory usage is stable at approximately 72MB.}
  \label{fig:overhead}
\end{figure}