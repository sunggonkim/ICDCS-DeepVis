\section{Evaluation}
\label{sec:evaluation}

We evaluate DeepVis on a production Google Cloud Platform (GCP) infrastructure using real compiled rootkits and realistic attack scenarios. Our evaluation aims to answer the following research questions:

\begin{itemize}
    \item \textbf{RQ1 (Detection Accuracy):} Can the multi-modal RGB encoding distinguish between high-entropy packed malware and low-entropy native rootkits?
    \item \textbf{RQ2 (Scalability):} Does the system maintain constant-time inference performance as the file system scales to millions of files?
    \item \textbf{RQ3 (Churn Tolerance):} Does the Local Max ($L_\infty$) detection eliminate false positives during legitimate system updates?
    \item \textbf{RQ4 (Feature Orthogonality):} Do the R, G, and B channels independently capture distinct classes of attack vectors?
    \item \textbf{RQ5 (System Overhead):} Does the agentless architecture maintain low resource utilization on the host kernel?
    \item \textbf{RQ6 (Adversarial Robustness):} Is the system resilient against adaptive attackers attempting to evade detection via entropy manipulation?
\end{itemize}

%=====================================================================
\subsection{Experimental Methodology}
\label{eval_setup}
%=====================================================================

\noindent\textbf{Testbed Environment.} 
We conduct experiments on three distinct hardware configurations representing a spectrum of cloud instances, as detailed in Table~\ref{tab:hardware_specs}. The primary evaluation uses the High tier (c2-standard-4) to demonstrate performance on modern NVMe storage. To simulate a production environment, we populated the file system with a diverse set of benign artifacts, including system binaries (e.g., \texttt{nginx}, \texttt{gcc}), configuration files, and Python scripts, scaling up to 50 million files for stress testing.

\begin{table}[h]
  \centering
  \caption{Hardware configurations for scalability evaluation.}
  \label{tab:hardware_specs}
  \resizebox{0.8\textwidth}{!}{%
  \begin{tabular}{|l|l|c|c|l|}
    \hline
    \textbf{Tier} & \textbf{Instance Type} & \textbf{vCPU} & \textbf{RAM} & \textbf{Storage Interface} \\
    \hline
    Low & e2-micro & 2 & 1GB & Standard HDD (SATA) \\
    Mid & e2-standard-2 & 2 & 8GB & SSD (SATA) \\
    High & c2-standard-4 & 4 & 16GB & NVMe SSD (PCIe) \\
    \hline
  \end{tabular}%
  }
\end{table}

\noindent\textbf{Multi-Modal Feature Definition.} 
Based on the design principles, we configured the RGB channels to capture orthogonal security properties:
\begin{itemize}
    \item \textbf{R (Red) = Information Density:} Measures Shannon entropy to detect packed or encrypted payloads.
    \item \textbf{G (Green) = Contextual Hazard:} Aggregates environmental risk factors. The score is computed as $G = \min(1.0, \; P_{path} + P_{pattern} + P_{hidden} + P_{perm})$, where weights are assigned to volatile paths ($P_{path}$), dangerous syscall patterns ($P_{pattern}$), and permission anomalies ($P_{perm}$).
    \item \textbf{B (Blue) = Structural Deviation:} Detects type mismatches (e.g., ELF headers in text files) and anomalous zero-byte sparsity in binaries.
\end{itemize}

\noindent\textbf{Threshold Learning.} 
We employed a maximum-margin approach to determine detection boundaries. The thresholds were learned from the benign baseline as $\tau_c = \max(\text{Benign}_c) + 0.1$, ensuring a 0\% False Positive Rate during calibration. This resulted in $\tau_R=0.75$, $\tau_G=0.25$, and $\tau_B=0.30$.

%=====================================================================
\subsection{Detection Accuracy and Feature Orthogonality (RQ1, RQ4)}
\label{eval_accuracy}
%=====================================================================

We deployed seven realistic attack scenarios, ranging from high-entropy packed miners to low-entropy compiled rootkits. Table~\ref{tab:v3_results} summarizes the detection results. DeepVis successfully detected 100\% of the malicious artifacts with zero false positives against the benign baseline.

\begin{table*}[t]
\centering
\caption{\textbf{Detection Accuracy on Real GCP Workloads.} We deployed 15 malware artifacts from public repositories on \texttt{deepvis-mid}, staging them in attack-realistic locations (\texttt{/var/tmp}, \texttt{/dev/shm}, \texttt{/var/www}, \texttt{\textasciitilde/.ssh}). DeepVis achieved 100\% recall with 0\% FPR across 25 test samples. Thresholds: $\tau_R$=0.75, $\tau_G$=0.25, $\tau_B$=0.30.}
\label{tab:v3_results}
\begin{tabular}{ll l ccc c}
\toprule
\textbf{Category} & \textbf{Artifact} & \textbf{Staged Location} & \textbf{R} & \textbf{G} & \textbf{B} & \textbf{Result} \\
\midrule
\multicolumn{7}{l}{\textit{\textbf{Real-World Malware (GitHub)}}} \\
LKM Rootkit & \texttt{Diamorphine}\footnotemark[1] & \texttt{/var/tmp/nvidia.ko} & 0.52 & \textbf{0.60} & \textbf{0.50} & \textbf{Detected} \\
LD\_PRELOAD Rootkit & \texttt{Azazel}\footnotemark[2] & \texttt{/var/tmp/libsystem.so} & 0.37 & \textbf{0.60} & 0.00 & \textbf{Detected} \\
Crypto Miner & \texttt{XMRig}\footnotemark[3] & \texttt{/var/tmp/systemd/kthreadd} & 0.32 & \textbf{0.60} & 0.00 & \textbf{Detected} \\
Packed Miner & \texttt{kworker-upd} & \texttt{/dev/shm/.systemd-private} & \textbf{0.88} & \textbf{0.90} & 0.40 & \textbf{Detected} \\
Encrypted Rootkit & \texttt{azazel\_enc.so} & \texttt{/dev/shm/cache/.enc} & \textbf{1.00} & \textbf{0.90} & \textbf{0.80} & \textbf{Detected} \\
Ransomware & \texttt{Cerber}\footnotemark[4] & \texttt{/var/tmp/.update.log} & 0.68 & \textbf{0.80} & \textbf{0.90} & \textbf{Detected} \\
Ransomware & \texttt{WannaCry}\footnotemark[4] & \texttt{/dev/shm/cache/.service} & 0.51 & \textbf{0.90} & 0.00 & \textbf{Detected} \\
Webshell & \texttt{.config.php} & \texttt{/var/www/html/.config.php} & 0.58 & \textbf{0.70} & 0.00 & \textbf{Detected} \\
Reverse Shell & \texttt{rev\_shell} & \texttt{/dev/shm/rev\_shell} & \textbf{1.00} & \textbf{0.70} & 0.00 & \textbf{Detected} \\
Disguised ELF & \texttt{access.log} & \texttt{/var/log/access.log} & 0.55 & 0.00 & \textbf{1.00} & \textbf{Detected} \\
Cron Backdoor & \texttt{.cron\_job} & \texttt{/var/tmp/.cron\_job} & 0.37 & \textbf{0.80} & 0.00 & \textbf{Detected} \\
SSH Key Inject & \texttt{.backdoor\_key} & \texttt{\textasciitilde/.ssh/.backdoor\_key} & 0.52 & \textbf{0.35} & 0.00 & \textbf{Detected} \\
Ransomware & \texttt{Petya}\footnotemark[4] & \texttt{/var/tmp/petya.bin} & \textbf{0.78} & \textbf{0.60} & 0.00 & \textbf{Detected} \\
Banking Trojan & \texttt{Emotet}\footnotemark[4] & \texttt{/var/tmp/svchost.bin} & \textbf{0.78} & \textbf{0.60} & 0.00 & \textbf{Detected} \\
ATM Backdoor & \texttt{Tyupkin}\footnotemark[4] & \texttt{/var/tmp/.atm\_svc} & 0.44 & \textbf{0.80} & 0.00 & \textbf{Detected} \\
\midrule
\multicolumn{7}{l}{\textit{\textbf{Benign System Files}}} \\
Python Interpreter & \texttt{python3} & \texttt{/usr/bin/python3} & 0.67 & 0.00 & 0.00 & Clean \\
Package Manager & \texttt{apt} & \texttt{/usr/bin/apt} & 0.32 & 0.00 & 0.00 & Clean \\
Service Manager & \texttt{systemctl} & \texttt{/usr/bin/systemctl} & 0.20 & 0.00 & 0.00 & Clean \\
SSH Client & \texttt{ssh} & \texttt{/usr/bin/ssh} & 0.19 & 0.00 & 0.00 & Clean \\
HTTP Client & \texttt{curl} & \texttt{/usr/bin/curl} & 0.38 & 0.00 & 0.00 & Clean \\
Web Server & \texttt{nginx} & \texttt{/usr/sbin/nginx} & 0.67 & 0.00 & 0.00 & Clean \\
System Shell & \texttt{bash} & \texttt{/bin/bash} & 0.51 & 0.00 & 0.00 & Clean \\
User Database & \texttt{passwd} & \texttt{/etc/passwd} & 0.60 & 0.00 & 0.00 & Clean \\
System Log & \texttt{syslog} & \texttt{/var/log/syslog} & 0.63 & 0.00 & 0.00 & Clean \\
Core Library & \texttt{libc.so.6} & \texttt{/lib/x86\_64.../libc.so.6} & 0.66 & 0.00 & 0.00 & Clean \\
\bottomrule
\end{tabular}
\footnotetext[1]{\url{https://github.com/m0nad/Diamorphine}}
\footnotetext[2]{\url{https://github.com/chokepoint/azazel}}
\footnotetext[3]{\url{https://github.com/xmrig/xmrig}}
\footnotetext[4]{\url{https://github.com/ytisf/theZoo}}
\end{table*}

\noindent\textbf{Analysis of Low-Entropy Malware (The MSE Paradox).} 
A critical finding from our real-world deployment is that many modern attacks do \textit{not} exhibit the high entropy typically associated with packing. As shown in Table~\ref{tab:v3_results}, the real-world Miner (\texttt{kworker-upd}) and Rootkits (\texttt{diamorphine}) exhibited entropy scores ($R \approx 0.55$) indistinguishable from benign system binaries ($R \approx 0.61$). Attempts to detect these solely via entropy (R-channel) failed, validating our hypothesis that single-modal detection is insufficient. However, DeepVis successfully flagged these artifacts via the Structural (B) and Contextual (G) channels: the Miner triggered a high Context Hazard ($G=0.60$) due to its anomalous path, and the Rootkits triggered Structural Deviation ($B=0.50$) due to their relocatable ELF type (\texttt{ET\_REL}) in a temporary directory.

\begin{figure}[t]
    \centering
    \subfloat[Clean Baseline (10K Benign Files)]{\includegraphics[width=0.48\linewidth]{Figures/tensor_clean.png}\label{fig:tensor_clean}}
    \hfill
    \subfloat[Infected State (10K Benign + 10 Malware)]{\includegraphics[width=0.48\linewidth]{Figures/tensor_infected.png}\label{fig:tensor_infected}}
    \caption{Tensor Visualization on GCP \texttt{deepvis-mid}. (a) Clean baseline with 10K benign files from \texttt{/usr}. (b) Infected state with 10 malware files injected into \texttt{/tmp}, \texttt{/var/tmp}, and \texttt{/dev/shm}. Red circles indicate malware locations detected via G-channel (contextual hazard) activation.}
    \label{fig:clean_vs_infected}
\end{figure}

\noindent\textbf{Visual Isolation.} 
Figure~\ref{fig:clean_vs_infected} visualizes the multi-modal detection capability of DeepVis. The clean baseline (a) shows diffuse, low-intensity background noise from 10K benign files in \texttt{/usr}. The infected state (b) clearly reveals malware locations (cyan circles) via G-channel activationâ€”files placed in volatile paths (\texttt{/tmp}, \texttt{/dev/shm}) trigger high contextual hazard scores regardless of their entropy. This confirms that DeepVis provides robust detection even for low-entropy, non-packed rootkits.

%=====================================================================
\subsection{Scalability and Performance Analysis (RQ2)}
\label{eval_scalability}
%=====================================================================

The primary architectural claim of DeepVis is the decoupling of verification latency from file system size. Figure~\ref{fig:scalability} illustrates the execution time comparison between DeepVis and AIDE across the defined hardware tiers. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/scalability_rq2.png}
  \caption{Throughput comparison between DeepVis and AIDE. DeepVis achieves up to 7.7$\times$ speedup on the High tier by leveraging asynchronous I/O and parallel spatial hashing.}
  \label{fig:scalability}
\end{figure}

\noindent\textbf{Throughput Analysis.} 
Table~\ref{tab:throughput_results} presents the detailed throughput comparison between DeepVis and AIDE across different file counts on the Mid tier (e2-standard-2). DeepVis consistently achieves over 60,000 files/sec using the Rust-based scanner with header-only entropy calculation, compared to AIDE's 7,000--15,000 files/sec with full SHA-256 hashing. The speedup ranges from 4.0$\times$ at 5,000 files to 9.9$\times$ at 10,000 files, averaging 8.0$\times$ across all workloads.

\begin{table}[h]
\centering
\caption{DeepVis vs AIDE Throughput on GCP Mid-tier (e2-standard-2).}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|r|r|r|r|r|c|}
\hline
\textbf{Files} & \textbf{DeepVis (s)} & \textbf{AIDE (s)} & \textbf{DeepVis (files/s)} & \textbf{AIDE (files/s)} & \textbf{Speedup} \\
\hline
1,000 & 0.016 & 0.134 & 63,350 & 7,438 & 8.5$\times$ \\
5,000 & 0.082 & 0.330 & 60,333 & 15,160 & 4.0$\times$ \\
10,000 & 0.161 & 1.601 & 61,294 & 6,245 & 9.9$\times$ \\
50,000 & 0.699 & 6.791 & 60,471 & 7,363 & 9.7$\times$ \\
\hline
\multicolumn{5}{|r|}{\textbf{Average Speedup}} & \textbf{8.0$\times$} \\
\hline
\end{tabular}%
}
\label{tab:throughput_results}
\end{table}

\noindent\textbf{Inference Latency Decomposition.}
To validate the $O(1)$ inference complexity claim, we decompose the total execution time into the \textit{Snapshot Phase} (I/O-bound) and the \textit{Inference Phase} (Compute-bound). Figure~\ref{fig:latency} demonstrates that while the Snapshot phase scales linearly ($O(N)$) with the number of files, the Inference phase remains strictly constant at approximately 5ms. This is a direct consequence of the Hash-Based Spatial Mapping, which projects the variable-cardinality file system into a fixed-size $128 \times 128$ tensor prior to neural network ingestion.

\noindent\textbf{Batch Inference Scalability.}
Figure~\ref{fig:batch_inference} demonstrates the scalability of our 1$\times$1 Convolution-based detector (equivalent to pixel-wise MLP) across varying batch sizes. We measured real execution times on \texttt{deepvis-mid} using 20,000 files from \texttt{/usr}. Tensor generation time scales linearly (4.1s for 1 image to 45.8s for 100 images, approximately 0.46s per image). Critically, the inference time remains sub-linear: 0.18ms for 1 image to 9.1ms for 100 images, averaging 0.09ms per image. This validates that the 1$\times$1 kernel architecture enables near-constant per-pixel inference cost, making DeepVis suitable for real-time batch monitoring of hyperscale fleets.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/fig_scalability_inference.pdf}
    \caption{Batch Scalability of DeepVis on GCP \texttt{deepvis-mid}. (a) Tensor generation time scales linearly with image count. (b) 1$\times$1 Conv inference remains sub-10ms even for 100 images, averaging 0.09ms per image.}
    \label{fig:batch_inference}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/rq8_latency.png}
  \caption{Latency decomposition of DeepVis. The inference time remains constant ($O(1)$) due to fixed-size tensor projection, while I/O scales linearly ($O(N)$).}
  \label{fig:latency}
\end{figure}

%=====================================================================
\subsection{System Robustness and Sensitivity (RQ3, RQ6)}
\label{eval_robustness}
%=====================================================================

A critical failure mode of traditional FIM is the inability to distinguish between legitimate system maintenance (churn) and malicious modifications. 

\noindent\textbf{Sensitivity Analysis.}
Figure~\ref{fig:sensitivity_heatmap} presents a heatmap of detection confidence (L$\infty$ score) as a function of background noise (100 to 50,000 benign files) and attack signal strength. The results demonstrate a \textit{Region of Stability}: even with 50,000 files causing 95\% grid saturation, attacks with signal strength $\geq 0.5$ are detected with 100\% accuracy. This robustness stems from the Local Max detection strategy, which isolates the single worst violation regardless of the global noise distribution.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/fig8_sensitivity_heatmap.png}
  \caption{Sensitivity heatmap showing detection confidence across noise levels and attack strengths. The ``Region of Stability'' demonstrates that DeepVis maintains detection fidelity even under high background churn.}
  \label{fig:sensitivity_heatmap}
\end{figure}

\noindent\textbf{Methodology Comparison (ROC).}
To mathematically validate the superiority of Local Max (L$\infty$) detection over traditional Mean Squared Error (MSE), we plot Receiver Operating Characteristic (ROC) curves in Figure~\ref{fig:roc_curve}. The L$\infty$ method achieves an Area Under Curve (AUC) of \textbf{0.93}, compared to 0.50 for MSE. The sharp ``elbow'' indicates that L$\infty$ can achieve near-perfect true positive rates with minimal false alarms, effectively solving the MSE Paradox.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/fig9_roc_curve.png}
  \caption{ROC curve comparison. L$\infty$ detection (AUC=0.93) outperforms MSE (AUC=0.50) by 86\%, demonstrating that local maximum deviation is superior for detecting sparse anomalies.}
  \label{fig:roc_curve}
\end{figure}

\noindent\textbf{Hyperscale Saturation.}
Table~\ref{tab:hyperscale_results} presents the results of stress-testing the Hash-Based Spatial Mapping at extreme scale. Even with 50 million files causing 3,052 average collisions per pixel on a $128 \times 128$ grid (100\% saturation), the attack detection recall remains at \textbf{100\%}. This is guaranteed by the Max-Pooling aggregation: for any pixel, the final value is determined by the highest feature score, ensuring malicious files are never masked by benign collisions.

\begin{table}[h]
\centering
\caption{Hyperscale Saturation Test Results on $128 \times 128$ Grid.}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{|r|c|r|c|c|}
\hline
\textbf{Files} & \textbf{Grid Saturation} & \textbf{Avg Collisions/Pixel} & \textbf{Attacks Detected} & \textbf{Recall} \\
\hline
100,000 & 99.8\% & 6.1 & 10/10 & 100\% \\
1,000,000 & 100\% & 61.0 & 10/10 & 100\% \\
10,000,000 & 100\% & 610.4 & 10/10 & 100\% \\
50,000,000 & 100\% & 3,051.8 & 10/10 & \textbf{100\%} \\
\hline
\end{tabular}%
}
\label{tab:hyperscale_results}
\end{table}

%=====================================================================
\subsection{Resource Overhead Analysis (RQ5)}
\label{eval_overhead}
%=====================================================================

Figure~\ref{fig:overhead} depicts the system resource utilization during a continuous scan cycle. DeepVis exhibits a minimal resource footprint, with CPU usage averaging below 5\% on the Mid tier instance. Memory consumption stabilizes at approximately 72MB and does not grow with the size of the target filesystem, as the streaming architecture processes metadata in bounded batches. This predictable resource profile validates that DeepVis is suitable for co-location with latency-sensitive production workloads.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/system_overhead.png}
  \caption{System overhead during DeepVis scan. CPU usage remains below 5\% on average, and memory usage is stable at approximately 72MB.}
  \label{fig:overhead}
\end{figure}