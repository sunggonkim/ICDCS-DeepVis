%=====================================================================
\subsection{Detection Accuracy and Feature Orthogonality (RQ1)}
\label{eval_accuracy}
%=====================================================================

\noindent\textbf{Baseline and Model Configuration.} 
Reference tools were executed with default configurations to represent realistic deployment scenarios: ClamAV utilized the standard virus database, and YARA employed a minimal rule set matching common malware keywords and ELF headers. For the proposed \DeepVis system, the CAE was trained exclusively on benign artifacts to establish a normality baseline, resulting in a calibrated anomaly detection threshold of $\tau_{L_{\infty}} = 0.52$. To isolate the architectural benefit of this learned representation, we defined ``DeepVis (Entropy)'' as a heuristic baseline that applies a static threshold solely to the R-channel without multi-modal fusion.

\noindent\textbf{Platform-Specific Analysis.}
Table~\ref{tab:unified_detection} presents the detection recall across four dominant platforms using tuned baselines. ClamAV (Tuned) achieves high recall (95.4--96.6\%) on Linux and Windows by explicitly hashing known samples, relying on prior knowledge infeasible for zero-day variants. YARA (Tuned) shows moderate performance on Web (68.1\%) but incurs a high false positive rate (31.0\%) on benign files due to aggressive heuristic rules. In contrast, \DeepVis demonstrates exceptional generalization on binary-heavy platforms without prior knowledge. It achieves 97.1\% Recall on Linux and 100.0\% on Mobile, significantly outperforming heuristic rules. While Windows detection (15.8\%) is lower than signature-matched tools, it represents a significant capability for a Linux-trained model to flag anomalous PE structures. Crucially, \DeepVis maintains a negligible False Positive rate of 0.3\%, confirming that the learned normality model is far more robust to benign variations than static heuristics.

%---------------------------------------------------------------------
% TABLE III: Unified Performance
%---------------------------------------------------------------------
\begin{table}[t]
\centering
\caption{\textbf{Unified Detection Performance.} Comparison of \DeepVis against Tuned Baselines. While ClamAV (Tuned with known hashes) sets a theoretical upper bound, \DeepVis achieves superior recall on Linux/Mobile threats via unsupervised learning, with significantly lower false positives (0.3\%) than heuristic rules (YARA: 31.0\%).}
\label{tab:unified_detection}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l cccc c}
\toprule
\multirow{2}{*}{\textbf{System}} & \multicolumn{4}{c}{\textbf{Active Recall by Platform}} & \textbf{False Pos.} \\
& \textbf{Linux} & \textbf{Windows} & \textbf{Web} & \textbf{Mobile} & \textbf{(Benign)} \\
\midrule
ClamAV (Tuned)$^\dagger$ & 95.4\% & \textbf{96.6\%} & 82.6\% & 50.0\% & \textbf{0.0\%} \\
YARA (Tuned)$^\ddagger$ & 24.6\% & 2.6\% & 68.1\% & 79.2\% & 31.0\% \\
AIDE & \textbf{100.0\%} & \textbf{100.0\%} & \textbf{100.0\%} & \textbf{100.0\%} & 100.0\% \\
\midrule
\rowcolor{gray!10} 
\textbf{\DeepVis} & \textbf{97.1\%} & 15.8\% & \textbf{89.6\%} & \textbf{100.0\%} & \textbf{0.3\%} \\
\bottomrule
\multicolumn{6}{l}{\scriptsize $^\dagger$Using custom DB of known hashes. $^\ddagger$Using custom rules for LKM/Webshells.}
\end{tabular}%
}
\end{table}

\noindent\textbf{Failure Mode Analysis.} 
Table~\ref{tab:detailed_breakdown} provides a granular analysis of detection capabilities and limitations. \DeepVis detects evasive threats through feature orthogonality. For instance, the rootkit Diamorphine evaded the Entropy channel ($R=0.52$) but was detected by the Context ($G=0.60$) and Structure ($B=0.50$) channels due to its nature as a kernel module residing in a temporary directory. Similarly, Azazel was identified via high Entropy ($R=1.00$) and Context anomalies ($G=0.90$). However, the header-only approach exhibits intrinsic blind spots against non-binary threats. As shown in the failure cases of Table~\ref{tab:detailed_breakdown}, \DeepVis failed to detect the public webshell \texttt{c99.php} and the DDoS tool \texttt{VirTool.TCP.a}. These files reside in structurally valid paths and lack binary packing anomalies, making them indistinguishable from benign scripts via headers alone. This limitation confirms that \DeepVis operates as a high-speed first-line defense for binary integrity rather than a full-content forensic scanner.

%---------------------------------------------------------------------
% TABLE IV: Detailed Breakdown
%---------------------------------------------------------------------
\begin{table}[t]
\centering
\caption{\textbf{Detailed Detection Analysis.} Multi-modal RGB features catch threats that single metrics miss. The "Miss" cases highlight the limitation against threats that perfectly mimic benign header statistics.}
\label{tab:detailed_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c ccc c}
\toprule
\textbf{Type} & \textbf{Name} & \textbf{R} & \textbf{G} & \textbf{B} & \textbf{Status} \\
\midrule
\multicolumn{6}{l}{\textit{Detected Active Threats}} \\
LKM Rootkit & \texttt{Diamorphine} & 0.52 & \textbf{0.60} & \textbf{0.50} & Det. \\
LD\_PRELOAD & \texttt{Azazel} & 0.37 & \textbf{0.60} & 0.00 & Det. \\
Crypto Miner & \texttt{XMRig} & 0.32 & \textbf{0.60} & 0.00 & Det. \\
Encrypted RK & \texttt{azazel\_enc} & \textbf{1.00} & \textbf{0.90} & \textbf{0.80} & Det. \\
Rev. Shell & \texttt{rev\_shell} & \textbf{1.00} & \textbf{0.70} & 0.00 & Det. \\
Disguised ELF & \texttt{access.log} & 0.55 & 0.00 & \textbf{1.00} & Det. \\
\midrule
\multicolumn{6}{l}{\textit{Undetected (Limitations)}} \\
Webshell & \texttt{c99.php} & 0.58 & 0.00 & 0.00 & Miss \\
Mimicry ELF & \texttt{libc\_fake.so} & 0.61 & 0.00 & 0.00 & Miss \\
DDoS Tool & \texttt{VirTool.TCP.a} & 0.58 & 0.00 & 0.00 & Miss \\
\midrule
\multicolumn{6}{l}{\textit{Benign Baselines (Clean)}} \\
Interpreter & \texttt{python3} & 0.67 & 0.00 & 0.00 & Clean \\
Library & \texttt{libc.so.6} & 0.66 & 0.00 & 0.00 & Clean \\
Image (PNG) & \texttt{ubuntu-logo} & 0.53 & 0.00 & 0.00 & Clean \\
\bottomrule
\end{tabular}
}
\end{table}
%---------------------------------------------------------------------
% TABLE IV: Detailed Breakdown (Micro View)
%---------------------------------------------------------------------
\begin{table}[t]
\centering
\caption{\textbf{Detailed Detection Analysis.} Multi-modal RGB features catch threats that single metrics miss. The table presents representative samples from our Benign (N=667) and Malware (N=68) datasets. The "Miss" cases highlight limitations against threats that mimic benign header statistics.}
\label{tab:detailed_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c ccc c}
\toprule
\textbf{Type} & \textbf{Name} & \textbf{R} & \textbf{G} & \textbf{B} & \textbf{Status} \\
\midrule
\multicolumn{6}{l}{\textit{Detected Active Threats (Multi-Platform)}} \\
LKM Rootkit & \texttt{Diamorphine} & 0.55 & \textbf{0.81} & \textbf{1.00} & Det. \\
Windows Spy & \texttt{FormGrab.exe} & 0.75 & 0.57 & \textbf{0.90} & Det. \\
Android Mal & \texttt{DEX Dropper} & \textbf{1.00} & 0.57 & \textbf{1.00} & Det. \\
Webshell & \texttt{TDshell.php} & 0.69 & 0.72 & \textbf{0.60} & Det. \\
Encrypted RK & \texttt{azazel\_enc} & \textbf{1.00} & \textbf{0.90} & \textbf{0.80} & Det. \\
\midrule
\multicolumn{6}{l}{\textit{Undetected (Limitations)}} \\
Obfuscated & \texttt{libc\_fake.so} & 0.61 & 0.00 & 0.00 & Miss \\
Mimicry Script & \texttt{setup.sh} & 0.58 & 0.00 & 0.00 & Miss \\
\midrule
\multicolumn{6}{l}{\textit{Benign Baselines (Clean)}} \\
Interpreter & \texttt{python3} & 0.78 & 0.96 & 0.10 & Clean \\
Library & \texttt{libc.so.6} & 0.79 & 0.90 & 0.10 & Clean \\
\multicolumn{6}{l}{\textit{False Positives (High Entropy)}} \\
Admin Tool & \texttt{snap} & \textbf{0.75} & \textbf{1.00} & 0.10 & False Pos. \\
Config Gen & \texttt{cloud-init} & 0.72 & 0.85 & 0.30 & False Pos. \\
\bottomrule
\end{tabular}
}
\end{table}

\noindent\textbf{Comparison with Set-based Approaches.} 
To evaluate the architectural advantage of the Hash-Grid Parallel CAE, we implemented a Set-based Autoencoder (Set-AE) baseline following the Deep Sets framework~\cite{zaheer2017deepsets}. As shown in Table~\ref{tab:unified_detection}, Set-AE fails to isolate sparse threats, achieving only 40\% recall on rootkits. This poor performance stems from the global feature pooling mechanism, which dilutes the signal of a single malicious file ($N=1$) against the variance of thousands of benign system files. In contrast, \DeepVis projects files onto a fixed Spatial Grid and employs $L_\infty$ pooling, ensuring that sparse anomalies remain locally distinct spikes rather than being averaged out globally.

%=====================================================================
\subsection{Scalability and Performance Analysis (RQ2)}
\label{eval_scalability}
%=====================================================================

The primary architectural claim of \DeepVis is the decoupling of verification latency from file system size. We validate this through two distinct lenses: processing throughput (micro-benchmark) and service interference (macro-benchmark).

\begin{figure}[t]
    \centering
    \subfloat[Throughput]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_final_throughput.pdf}
        \label{fig:perf_throughput}
    } \hfill
    \subfloat[Interference]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_final_latency.pdf}
        \label{fig:perf_latency}
    }
    \caption{\textbf{Comprehensive Performance Analysis.} (a) \textbf{Throughput}: DeepVis achieves hyperscale speeds ($\approx$40k files/s) via asynchronous I/O, outperforming synchronous baselines. (b) \textbf{Interference}: Despite its speed, \DeepVis maintains negligible latency overhead (+2\%) compared to massive spikes caused by AIDE (+291\%) and YARA (+547\%).}
    \label{fig:perf_analysis}
\end{figure}

\subsubsection{Micro-benchmark}
\noindent\textbf{Scan Throughput. }Figure~\ref{fig:perf_analysis}(a) compares DeepVis against AIDE to demonstrate operational feasibility. AIDE performs full-file cryptographic hashing, providing strong integrity guarantees but incurring $O(N \times Size)$ I/O complexity. This heavy I/O load often forces operators to restrict scanning to weekly maintenance windows. On a GCP High tier (c2-standard-4), \DeepVis achieves a 7.7$\times$ speedup over standard AIDE. Even against an optimized Partial-Hash AIDE baseline that reads only the first 128 bytes, \DeepVis maintains a 5.4$\times$ throughput advantage. This gain confirms that the performance boost stems not just from reading less data, but from the parallel \texttt{io\_uring} pipeline, which effectively hides I/O latency through massive concurrent queuing.

\noindent\textbf{Comparison with Commercial Scanners. }Benchmarking against fuzzy hashing (ssdeep) and signature scanners (ClamAV, YARA) on the full \texttt{/usr} directory (240,827 files) reveals that traditional tools are bottlenecked by synchronous content reads (127--1,004 files/s). In contrast, \DeepVis achieves 39,993 files/s, representing a 40$\times$ to 215$\times$ speedup over the baselines. This throughput demonstrates the efficiency of the asynchronous snapshot engine in hyperscale environments.

\subsubsection{Macro-benchmark}
\noindent\textbf{Service Interference. }Figure~\ref{fig:perf_analysis}(b) illustrates the P99 latency of a co-located NGINX web server during a full system scan. While raw throughput is critical, interference defines the operational constraint. Traditional tools severely impact system responsiveness; YARA and Heuristic engines cause degradation of +546\% and +324\% respectively due to CPU-intensive pattern matching. AIDE induces a +291\% latency spike (12.1ms) due to blocking I/O operations. In contrast, \DeepVis maintains a P99 latency of 3,162$\mu$s, reflecting a negligible +2.0\% overhead compared to the baseline (3,100$\mu$s). This confirms that the spatial hashing and asynchronous design allow the system to operate transparently in the background.

\noindent\textbf{CPU Resource Profile. }Resource contention analysis explains the latency results. Legacy FIMs and scanners such as Osquery and AIDE saturate the Global CPU at near 100\%, forcing the OS scheduler to throttle the web server. DeepVis, however, maintains a CPU profile of 11.2\%, nearly identical to the baseline (9.8\%). Unlike runtime monitors (e.g., Falco) which incur constant context-switching overhead (+58.3\% latency degradation), \DeepVis utilizes lightweight SIMD optimizations to ensure security monitoring remains strictly orthogonal to the primary service performance.


%=====================================================================
\subsection{Impact of Spatial Dimension and Hash Saturation (RQ3, RQ6)}
\label{eval_saturation}
%=====================================================================

We evaluate the structural limits of the fixed-size tensor representation, focusing on signal preservation against dimensional reduction and robustness against hash collisions.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/comparison_dilution.pdf} 
    \caption{Visualizing Signal Preservation. (Top) \DeepVis maintains spatial locality, isolating the malware as a distinct red peak ($L_\infty$ Spike). (Bottom) Set-AE averages the features into a single global vector, causing the attack signal to dilute into the background noise (Signal Dilution), resulting in detection failure.}
    \label{fig:signal_comparison}
\end{figure}

\noindent\textbf{Impact of Spatial Dimension. }Figure~\ref{fig:signal_comparison} compares the internal representations under an active attack scenario. The top panel demonstrates that the 2D Hash-Grid architecture maintains the spatial isolation of anomalies, manifesting injected malware as sharp, localized peaks against diffuse background noise. In contrast, the bottom panel shows that reducing the dimension to a single global vector (Set-AE) aggregates sparse attack signals with thousands of benign signals, washing out the anomaly. Quantitatively, measurements during a live system update confirm this observation. The global pooling approach fails to distinguish the attack from update noise, resulting in a negligible Signal-to-Noise Ratio (SNR) of 1.09. Conversely, the spatial isolation of \DeepVis yields a superior SNR of 2.71, ensuring robust detection even during high churn.

\noindent\textbf{Resilience to Hash Saturation. }To validate the stability of the hash mapping as the file count ($N$) exceeds the grid capacity ($W \times H$), we stress-tested the system by injecting up to 204,000 files into the $128 \times 128$ grid. Table~\ref{tab:hyperscale_saturation} shows that even at 99.99\% saturation (high collision state), the system maintains stability. Unlike traditional hash tables where collisions degrade performance to $O(N)$, our Max-Risk Pooling strategy ($\text{Grid}[h] = \max(\text{Grid}[h], s)$) ensures that tensor construction remains strictly $O(1)$. Collisions do not increase computational overhead; they merely aggregate risk scores, ensuring that detection latency remains constant regardless of file density.

\begin{table}[t]
\centering
\caption{Hash Saturation Analysis. High collision rates do not impact processing overhead due to $O(1)$ Max-Risk Pooling.}
\label{tab:hyperscale_saturation}
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{r c c}
\toprule
Files ($N$) & Grid Saturation & Avg. Collisions \\
\midrule
10,000 & 45.47\% & 0.61 \\
50,000 & 95.21\% & 3.05 \\
100,000 & 99.87\% & 6.10 \\
\rowcolor{gray!10} 
204,000 & 99.99\% & 12.45 \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Component Overhead. }Component analysis at scale (500K files) confirms that the hashing and mapping process is computationally efficient. The \texttt{io\_uring} based file reading consumes 90.1\% of the total scan time, while hashing, tensor mapping, and CAE inference account for negligible overhead ($<3\%$). This validates that the Hash-Grid architecture effectively decouples detection complexity from file system size without introducing computational bottlenecks.



%=====================================================================
\subsection{Fleet-Scale Scalability (RQ7)}
\label{eval_fleet}
%=====================================================================

A key requirement for distributed systems is demonstrating scalability across a fleet of nodes under realistic network conditions. We evaluate the capability of \DeepVis to verify a large distributed cluster by deploying 100 concurrent nodes in a public cloud environment.

\noindent\textbf{Experimental Setup and Orchestration at Scale. }Deploying 100 concurrent nodes presents significant orchestration challenges, including API rate limits and network saturation. To mitigate these, we distributed the fleet across three geographically distant GCP regions: \texttt{us-central1} (Iowa), \texttt{us-east1} (South Carolina), and \texttt{us-west1} (Oregon). A hierarchical orchestration architecture was employed where a single bastion node (\texttt{deepvis-mid}) located in \texttt{asia-northeast3} (Seoul) coordinated the entire US-based fleet via GCP's internal VPC network. This cross-region control plane demonstrates effective management of global deployments without co-location. Each e2-micro node, provisioned with a custom Golden Image containing the Rust-based \DeepVis scanner, performed a full scan of local directories upon activation. The resulting $128 \times 128 \times 3$ RGB tensors were transmitted to the aggregator via the asynchronous protocol.

\begin{figure}[t]
  \centering
  \subfloat[Linear Scalability]{
    \includegraphics[width=0.8\linewidth]{Figures/fig_fleet_scalability.pdf}
    \label{fig:fleet_scale}
  } \\
  \subfloat[Geo-Stability]{
    \includegraphics[width=0.8\linewidth]{Figures/fig_fleet_vis.pdf}
    \label{fig:fleet_heatmap}
  }
  \caption{\textbf{Fleet-Scale Performance.} (a) \textbf{Linear Scalability:} Throughput increases linearly with fleet size, reaching $\approx$206k files/s at 100 nodes. (b) \textbf{Geo-Stability:} The latency heatmap across 100 nodes shows consistent performance (avg 4.29s) across three US regions, confirming resilience against network variance.}
  \label{fig:fleet_perf}
\end{figure}

\noindent\textbf{Linear Scalability. }Figure~\ref{fig:fleet_perf}(a) demonstrates the linear scaling capability of \DeepVis. As the fleet size scales from 1 to 100 nodes, the aggregate verification throughput grows proportionally, reaching \textbf{206,611 files/sec}. This confirms that the stateless nature of the Hash-Grid architecture effectively decouples the processing load, eliminating central bottlenecks.

\noindent\textbf{Global Stability. }Figure~\ref{fig:fleet_perf}(b) visualizes the scan latency distribution across the 100 nodes. Despite the geographical dispersion, \DeepVis maintains a tight latency bound with an average of 4.29s and a maximum of 6.0s. Crucially, the aggregation overhead for the entire fleet was merely 548ms. This validates that \DeepVis ensures consistent performance SLAs even in "noisy neighbor" public cloud environments.

\noindent\textbf{Network Efficiency. }Tensor-based verification drastically reduces bandwidth consumption. Each node transmits a fixed-size 49KB tensor regardless of file count, totaling only 4.9MB for the 100-node fleet. This represents a 100$\times$ reduction in network overhead compared to provenance-based systems, which can exceed 500MB under similar workloads.

\begin{table}[h]
\centering
\caption{\textbf{Component Time Breakdown (Cold Cache).} I/O dominates ($\approx$90\%), confirming that \\DeepVis computational overhead (Hashing, Entropy, Tensor Update) is negligible.}
\label{tab:overhead_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l rrrr}
\toprule
\textbf{Component} & \textbf{10K} & \textbf{100K} & \textbf{200K} & \textbf{500K} \\
\midrule
Traversal & 127ms (14\%) & 1.74s (11\%) & 3.93s (9.5\%) & 8.88s (7.0\%) \\
\textbf{I/O (Header Read)} & \textbf{675ms (77\%)} & \textbf{12.77s (84\%)} & \textbf{36.09s (87\%)} & \textbf{113.2s (89.9\%)} \\
Hashing & 24ms (2.8\%) & 209ms (1.4\%) & 418ms (1.0\%) & 1.31s (1.0\%) \\
Entropy Calc & 30ms (3.4\%) & 322ms (2.1\%) & 557ms (1.3\%) & 1.86s (1.5\%) \\
Tensor Update & 21ms (2.4\%) & 132ms (0.9\%) & 321ms (0.8\%) & 746ms (0.6\%) \\
\midrule
\textbf{Total Time} & \textbf{877ms} & \textbf{15.17s} & \textbf{41.31s} & \textbf{126.0s} \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Computational Overhead Analysis. }Table~\ref{tab:overhead_breakdown} presents the execution time breakdown across varying file counts ranging from 10K to 500K under a cold cache scenario. I/O operations dominate the runtime, consuming approximately 90\% of the total execution time at scale (500K files). Conversely, computational tasks, including hashing, entropy calculation, and tensor updates, collectively account for less than 3.1\% of the total time. This indicates that the computational overhead of the hash-grid mapping and feature extraction remains negligible, confirming that the performance of \DeepVis is limited effectively by storage bandwidth rather than CPU resources.



%=====================================================================
\subsection{Ablation Study: Component Contribution Analysis}
\label{eval_ablation}
%=====================================================================

To quantify the contribution of each architectural decision, we evaluated system performance by selectively disabling key components. Table~\ref{tab:ablation} separates \textbf{Performance-critical} components (affecting throughput) from \textbf{Accuracy-critical} components (affecting detection).

\begin{table}[t]
\centering
\caption{Component-wise Ablation Study. Performance components (I/O, Sampling) determine throughput; Accuracy components (Hash-Grid, RGB) determine detection capability.}
\label{tab:ablation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l l | c c}
\toprule
Category & Configuration & Rate (files/s) & F1-Score \\
\midrule
\multirow{3}{*}{\textbf{Performance}} 
    & Baseline (Sync + Full Read) & 1,455 & -- \\
    & + Async I/O (\texttt{io\_uring}) & 2,910 & -- \\
    & + Header Sampling (128B) & 39,993 & -- \\
\midrule
\multirow{3}{*}{\textbf{Accuracy}} 
    & Entropy Only (R-channel) & -- & 0.25 \\
    & + Hash-Grid ($L_\infty$) & -- & 0.35 \\
    & + RGB Fusion (Full DeepVis) & -- & \textbf{0.98} \\
\midrule
\rowcolor{gray!15}
\multicolumn{2}{l|}{\textbf{DeepVis (All Components)}} & \textbf{39,993} & \textbf{0.98} \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Performance Optimization. }Replacing synchronous operations with asynchronous I/O doubles the throughput from 1,455 files/s to 2,910 files/s. This indicates that eliminating kernel-user context switching overhead improves efficiency even when reading entire files. The most significant gain stems from header sampling. Limiting the read scope to the first 128 bytes increases the scan rate to 39,993 files/s, representing a 27$\times$ speedup. This confirms that minimizing I/O volume is the primary factor for achieving hyperscale monitoring capabilities.

\noindent\textbf{Detection Fidelity. }Relying solely on entropy results in a poor F1-score of 0.25 due to false positives from benign compressed files. Applying the hash-grid architecture with local max pooling raises the score to 0.35, as it preserves sparse signals that are otherwise diluted by global pooling. However, the definitive improvement is achieved through multi-modal fusion. Integrating context and structure channels with entropy enables combinatorial reasoning, increasing the F1-score to 0.98. This demonstrates that architectural isolation must be paired with orthogonal feature sets to distinguish malicious artifacts from legitimate system noise.






