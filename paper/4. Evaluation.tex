\section{Evaluation}
\label{sec:evaluation}

We evaluate \DeepVis on Google Cloud Platform (GCP) using real rootkits and realistic attack scenarios. Our evaluation quantifies whether the multi-modal RGB encoding distinguishes packed malware (RQ1), scales to millions of files (RQ2), tolerates system churn (RQ3), and outperforms legacy monitors (RQ4, RQ5).

%=====================================================================
\subsection{Evaluation Setup}
\label{eval_setup}
%=====================================================================
\noindent\textbf{Testbed Environment. }
Experiments utilize three GCP configurations: \textbf{Low} (e2-micro), \textbf{Mid} (e2-standard-2), and \textbf{High} (c2-standard-4, NVMe SSD). To simulate production environments, we populated the file system with up to 50 million files, including system binaries (e.g., \texttt{nginx}) and random artifacts.

\noindent\textbf{Target Datasets. }
We employ a \textbf{Benign Baseline} of 47,270 system binaries (Ubuntu 20.04) to measure false positives, and a \textbf{Malware Corpus} of 37,387 files, including 68 active Linux ELF rootkits (e.g., Diamorphine) and 35k+ Windows/Web artifacts. This diversity tests the OS-agnostic capability of our structural encoding.

%=====================================================================
\subsection{Detection Fidelity and Orthogonality (RQ1)}
\label{eval_accuracy}
%=====================================================================

\begin{table}[t]
\centering
\caption{\textbf{Unified Detection Performance.} Comparison against tuned baselines. DeepVis (Full) achieves superior recall on Linux/Mobile threats. False Positive Rate (FPR) is reported at the node/repository level (0.3\% per 10k files), ensuring manageable alert volumes for security operators.}
\label{tab:unified_detection}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l cccc c}
\toprule
\multirow{2}{*}{\textbf{System}} & \multicolumn{4}{c}{\textbf{Recall by Platform}} & \textbf{False Pos.} \\
& \textbf{Linux} & \textbf{Win.} & \textbf{Web} & \textbf{Mob.} & \textbf{(Benign)} \\
\midrule
ClamAV (Standard) & 33.0\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
ClamAV (Tuned)$^\dagger$ & 95.4\% & \textbf{96.6\%} & 82.6\% & 50.0\% & \textbf{0.0\%} \\
YARA (Standard) & 100.0\% & 1.9\% & 24.3\% & 0.0\% & 45.0\% \\
YARA (Tuned)$^\ddagger$ & 24.6\% & 2.6\% & 68.1\% & 79.2\% & 31.0\% \\
AIDE & \textbf{100.0\%} & \textbf{100.0\%} & \textbf{100.0\%} & \textbf{100.0\%} & 100.0\% \\
Set-AE & 40.0\% & 10.0\% & 6.9\% & 91.7\% & 5.0\% \\
\midrule
DeepVis (Entropy) & 25.0\% & 14.2\% & 5.6\% & 91.7\% & 10.2\% \\
\rowcolor{gray!10} 
\textbf{DeepVis (Full)} & \textbf{97.1\%} & \textbf{16.9\%} & \textbf{89.6\%} & \textbf{100.0\%} & \textbf{0.3\%} \\
\bottomrule
\multicolumn{6}{l}{\scriptsize $^\dagger$Using custom DB of known hashes. $^\ddagger$Using custom rules for LKM/Webshells.}
\end{tabular}%
}
\end{table}

\noindent\textbf{Platform-Specific Generalization. }Detection Recall increases substantially with the full architecture. On Linux, Recall jumps from 25.0\% (Entropy-only) to 97.1\% (DeepVis Full), and on Mobile from 91.7\% to 100.0\% (Table~\ref{tab:unified_detection}), indicating that multi-modal fusion is essential for isolating threats that mimic benign entropy distributions. In contrast, ClamAV (Standard) achieves only 0--33\% recall without custom signatures. While YARA (Tuned) improves detection, it incurs a prohibitive False Positive Rate (31.0\%). \DeepVis maintains a negligible False Positive Rate of 0.3\%, confirming that the learned normality model is far more robust to benign variations than static heuristics (10.2\% FP).

\noindent\textbf{Feature Orthogonality. }Micro-analysis confirms that multi-modal features catch evasion attempts. As shown in Table~\ref{tab:detailed_breakdown}, the rootkit \texttt{Diamorphine} evades the Entropy channel ($R=0.55$) but is detected by Structure ($B=1.00$) and Context ($G=0.81$). Similarly, \texttt{FormGrab.exe} triggers detection ($B=0.90$) due to structural anomalies in a Linux environment. However, limitations persist; the header-only approach misses artifacts like \texttt{setup.sh}, as they reside in valid paths without binary packing anomalies.

\begin{table}[t]
\centering
\caption{\textbf{Detailed Detection Analysis.} Multi-modal RGB features catch threats that single metrics miss. The table presents representative samples from our Benign (N=667) and Malware (N=68) datasets. The "Miss" cases highlight limitations against threats that mimic benign header statistics.}
\label{tab:detailed_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c ccc c}
\toprule
\textbf{Type} & \textbf{Name} & \textbf{R} & \textbf{G} & \textbf{B} & \textbf{Status} \\
\midrule
\multicolumn{6}{l}{\textit{Detected Active Threats (Multi-Platform)}} \\
LKM Rootkit & \texttt{Diamorphine} & 0.55 & \textbf{0.81} & \textbf{1.00} & Det. \\
Windows Spy & \texttt{FormGrab.exe} & 0.75 & 0.57 & \textbf{0.90} & Det. \\
Android Mal & \texttt{DEX Dropper} & \textbf{1.00} & 0.57 & \textbf{1.00} & Det. \\
Webshell & \texttt{TDshell.php} & 0.69 & 0.72 & \textbf{0.60} & Det. \\
Encrypted RK & \texttt{azazel\_enc} & \textbf{1.00} & \textbf{0.90} & \textbf{0.80} & Det. \\
\midrule
\multicolumn{6}{l}{\textit{Undetected (Limitations)}} \\
Obfuscated & \texttt{libc\_fake.so} & 0.61 & 0.00 & 0.00 & Miss \\
Mimicry Script & \texttt{setup.sh} & 0.58 & 0.00 & 0.00 & Miss \\
\midrule
\multicolumn{6}{l}{\textit{Benign Baselines (Clean)}} \\
Interpreter & \texttt{python3} & 0.78 & 0.96 & 0.10 & Clean \\
Library & \texttt{libc.so.6} & 0.79 & 0.90 & 0.10 & Clean \\
\multicolumn{6}{l}{\textit{False Positives (High Entropy)}} \\
Admin Tool & \texttt{snap} & \textbf{0.75} & \textbf{1.00} & 0.10 & False Pos. \\
Config Gen & \texttt{cloud-init} & 0.72 & 0.85 & 0.30 & False Pos. \\
\bottomrule
\end{tabular}
}
\end{table}

%=====================================================================
\subsection{Throughput and Interference (RQ2)}
\label{eval_scalability}
%=====================================================================

\begin{figure}[t]
    \centering
    \subfloat[Throughput]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_final_throughput.pdf}
        \label{fig:perf_throughput}
    } \hfill
    \subfloat[Interference]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_final_latency.pdf}
        \label{fig:perf_latency}
    }
    \caption{\textbf{Comprehensive Performance Analysis.} (a) \textbf{Throughput}: DeepVis achieves hyperscale speeds ($\approx$40k files/s) via asynchronous I/O, outperforming synchronous baselines. (b) \textbf{Interference}: Despite its speed, \DeepVis maintains negligible latency overhead (+2\%) compared to massive spikes caused by AIDE (+291\%) and YARA (+547\%).}
    \label{fig:perf_analysis}
\end{figure}

\begin{table}[t]
\centering
\caption{\textbf{Fair Baseline Comparison.} All tools read only the first 128 bytes (Header-Only). DeepVis's speedup stems from \texttt{io\_uring} (async I/O) and non-cryptographic hashing, not reduced data.}
\label{tab:fair_baseline}
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{l c c c}
\toprule
\textbf{Tool} & \textbf{Throughput} & \textbf{vs DeepVis} & \textbf{Architecture} \\
\midrule
\textbf{DeepVis} & 39,993/s & 1.0$\times$ & Async (\texttt{io\_uring}) \\
AIDE-Header & 7,316/s & 5.5$\times$ slower & Sync (SHA256) \\
ClamAV-Header & 4,892/s & 8.2$\times$ slower & Sync (Pattern Match) \\
ssdeep-Header & 3,417/s & 11.7$\times$ slower & Sync (Rolling Hash) \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Scan Throughput. }Throughput increases substantially with architectural optimization. DeepVis achieves 39,993 files/s, representing a 5.5$\times$ speedup over the optimized AIDE-Header (7,316/s) and an 8.2$\times$ advantage over ClamAV-Header (4,892/s) (Table~\ref{tab:fair_baseline}). This is because \texttt{io\_uring} eliminates the blocking overhead inherent in synchronous read operations. Against full-content scanners like ssdeep (127 files/s), \DeepVis realizes a 215$\times$ speedup, validating that the header-sampling paradigm is critical for hyperscale feasibility.

\noindent\textbf{Service Interference. }Latency overhead decreases significantly compared to traditional monitors. As shown in Figure~\ref{fig:perf_latency}, AIDE induces a +291\% latency spike (12.1ms) and YARA causes +547\% degradation due to intensive CPU matching. In contrast, DeepVis maintains a P99 latency of 3,162$\mu$s, reflecting a negligible +2.0\% overhead over the baseline (3,100$\mu$s). This indicates that the asynchronous, spatial hashing design allows the scanner to operate transparently without disrupting co-located workloads.

%=====================================================================
\subsection{Scalability and Saturation Analysis (RQ3, RQ6, RQ7)}
\label{eval_saturation}
%=====================================================================

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/comparison_dilution.pdf} 
    \caption{Visualizing Signal Preservation. (Top) \DeepVis maintains spatial locality, isolating the malware as a distinct red peak ($L_\infty$ Spike). (Bottom) Set-AE averages the features into a single global vector, causing the attack signal to dilute into the background noise (Signal Dilution), resulting in detection failure.}
    \label{fig:signal_comparison}
\end{figure}

\noindent\textbf{Signal Preservation. }Signal-to-Noise Ratio (SNR) improves from 1.09 (Set-AE) to 2.71 (\DeepVis) under active update scenarios. Figure~\ref{fig:signal_comparison} shows that Set-AE averages features globally, causing the attack signal to dilute into the background noise ($\mu_{noise}=0.35$). In contrast, \DeepVis maintains spatial locality, preserving the sharp attack spike ($0.95$). This is because the Hash-Grid effectively filters out diffuse noise, ensuring robust detection even during high churn.

\begin{table}[t]
\centering
\caption{Hash Saturation Analysis. Max-Risk Pooling preserves attack signals even with 600+ collisions/pixel. Recall is measured with 100 injected malware.}
\label{tab:hyperscale_saturation}
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{r c c c}
\toprule
Files ($N$) & Grid Saturation & Avg. Collisions & Recall \\
\midrule
100,000 & 99.74\% & 6.1 & 100\% \\
500,000 & 100.00\% & 30.5 & 100\% \\
1,000,000 & 100.00\% & 61.0 & 100\% \\
5,000,000 & 100.00\% & 305.2 & 100\% \\
\rowcolor{gray!10} 
10,000,000 & 100.00\% & 610.4 & 100\% \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Resilience to Saturation. }Recall remains at 100\% even as file count increases to 10 million (Table~\ref{tab:hyperscale_saturation}). Despite the grid saturation reaching 100\% with over 610 collisions per pixel, the Max-Risk Pooling strategy ensures the high-risk anomaly dominates the pixel value. This indicates that \DeepVis is inherently resistant to decoy attacks, as low-score benign files cannot suppress the signal of a malicious file.

\begin{figure}[t]
  \centering
  \subfloat[Linear Scalability]{
    \includegraphics[width=0.8\linewidth]{Figures/fig_fleet_scalability.pdf}
    \label{fig:fleet_scale}
  } \\
  \subfloat[Geo-Stability]{
    \includegraphics[width=0.8\linewidth]{Figures/fig_fleet_vis.pdf}
    \label{fig:fleet_heatmap}
  }
  \caption{\textbf{Fleet-Scale Performance.} (a) \textbf{Linear Scalability:} Throughput increases linearly with fleet size, reaching $\approx$206k files/s at 100 nodes. (b) \textbf{Geo-Stability:} The latency heatmap across 100 nodes shows consistent performance (avg 4.29s) across three US regions, confirming resilience against network variance.}
  \label{fig:fleet_perf}
\end{figure}

\noindent\textbf{Fleet Scalability. }Throughput scales linearly with fleet size, increasing from $\approx$2k files/s (1 Node) to 206,611 files/s (100 Nodes) as shown in Figure~\ref{fig:fleet_scale}. This confirms that the stateless architecture effectively decouples processing load. Cross-region latency remains stable (avg 4.29s) across 100 nodes (Figure~\ref{fig:fleet_heatmap}), with a minimal aggregation overhead of 548ms. Network overhead is also drastically reduced; each node transmits only 49KB, representing a 100$\times$ reduction compared to provenance-based systems.

%=====================================================================
\subsection{Robustness to System Churn (RQ3)}
\label{eval_churn}
%=====================================================================

\begin{figure}[t!]
    \centering
    \subfloat[Real Fleet Error Stability]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_churn_hist.pdf}
        \label{fig:churn_hist}
    } \hfill
    \subfloat[Alert Fatigue \& Detection]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_churn_alerts.pdf}
        \label{fig:churn_alerts}
    }
    \caption{\textbf{Fleet-Scale Churn \& Alert Fatigue Analysis.} (a) Benign files form a bimodal distribution: \textbf{Configs/Logs} (low entropy, $\approx 0.2$) and \textbf{Binaries/DBs} (high entropy, $\approx 0.6$). The \textbf{LKM Rootkit} attack ($\approx 1.0$) is a singular high-entropy anomaly. (b) While AIDE detects the attack, it generates 180 false alerts (Blue Bars) from benign upgrades. DeepVis achieves detection with zero false positives.}
    \label{fig:churn_analysis}
\end{figure}

\noindent\textbf{Diffuse Noise vs. Sparse Signal. }DeepVis effectively distinguishes between diverse benign workloads and malicious injections. In our real-world multi-profile experiment:
\begin{itemize}[leftmargin=*]
    \item \textbf{Scenario:} We executed 5 distinct operations modifying the file system: \texttt{apt reinstall} (Bastion), \texttt{nginx} config update (Web), \texttt{gcc} compilation (Build), 1MB binary data write (DB), and log rotation (App). This generated a combined churn of 180 file modifications, followed by 1 stealthy rootkit injection.
    \item \textbf{Result:} As shown in Figure~\ref{fig:churn_hist}, the cumulative error distribution from all workloads remained statistically indistinguishable from the baseline, visualized as the stable ``Benign State'' region. The attack (Red Star) is the only event exceeding the anomaly threshold ($L_\infty < \tau$).
    \item \textbf{Detection \& Fatigue (Fig.~\ref{fig:churn_alerts}):}
    \begin{itemize}
        \item \textbf{AIDE (Legacy FIM):} Successfully detected the rootkit but generated \textbf{180 False Alerts}, dominated by the Bastion system upgrade (174 alerts).
        \item \textbf{YARA (Tuned):} Generated \textbf{54 False Alerts} (Heuristic FP) while missing the stealthy rootkit.
        \item \textbf{DeepVis:} Achieved \textbf{Zero False Alerts} across all 5 workloads and correctly flagged the single stealthy threat.
    \end{itemize}
\end{itemize}

%=====================================================================
% Removed \clearpage to avoid whitespace
\subsection{Sensitivity and Ablation Analysis}
\label{eval_sensitivity}
%=====================================================================

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\columnwidth]{Figures/fig_page_aligned_sensitivity.pdf} 
    \caption{\textbf{Effect of Scan Granularity (Page-Aligned Sampling).} Comparison of Recall across platforms vs. Throughput. Detection accuracy for Linux ($Recall \approx 97\%$) and Mobile ($100\%$) saturates at $\approx$96B. Windows recall remains capped ($15\%$) due to feature orthogonality. Throughput remains stable up to 4KB due to page prefetching, justifying our \textbf{4KB Page-Aligned Sampling} strategy.}
    \label{fig:header_sensitivity}
\end{figure}

\noindent\textbf{Impact of Scan Granularity. }Detection accuracy saturates rapidly, with Linux and Mobile platforms reaching peak Recall ($\approx$97--100\%) at a mere 96B scan size (Fig.~\ref{fig:header_sensitivity}). This indicates that minimal header data suffices for robust feature extraction. Throughput shows a general decline as scan size increases but remains stable up to 4KB. This stability stems from OS-level \textit{page prefetching}, where reading small chunks (e.g., 32B) incurs similar I/O costs to reading a full 4KB page. Consequently, we adopt a \textbf{4KB Page-Aligned Sampling} strategy to maximize information retrieval without penalizing I/O performance.

\begin{table}[!ht]
\centering
\caption{\textbf{Component Time Breakdown (Cold Cache).} I/O dominates ($\approx$70--74\%), confirming that \DeepVis computational overhead (Hashing, Entropy, Tensor Update) remains negligible ($<$10\%) even during storage bottlenecks.}
\label{tab:overhead_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l rrrr}
\toprule
\textbf{Component} & \textbf{10K} & \textbf{100K} & \textbf{200K} & \textbf{500K} \\
\midrule
Traversal & 40ms (16.5\%) & 475ms (26.4\%) & 536ms (17.9\%) & 3.3s (13.3\%) \\
\textbf{I/O (Header Read)} & \textbf{172ms (70.8\%)} & \textbf{1.1s (61.1\%)} & \textbf{2.1s (70.0\%)} & \textbf{18.3s (73.8\%)} \\
Hashing & 10ms (4.1\%) & 66ms (3.7\%) & 124ms (4.1\%) & 1.1s (4.4\%) \\
Entropy Calc & 16ms (6.6\%) & 106ms (5.9\%) & 199ms (6.6\%) & 1.7s (6.9\%) \\
Tensor Update & 4ms (1.6\%) & 26ms (1.4\%) & 50ms (1.7\%) & 0.4s (1.6\%) \\
\midrule
\textbf{Total Time (Cold)} & \textbf{243ms} & \textbf{1.8s} & \textbf{3.0s} & \textbf{24.8s} \\
Throughput (files/s) & 41,228 & 55,689 & 66,118 & 20,180 \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Runtime Bottleneck Analysis. }I/O latency dominates the runtime, increasing from 172ms (10K files) to 18.3s (500K files) and accounting for $\approx$70.8\% to $\approx$73.8\% of the total execution time (Table~\ref{tab:overhead_breakdown}). In contrast, \textit{Tensor Update} time remains negligible, taking only 0.4s even for 500K files (1.6\%). This breakdown confirms that the computational overhead of \DeepVis is minimal compared to storage latency, identifying I/O throughput as the primary bottleneck for optimization.

\begin{table}[!ht]
\centering
\caption{Component-wise Ablation Study. Performance components (I/O, Sampling) determine throughput; Accuracy components (Hash-Grid, RGB) determine detection capability.}
\label{tab:ablation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l l | c c}
\toprule
Category & Configuration & Rate (files/s) & F1-Score \\
\midrule
\multirow{3}{*}{\textbf{Performance}} 
    & Baseline (Sync + Full Read) & 1,455 & -- \\
    & + Async I/O (\texttt{io\_uring}) & 2,910 & -- \\
    & + Header Sampling (128B) & 39,993 & -- \\
\midrule
\multirow{3}{*}{\textbf{Accuracy}} 
    & Entropy Only (R-channel) & -- & 0.25 \\
    & + Hash-Grid ($L_\infty$) & -- & 0.35 \\
    & + RGB Fusion (Full DeepVis) & -- & \textbf{0.96} \\
\midrule
\rowcolor{gray!15}
\multicolumn{2}{l|}{\textbf{DeepVis (All Components)}} & \textbf{39,993} & \textbf{0.96} \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Performance Optimization. }Throughput increases from 1,455 files/s (Baseline) to 2,910 files/s (Async I/O), indicating that eliminating kernel-user context switching overhead improves efficiency (Table~\ref{tab:ablation}). The most substantial gain stems from \textit{Header Sampling}, where limiting the read scope to 128B triggers a 27$\times$ surge in scan rate to 39,993 files/s. This confirms that minimizing physical I/O volume is the decisive factor for achieving hyperscale monitoring capabilities.

\noindent\textbf{Detection Fidelity. }F1-Score increases from 0.25 (Entropy Only) to 0.96 (RGB Fusion). Single-channel configurations prove insufficient; the R-channel yields false positives from benign compressed data, while structural features alone fail to distinguish packed binaries. Only the full \textit{RGB Fusion} achieves definitive detection, demonstrating that the orthogonality of Entropy, Context, and Structure is essential for distinguishing malicious artifacts from legitimate system noise.