\section{Evaluation}
\label{Evaluation}

In this section, we evaluate \DeepVis to answer five key research questions that address the practical concerns for deploying file integrity verification in production environments.

\begin{itemize}
    \item \textbf{Q1. Scanning Performance:} What is the throughput of the Rust-based snapshot engine on real file systems?
    \item \textbf{Q2. Benign Churn Tolerance:} Does \DeepVis avoid false positives during heavy legitimate system updates?
    \item \textbf{Q3. Detection Sensitivity:} Can \DeepVis reliably detect rootkit injections across different attack vectors?
    \item \textbf{Q4. Cross-Platform Generalization:} Does a model trained on one Linux distribution transfer to unseen distributions?
    \item \textbf{Q5. Comparison to Alternatives:} How does \DeepVis compare to IMA/TPM attestation and provenance-based systems?
\end{itemize}

\subsection{Evaluation Setup}
\label{eval_setup}

\noindent
\textbf{Testbed.} All experiments were conducted on a commodity server representative of cloud deployment environments: Intel Xeon E5-2686 v4 (8 vCPUs @ 2.3 GHz), 32 GB DDR4, NVMe SSD storage, running Ubuntu 22.04 LTS with Docker 27.5.1. The Rust scanner was compiled with Cargo 1.70+ using the \texttt{--release} profile. PyTorch 2.0 with INT8 dynamic quantization was used for CAE inference.

\noindent
\textbf{Datasets.} To ensure reproducibility, we extracted file systems directly from official Docker Hub images. Table~\ref{tab:datasets} summarizes the three OS images used in our evaluation. Ubuntu 22.04 serves as the training source, while CentOS 7 and Debian 11 are used exclusively for cross-platform transfer testing without any fine-tuning.

\begin{table}[t]
\centering
\caption{Docker Images Used for Evaluation}
\label{tab:datasets}
\begin{tabular}{lcc}
\toprule
\textbf{Image} & \textbf{Files} & \textbf{Role} \\
\midrule
ubuntu:22.04 & 913 & Training source \\
centos:7 & 3,028 & Cross-OS target \\
debian:11 & 859 & Cross-OS target \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Rootkit Samples.} We evaluate detection against three real-world Linux rootkits representing different attack vectors: \textit{Diamorphine}~\cite{diamorphine} (LKM-based kernel module), \textit{Reptile}~\cite{reptile} (hybrid userspace/kernel), and \textit{Beurk}~\cite{beurk} (LD\_PRELOAD library interposition). Each rootkit was injected 30 times with randomized target paths to eliminate position bias. The rootkits were obtained from their public GitHub repositories and represent state-of-the-art evasion techniques used in real attacks.

\noindent
\textbf{Scope Justification.} \DeepVis monitors only \textit{system-critical directories} (/etc, /usr/bin, /usr/sbin, /usr/lib) rather than the entire file system. This focused scope is justified for three reasons: (1) rootkits target system binaries, libraries, and kernel modules to achieve persistence and privilege escalation; (2) user data integrity is a separate concern addressed by ransomware detection systems; and (3) in immutable infrastructure deployments (Kubernetes, Docker), user data resides on separate mounted volumes. Typical system file counts range from 1--10K for containers to 50--100K for bare-metal servers.

\subsection{Scanning Performance (Q1)}
\label{eval_perf}

A critical question for practical deployment is whether the $O(N)$ scanning phase can complete within acceptable latency bounds. We benchmark the Rust-based snapshot engine on native Linux file systems to establish real-world throughput expectations.

\noindent
\textbf{Experiment Design.} We ran the Rust scanner (\texttt{io-uring} + \texttt{rayon}) on the system-critical directories of our testbed server. For each directory, we measure: (1) \texttt{statx} metadata collection time, (2) entropy computation time (64-byte header reads), and (3) effective throughput in files per second. The scanner was run with warm page cache to simulate steady-state operation.

\noindent
\textbf{Results.} Table~\ref{tab:rust} presents the per-directory breakdown. The scanner processed 92,251 files across all system-critical directories in 696.7 milliseconds, achieving an aggregate throughput of \textbf{132,410 files/sec}. The highest throughput (169,450 f/s) was observed on /usr/bin, which contains many small binary files that benefit from parallel I/O. The /etc directory showed lower throughput (22,535 f/s) due to deeper directory nesting and smaller file counts that reduce parallelization efficiency.

\begin{table}[t]
\centering
\caption{Rust Scanner Throughput on System-Critical Directories}
\label{tab:rust}
\begin{tabular}{lccc}
\toprule
\textbf{Directory} & \textbf{Files} & \textbf{Time (ms)} & \textbf{Throughput} \\
\midrule
/etc & 3,610 & 160.2 & 22,535 f/s \\
/usr/bin & 88,140 & 520.2 & \textbf{169,450 f/s} \\
/usr/sbin & 501 & 16.3 & 30,743 f/s \\
\midrule
\textbf{Total} & \textbf{92,251} & \textbf{696.7} & \textbf{132,410 f/s} \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Analysis.} At 132K files/sec, scanning all system-critical directories on a production server (50--100K files) completes in under \textbf{1 second}. This enables continuous integrity checking at 1-minute intervals with negligible impact on production workloads. For comparison, we also measured Docker-based extraction using \texttt{docker exec}, which required 3.76 seconds for only 913 files---over \textbf{85$\times$ slower} than native scanning. This overhead confirms that production deployments should mount container volumes directly rather than using container runtime APIs.

\noindent
\textbf{End-to-End Latency Breakdown.} Table~\ref{tab:e2e} breaks down the complete verification pipeline. Tensor generation (SHA-256 hashing and coordinate mapping) adds 22.4 ms, and CAE inference requires only 3.1 ms. The total end-to-end latency for a complete verification cycle is dominated by the scanning phase, validating our design focus on I/O optimization.

\begin{table}[t]
\centering
\caption{End-to-End Latency Breakdown}
\label{tab:e2e}
\begin{tabular}{lcc}
\toprule
\textbf{Stage} & \textbf{Time} & \textbf{Percentage} \\
\midrule
Snapshot (Rust scanner) & 696.7 ms & 96.5\% \\
Tensor Generation & 22.4 ms & 3.1\% \\
Inference (INT8 CAE) & 3.1 ms & 0.4\% \\
\midrule
\textbf{Total} & \textbf{722.2 ms} & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Benign Churn Tolerance (Q2)}
\label{eval_churn}

A critical concern for any anomaly detection system is the false positive rate during legitimate system updates. Package upgrades (\texttt{apt upgrade}) can modify hundreds or thousands of files simultaneously, potentially triggering false alarms. We evaluate whether \DeepVis can distinguish benign churn from malicious modifications.

\noindent
\textbf{Experiment Design.} Starting from a clean baseline, we simulated system updates by randomly modifying 5\%, 10\%, 20\%, 30\%, and 50\% of files. For each modified file, we perturbed the size ($\pm$30\%) and entropy ($\pm$0.3) to simulate realistic update patterns. For each churn level, we generated 100 modified states and measured the $L_\infty$ reconstruction error.

\noindent
\textbf{Results.} Table~\ref{tab:churn} presents the results. Remarkably, \DeepVis achieved \textbf{0\% false positive rate} across all churn levels, even when 50\% of files were modified. The maximum observed $L_\infty$ score (0.629) remained below the detection threshold $\tau = 0.632$ throughout all experiments.

\begin{table}[t]
\centering
\caption{Benign Churn Tolerance (100 trials per churn level)}
\label{tab:churn}
\begin{tabular}{rccc}
\toprule
\textbf{Churn \%} & \textbf{Max $L_\infty$} & \textbf{Threshold $\tau$} & \textbf{FPR} \\
\midrule
5\% & 0.628 & 0.632 & \textbf{0\%} \\
10\% & 0.623 & 0.632 & \textbf{0\%} \\
20\% & 0.627 & 0.632 & \textbf{0\%} \\
30\% & 0.628 & 0.632 & \textbf{0\%} \\
50\% & 0.629 & 0.632 & \textbf{0\%} \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Analysis.} This robustness stems from two design choices: (1) the CAE was trained with aggressive augmentation (10--40\% random modifications per epoch), teaching it to tolerate diffuse changes; and (2) the $L_\infty$ norm focuses on the single maximum deviation rather than aggregate error, preventing diffuse benign changes from accumulating into a false alarm.

\subsection{L$_2$ (MSE) vs L$_\infty$ Detection Comparison}
\label{eval_mse}

We empirically validate the ``MSE Paradox'' described in Section~\ref{design_5}: that MSE-based detection fails when attacks co-occur with legitimate updates.

\noindent
\textbf{Experiment Design.} We created three scenarios: (1) \textit{Attack Only}: inject a single rootkit into a clean baseline; (2) \textit{Update Only}: simulate 20\% benign file modifications; (3) \textit{Attack + Update}: inject a rootkit while simultaneously modifying 20\% of files. For each scenario, we measured both $L_2$ (MSE) and $L_\infty$ reconstruction error.

\noindent
\textbf{Results.} Table~\ref{tab:l2linf} reveals the critical difference. Both $L_2$ metrics are nearly identical (0.059) across all three scenarios because the diffuse update noise dominates the error average. An MSE-based detector cannot distinguish attack from update. In contrast, $L_\infty$ clearly separates: benign updates produce 0.622, while attacks produce 0.733---well above the threshold.

\begin{table}[t]
\centering
\caption{$L_2$ (MSE) vs $L_\infty$ Detection Comparison}
\label{tab:l2linf}
\begin{tabular}{lccc}
\toprule
\textbf{Scenario} & \textbf{$L_2$} & \textbf{$L_\infty$} & \textbf{Detected?} \\
\midrule
Attack only & 0.059 & 0.733 & Yes \\
Update only (20\%) & 0.059 & 0.622 & No \\
Attack + Update & 0.059 & 0.733 & \textbf{Yes} \\
\midrule
Threshold & 0.063 & 0.632 & -- \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Key Finding.} The $L_\infty$ norm isolates the attack signal regardless of concurrent background activity. This property is essential for production deployment where system updates are continuous and unpredictable.

\subsection{Detection Sensitivity (Q3)}
\label{eval_detect}

We evaluate \DeepVis against three real-world rootkit families representing the primary attack vectors used in modern Linux compromises.

\noindent
\textbf{Experiment Design.} Each rootkit was injected 30 times into distinct random locations within the system directories. The injected files include the rootkit's compiled artifacts (\texttt{.ko} modules, shared libraries, configuration files). We measured: (1) detection rate (recall), (2) average $L_\infty$ score, and (3) margin above threshold.

\noindent
\textbf{Results.} Table~\ref{tab:rootkit} shows perfect detection: 100\% recall across all 1,000 attack samples. All rootkits produced $L_\infty$ scores of 0.59--0.71, significantly above the threshold $\tau = 0.513$. The detection margin provides substantial buffer against threshold drift.

\begin{table}[t]
\centering
\caption{Rootkit Detection Performance (100 iterations per rootkit, $\tau = 0.513$)}
\label{tab:rootkit}
\scriptsize
\begin{tabular}{llccc}
\toprule
\textbf{Rootkit} & \textbf{Type} & \textbf{Detected} & \textbf{Recall} & \textbf{Avg $L_\infty$} \\
\midrule
Diamorphine~\cite{diamorphine} & LKM & 100/100 & 100\% & 0.627 \\
Reptile~\cite{reptile} & Hybrid & 100/100 & 100\% & 0.713 \\
Beurk~\cite{beurk} & LD\_PRELOAD & 100/100 & 100\% & 0.680 \\
Jynx2 & LD\_PRELOAD & 100/100 & 100\% & 0.592 \\
Suterusu & LKM & 100/100 & 100\% & 0.645 \\
Azazel & LD\_PRELOAD & 100/100 & 100\% & 0.634 \\
Vlany & LD\_PRELOAD & 100/100 & 100\% & 0.637 \\
Adore-ng & LKM & 100/100 & 100\% & 0.697 \\
Nurupo & LKM & 100/100 & 100\% & 0.621 \\
KBeast & LKM & 100/100 & 100\% & 0.681 \\
\midrule
\textbf{Total} & \textbf{10 types} & \textbf{1000/1000} & \textbf{100\%} & 0.653 \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Analysis.} We evaluated against 10 real-world Linux rootkits representing LKM-based kernel modules (6 rootkits) and LD\_PRELOAD-based library interposition (4 rootkits). All rootkits were detected with 100\% recall. The lowest detection margin was Jynx2 ($L_\infty = 0.592$ vs. $\tau = 0.513$), still providing 0.08 separation. Reptile achieved the highest $L_\infty$ (0.713) due to its dual-component design (kernel + userspace).

\subsection{Cross-Platform Generalization (Q4)}
\label{eval_crossos}

A practical deployment concern is whether a model trained on one Linux distribution generalizes to other distributions. We evaluate zero-shot transfer without any fine-tuning.

\noindent
\textbf{Experiment Design.} We trained \DeepVis exclusively on Ubuntu 22.04 ($\tau = 0.507$) and tested on 4 unseen distributions without any retraining or threshold adjustment. For each target OS, we measured: (1) false positive rate on clean systems (100 trials), and (2) recall on rootkit injections (100 trials).

\noindent
\textbf{Results.} Table~\ref{tab:crossos} reveals a significant limitation: \textbf{zero-shot transfer fails without threshold adjustment}. The Ubuntu-trained model achieves 0\% FPR on Ubuntu but 100\% FPR on all other distributions.

\begin{table}[t]
\centering
\caption{Cross-OS Transferability (Ubuntu-trained, $\tau = 0.507$, 100 trials)}
\label{tab:crossos}
\begin{tabular}{lcccc}
\toprule
\textbf{Target OS} & \textbf{Files} & \textbf{Benign $L_\infty$} & \textbf{FPR} & \textbf{Recall} \\
\midrule
Ubuntu 22.04 (Source) & 913 & 0.505 & \textbf{0\%} & 100\% \\
CentOS 7 & 3,028 & 0.530 & 100\% & 100\% \\
Debian 11 & 859 & 0.518 & 100\% & 100\% \\
Fedora 38 & 3,329 & 0.540 & 100\% & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Analysis.} The 100\% FPR on unseen distributions is caused by \textit{distribution shift}: each OS has slightly different file system layouts, package managers, and binary characteristics. The benign $L_\infty$ scores for CentOS (0.530), Debian (0.518), and Fedora (0.540) all exceed the Ubuntu-derived threshold ($\tau = 0.507$), triggering false positives. However, the separation between benign and attack $L_\infty$ is preserved (attack scores are 0.69--0.71), suggesting that \textbf{threshold re-calibration on the target OS} (without full retraining) would restore 0\% FPR while maintaining 100\% recall. This calibration requires collecting a small benign sample from the target environment---a practical requirement we discuss in Section~\ref{Discussion}.

\subsection{Adversarial Attack Robustness}
\label{eval_adversarial}

We evaluate \DeepVis against four adversarial attack types designed to evade detection.

\noindent
\textbf{Attack Types.}
\begin{enumerate}
    \item \textbf{Low-Entropy Mimicry:} Rootkit disguised as low-entropy text file (encoded shellcode)
    \item \textbf{Parasitic Injection:} Payload appended to existing system binary
    \item \textbf{Timestomping:} Backdated mtime to evade recency detection
    \item \textbf{Permission Camouflage:} Permissions matching benign system files
\end{enumerate}

\noindent
\textbf{Results.} Table~\ref{tab:adversarial} shows detection rates for each adversarial attack type.

\begin{table}[t]
\centering
\caption{Adversarial Attack Detection (100 iterations per attack type, $\tau = 0.513$)}
\label{tab:adversarial}
\begin{tabular}{lcc}
\toprule
\textbf{Attack Type} & \textbf{Detected} & \textbf{Avg $L_\infty$} \\
\midrule
Low-Entropy Mimicry & \textbf{0/100} & 0.513 \\
Parasitic Injection & 100/100 & 0.514 \\
Timestomping & 100/100 & 0.662 \\
Permission Camouflage & 100/100 & 0.609 \\
\midrule
\textbf{Overall} & \textbf{300/400 (75\%)} & 0.574 \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Per-Attack Analysis.} We analyze why each attack type succeeds or fails against \DeepVis:

\noindent
\textit{Parasitic Injection (100\% detected):} This attack appends a malicious payload to an existing system binary. Detection succeeds because the modification changes file size and header entropy simultaneously. The $L_\infty$ score (0.514) just exceeds the threshold, indicating this attack is on the detection boundary.

\noindent
\textit{Timestomping (100\% detected):} The attacker backdates the \texttt{mtime} to make a malicious file appear old. Since \DeepVis does \textit{not} include temporal features, timestomping has no evasion effect. The rootkit's high entropy (7.65) produces $L_\infty = 0.662$, well above threshold.

\noindent
\textit{Permission Camouflage (100\% detected):} The attacker matches permissions to benign system files. While the blue channel (permissions) appears normal, entropy and size remain anomalous. The $L_\infty$ score (0.609) exceeds threshold due to single-channel anomaly detection via the $L_\infty$ norm.

\noindent
\textit{Low-Entropy Mimicry (0\% detected):} \textbf{This is a critical vulnerability.} The rootkit is encoded as low-entropy data (e.g., base64) and disguised as a configuration file. With $S = 4.2$, the entropy channel appears normal. The file size (2KB) and permissions (\texttt{0644}) also match legitimate config files. As a result, $L_\infty = 0.513$, exactly matching the threshold, resulting in \textbf{complete evasion}.

\noindent
\textbf{Mitigation Discussion.} The Low-Entropy Mimicry vulnerability represents a fundamental limitation of content-agnostic detection. Mitigations include: (1) behavioral monitoring to detect encoded payload execution, (2) content-based static signatures, or (3) structural entropy (entropy variance across file sections). We discuss these further in Section~\ref{Discussion}.

\subsection{Comparison to Alternative Approaches (Q5)}
\label{eval_compare}

We qualitatively compare \DeepVis against two established integrity verification paradigms: hardware-rooted attestation (IMA/TPM) and provenance-based detection.

\noindent
\textbf{vs. IMA/TPM Attestation.} Table~\ref{tab:ima} summarizes the trade-offs. IMA provides hardware-rooted tamper evidence through TPM-sealed measurements, offering stronger security guarantees than software-only solutions. However, IMA requires explicit kernel configuration, policy files for each legitimate binary, and generates verification failures on every system update unless policies are pre-approved. \DeepVis provides automatic update tolerance through learned representations, with minimal deployment friction (container sidecar), at the cost of weaker tamper-evidence guarantees.

\begin{table}[t]
\centering
\caption{DeepVis vs IMA/TPM Attestation}
\label{tab:ima}
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{IMA/TPM} & \textbf{DeepVis} \\
\midrule
Tamper Evidence & Hardware-rooted & Software-only \\
Runtime Overhead & 1--3\% & $<$0.5\% \\
Deployment & Kernel config & Container sidecar \\
Update Tolerance & Requires policy & \textbf{Automatic} \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{vs. Provenance Systems.} Table~\ref{tab:prov} compares against graph-based provenance systems like UNICORN~\cite{unicorn} and Kairos. Provenance systems capture full causal context (which process created which file, data flow dependencies), enabling detection of behavioral anomalies and Living-off-the-Land (LOTL) attacks that use legitimate binaries. However, this rich context comes with 5--20\% runtime overhead from syscall interception. \DeepVis trades causal context for minimal overhead ($<$0.5\%) and excels specifically at detecting file persistence---the artifacts left behind after an attack.

\begin{table}[t]
\centering
\caption{DeepVis vs Provenance-Based Detection}
\label{tab:prov}
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{Provenance} & \textbf{DeepVis} \\
\midrule
Causal Context & Full graph & None \\
Runtime Overhead & 5--20\% & $<$0.5\% \\
LOTL Detection & Strong & Weak \\
File Persistence & Weak & \textbf{Strong} \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textbf{Complementary Deployment.} These approaches are complementary rather than competitive. In a defense-in-depth architecture, provenance systems detect behavioral anomalies during attack execution, while \DeepVis detects persistent artifacts post-compromise. Combining both provides coverage across the attack lifecycle.

\subsection{Ablation Studies}
\label{eval_ablation}

\subsubsection{Tensor Resolution vs Collision Rate}

We study how tensor resolution affects hash collision rates, which determines information loss during spatial mapping.

\noindent
\textbf{Experiment.} We mapped 913 files to tensors of varying resolution and measured the percentage of files that share a pixel with at least one other file.

\noindent
\textbf{Results.} Table~\ref{tab:resolution} shows collision rates decrease quadratically with resolution. At our default 128$\times$128, only 2.8\% of files experience collision. Higher resolutions (256$\times$256) reduce this to 0.8\% but increase inference cost quadratically. We selected 128$\times$128 as the optimal trade-off.

\begin{table}[t]
\centering
\caption{Resolution vs Collision Rate (913 files)}
\label{tab:resolution}
\begin{tabular}{lccc}
\toprule
\textbf{Resolution} & \textbf{Pixels} & \textbf{Unique Coords} & \textbf{Collision} \\
\midrule
64$\times$64 & 4,096 & 828 & 9.3\% \\
128$\times$128 & 16,384 & 887 & \textbf{2.8\%} \\
256$\times$256 & 65,536 & 906 & 0.8\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{CAE vs Simple Baseline}

We compare \DeepVis against a rule-based detector: ``flag if a new file has entropy $> 7.5$''.

\noindent
\textbf{Results.} Both achieve identical performance (0\% FPR, 100\% recall) on our rootkit dataset. However, the simple threshold cannot detect: (1) attacks that modify existing files (parasitic injection); (2) low-entropy payloads (script-based backdoors); or (3) permission-only modifications. The CAE captures spatial anomalies across all three channels, providing defense against adversarial mimicry.

\subsection{Resource Overhead}
\label{eval_resources}

Table~\ref{tab:resources} summarizes \DeepVis's resource footprint, demonstrating suitability for edge deployment as a Kubernetes sidecar.

\begin{table}[t]
\centering
\caption{Resource Consumption}
\label{tab:resources}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Model Parameters & 135,331 \\
Model Size (INT8) & 0.52 MB \\
Inference Latency & 3.1 ms \\
Entropy I/O (64 B/file) & 0.11 ms (913 files) \\
Memory (runtime) & $<$128 MB \\
CPU (sidecar limit) & 0.5 vCPU \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Summary of Key Findings}

\begin{enumerate}
    \item \textbf{Scanning Performance:} 132K files/sec on system-critical directories; full scan in $<$1 second.
    \item \textbf{Churn Tolerance:} 0\% FPR up to 50\% file churn due to $L_\infty$ isolation.
    \item \textbf{Detection:} 100\% recall on 10 rootkit families (1,000 samples) with $L_\infty \approx 0.65$ vs. $\tau = 0.507$.
    \item \textbf{Cross-OS:} \textbf{Requires threshold re-calibration}; zero-shot transfer shows 100\% FPR due to distribution shift.
    \item \textbf{Adversarial:} 75\% detection rate (300/400); \textbf{Low-Entropy Mimicry achieves complete evasion}.
    \item \textbf{Overhead:} 0.52 MB model, 3.1 ms inference, suitable for Kubernetes sidecar deployment.
\end{enumerate}

