\section{Evaluation}
\label{sec:evaluation}

We evaluate \DeepVis on a production Google Cloud Platform (GCP) infrastructure using real compiled rootkits and realistic attack scenarios. Our evaluation answers whether the multi-modal RGB encoding distinguishes high-entropy packed malware (RQ1), scales to millions of files (RQ2), tolerates legitimate system churn (RQ3), compares favorably against runtime monitors and legacy scanners (RQ4), and resists hash collisions at hyperscale (RQ5).

%=====================================================================
\subsection{Experimental Methodology}
\label{eval_setup}
%=====================================================================

\noindent\textbf{Testbed Environment.} 
We conduct experiments on three distinct GCP configurations to represent a spectrum of cloud instances: \textbf{Low} (e2-micro, 2 vCPU, 1GB RAM, HDD), \textbf{Mid} (e2-standard-2, 2 vCPU, 8GB RAM, SSD), and \textbf{High} (c2-standard-4, 4 vCPU, 16GB RAM, NVMe SSD). The primary evaluation uses the High tier to demonstrate performance on modern NVMe storage. To simulate a production environment, we populated the file system with a diverse set of benign artifacts, including system binaries (e.g., \texttt{nginx}, \texttt{gcc}), configuration files, and Python scripts, scaling up to 50 million files for stress testing.



\noindent\textbf{Threshold Learning.} 
We employed a maximum-margin approach to determine detection boundaries. The thresholds were learned from the benign baseline as $\tau_c = \max(\text{Benign}_c) + 0.1$, ensuring a 0\% False Positive Rate during calibration. This resulted in $\tau_R=0.75$, $\tau_G=0.25$, and $\tau_B=0.30$.

%=====================================================================
\subsection{Detection Accuracy and Feature Orthogonality (RQ1)}
\label{eval_accuracy}
%=====================================================================

\noindent\textbf{Rigorous Binary Evaluation.} 
To overcome the noise inherent in gross repository statistics, we curated a precise Binary-Only Dataset consisting of 68 Active Malware Binaries (including unpacked rootkits and attack tools) and 667 Legitimate System Binaries sampled from \texttt{/usr/bin}. As summarized in Table~\ref{tab:unified_detection}, the evaluation reveals the fundamental limitation of single-metric heuristics. Detection based solely on Entropy failed to identify the majority of threats, achieving a recall of only 25.0\%. This failure occurs because many modern attack tools (e.g., \texttt{VirTool.DDoS}) are not packed, resulting in low entropy scores indistinguishable from benign software. Furthermore, the entropy-based approach suffered a 10.2\% False Positive rate, incorrectly flagging standard administrative tools like \texttt{uwsgi} and \texttt{snap} that employ internal compression. In contrast, \DeepVis leverages Multi-modal features by integrating Context (G) and Structure (B) channels. This fusion recovered the threats missed by entropy, achieving 96.0\% recall on the same malware set while suppressing false positives to 0.1\%. This improvement demonstrates that the Hash-Grid architecture effectively captures the intersection of anomalous features that single metrics miss.

\noindent\textbf{Global Selectivity.}
On the global repository containing 37,571 files, \DeepVis maintained a surgical Alert Rate of 0.6\%. This low percentage indicates high precision rather than low recall; DeepVis effectively filtered out the 99.4\% of dormant source code and text files that do not pose an immediate runtime integrity threat. In contrast, signature-based YARA flagged 2.7\% of the repository by matching text strings such as "hack" or "rootkit" within non-executable source files, generating significant noise. Traditional FIM (AIDE) flagged 100\% of the files as changed, rendering it unusable for pinpointing specific threats in a dynamic environment.

%---------------------------------------------------------------------
% TABLE III: Unified Performance (Macro View)
%---------------------------------------------------------------------
\begin{table}[t]
\centering
\caption{\textbf{Unified Detection Performance.} Evaluated on a rigorous \textbf{Binary-Only Dataset} (68 Malware, 667 Benign) and the \textbf{Global Repository} (37,571 Files). \DeepVis achieves 96\% recall on active binary threats. YARA and ClamAV are included as \textit{reference points} (full-content scanners) rather than direct baselines, to contextualize the noise and coverage trade-offs of header-only sampling.}
\label{tab:unified_detection}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l cc c l}
\toprule
\multirow{2}{*}{\textbf{System}} & \textbf{Malware Recall} & \textbf{Benign FP} & \textbf{Repo Alerts} & \multicolumn{1}{c}{\textbf{Primary Failure Mode}} \\
& \textbf{(N=68)} & \textbf{(N=667)} & \textbf{(N=37k)} & \multicolumn{1}{c}{\textit{(Source of Miss/FP)}} \\
\midrule
ClamAV & 33.0\% & 0.0\% & 0.0\% & Misses Unknown Malware \\
YARA & 100.0\% & 45.0\% & 2.7\% & Text Matches (FP) \\
AIDE & 100.0\% & 100.0\% & 100.0\% & System Updates (FP) \\
Set-AE & 40.0\% & 5.0\% & 5.0\% & Global Pooling (Miss) \\
\midrule
DeepVis (Entropy) & 25.0\% & 10.2\% & 10.2\% & Unpacked Binaries (Miss) \\
\rowcolor{gray!10} 
\textbf{DeepVis (Full)} & \textbf{96.0\%} & \textbf{0.1\%} & \textbf{0.6\%} & Admin Tools (FP) \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Failure Mode Analysis.} 
Table~\ref{tab:detailed_breakdown} provides a granular analysis of detection capabilities and limitations. DeepVis detects evasive threats through feature orthogonality. For instance, the rootkit Diamorphine evaded the Entropy channel ($R=0.52$) but was detected by the Context ($G=0.60$) and Structure ($B=0.50$) channels due to its nature as a kernel module residing in a temporary directory. Similarly, Azazel was identified via high Entropy ($R=1.00$) and Context anomalies ($G=0.90$). However, the header-only approach exhibits intrinsic blind spots against non-binary threats. As shown in the failure cases of Table~\ref{tab:detailed_breakdown}, \DeepVis failed to detect the public webshell \texttt{c99.php} and the DDoS tool \texttt{VirTool.TCP.a}. These files reside in structurally valid paths and lack binary packing anomalies, making them indistinguishable from benign scripts via headers alone. This limitation confirms that \DeepVis operates as a high-speed first-line defense for binary integrity rather than a full-content forensic scanner.

%---------------------------------------------------------------------
% TABLE IV: Detailed Breakdown (Micro View)
%---------------------------------------------------------------------
\begin{table}[t]
\centering
\caption{\textbf{Detailed Detection Analysis.} Multi-modal RGB features catch threats that single metrics miss. The "Miss" cases highlight the limitation against threats that perfectly mimic benign header statistics.}
\label{tab:detailed_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l c ccc c}
\toprule
\textbf{Type} & \textbf{Name} & \textbf{R} & \textbf{G} & \textbf{B} & \textbf{Status} \\
\midrule
\multicolumn{6}{l}{\textit{Detected Active Threats}} \\
LKM Rootkit & \texttt{Diamorphine} & 0.52 & \textbf{0.60} & \textbf{0.50} & Det. \\
LD\_PRELOAD & \texttt{Azazel} & 0.37 & \textbf{0.60} & 0.00 & Det. \\
Crypto Miner & \texttt{XMRig} & 0.32 & \textbf{0.60} & 0.00 & Det. \\
Encrypted RK & \texttt{azazel\_enc} & \textbf{1.00} & \textbf{0.90} & \textbf{0.80} & Det. \\
Rev. Shell & \texttt{rev\_shell} & \textbf{1.00} & \textbf{0.70} & 0.00 & Det. \\
Disguised ELF & \texttt{access.log} & 0.55 & 0.00 & \textbf{1.00} & Det. \\
\midrule
\multicolumn{6}{l}{\textit{Undetected (Limitations)}} \\
Webshell & \texttt{c99.php} & 0.58 & 0.00 & 0.00 & Miss \\
Mimicry ELF & \texttt{libc\_fake.so} & 0.61 & 0.00 & 0.00 & Miss \\
DDoS Tool & \texttt{VirTool.TCP.a} & 0.58 & 0.00 & 0.00 & Miss \\
\midrule
\multicolumn{6}{l}{\textit{Benign Baselines (Clean)}} \\
Interpreter & \texttt{python3} & 0.67 & 0.00 & 0.00 & Clean \\
Library & \texttt{libc.so.6} & 0.66 & 0.00 & 0.00 & Clean \\
Image (PNG) & \texttt{ubuntu-logo} & 0.53 & 0.00 & 0.00 & Clean \\
\multicolumn{6}{l}{\textit{False Positives (High Entropy Tools)}} \\
Admin Tool & \texttt{uwsgi} & \textbf{0.76} & 0.00 & 0.00 & False Pos. \\
\bottomrule
\end{tabular}
}
\end{table}

\noindent\textbf{Comparison with Set-based Approaches.} 
To evaluate the architectural advantage of the Hash-Grid Parallel CAE, we implemented a Set-based Autoencoder (Set-AE) baseline following the Deep Sets framework~\cite{zaheer2017deepsets}. As shown in Table~\ref{tab:unified_detection}, Set-AE fails to isolate sparse threats, achieving only 40\% recall on rootkits. This poor performance stems from the global feature pooling mechanism, which dilutes the signal of a single malicious file ($N=1$) against the variance of thousands of benign system files. In contrast, \DeepVis projects files onto a fixed Spatial Grid and employs $L_\infty$ pooling, ensuring that sparse anomalies remain locally distinct spikes rather than being averaged out globally.

%=====================================================================
\subsection{Scalability and Performance Analysis (RQ2)}
\label{eval_scalability}
%=====================================================================

The primary architectural claim of \DeepVis is the decoupling of verification latency from file system size. We validate this through two distinct lenses: processing throughput (micro-benchmark) and service interference (macro-benchmark).

\begin{figure}[t]
    \centering
    \subfloat[Throughput]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_final_throughput.pdf}
        \label{fig:perf_throughput}
    } \hfill
    \subfloat[Interference]{
        \includegraphics[width=0.45\linewidth]{Figures/fig_final_latency.pdf}
        \label{fig:perf_latency}
    }
    \caption{\textbf{Comprehensive Performance Analysis.} (a) \textbf{Throughput}: DeepVis achieves hyperscale speeds ($\approx$40k files/s) via asynchronous I/O, outperforming synchronous baselines. (b) \textbf{Interference}: Despite its speed, \DeepVis maintains negligible latency overhead (+2\%) compared to massive spikes caused by AIDE (+291\%) and YARA (+547\%).}
    \label{fig:perf_analysis}
\end{figure}

\subsubsection{Micro-benchmark}
\noindent\textbf{Scan Throughput. }Figure~\ref{fig:perf_analysis}(a) compares DeepVis against AIDE to demonstrate operational feasibility. AIDE performs full-file cryptographic hashing, providing strong integrity guarantees but incurring $O(N \times Size)$ I/O complexity. This heavy I/O load often forces operators to restrict scanning to weekly maintenance windows. On a GCP High tier (c2-standard-4), \DeepVis achieves a 7.7$\times$ speedup over standard AIDE. Even against an optimized Partial-Hash AIDE baseline that reads only the first 128 bytes, \DeepVis maintains a 5.4$\times$ throughput advantage. This gain confirms that the performance boost stems not just from reading less data, but from the parallel \texttt{io\_uring} pipeline, which effectively hides I/O latency through massive concurrent queuing.

\noindent\textbf{Comparison with Commercial Scanners. }Benchmarking against fuzzy hashing (ssdeep) and signature scanners (ClamAV, YARA) on the full \texttt{/usr} directory (240,827 files) reveals that traditional tools are bottlenecked by synchronous content reads (127--1,004 files/s). In contrast, \DeepVis achieves 39,993 files/s, representing a 40$\times$ to 215$\times$ speedup over the baselines. This throughput demonstrates the efficiency of the asynchronous snapshot engine in hyperscale environments.

\subsubsection{Macro-benchmark}
\noindent\textbf{Service Interference. }Figure~\ref{fig:perf_analysis}(b) illustrates the P99 latency of a co-located NGINX web server during a full system scan. While raw throughput is critical, interference defines the operational constraint. Traditional tools severely impact system responsiveness; YARA and Heuristic engines cause degradation of +546\% and +324\% respectively due to CPU-intensive pattern matching. AIDE induces a +291\% latency spike (12.1ms) due to blocking I/O operations. In contrast, \DeepVis maintains a P99 latency of 3,162$\mu$s, reflecting a negligible +2.0\% overhead compared to the baseline (3,100$\mu$s). This confirms that the spatial hashing and asynchronous design allow the system to operate transparently in the background.

\noindent\textbf{CPU Resource Profile. }Resource contention analysis explains the latency results. Legacy FIMs and scanners such as Osquery and AIDE saturate the Global CPU at near 100\%, forcing the OS scheduler to throttle the web server. DeepVis, however, maintains a CPU profile of 11.2\%, nearly identical to the baseline (9.8\%). Unlike runtime monitors (e.g., Falco) which incur constant context-switching overhead (+58.3\% latency degradation), \DeepVis utilizes lightweight SIMD optimizations to ensure security monitoring remains strictly orthogonal to the primary service performance.


%=====================================================================
\subsection{Impact of Spatial Dimension and Hash Saturation (RQ3, RQ6)}
\label{eval_saturation}
%=====================================================================

We evaluate the structural limits of the fixed-size tensor representation, focusing on signal preservation against dimensional reduction and robustness against hash collisions.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/comparison_dilution.pdf} 
    \caption{Visualizing Signal Preservation. (Top) \DeepVis maintains spatial locality, isolating the malware as a distinct red peak ($L_\infty$ Spike). (Bottom) Set-AE averages the features into a single global vector, causing the attack signal to dilute into the background noise (Signal Dilution), resulting in detection failure.}
    \label{fig:signal_comparison}
\end{figure}

\noindent\textbf{Impact of Spatial Dimension. }Figure~\ref{fig:signal_comparison} compares the internal representations under an active attack scenario. The top panel demonstrates that the 2D Hash-Grid architecture maintains the spatial isolation of anomalies, manifesting injected malware as sharp, localized peaks against diffuse background noise. In contrast, the bottom panel shows that reducing the dimension to a single global vector (Set-AE) aggregates sparse attack signals with thousands of benign signals, washing out the anomaly. Quantitatively, measurements during a live system update confirm this observation. The global pooling approach fails to distinguish the attack from update noise, resulting in a negligible Signal-to-Noise Ratio (SNR) of 1.09. Conversely, the spatial isolation of \DeepVis yields a superior SNR of 2.71, ensuring robust detection even during high churn.

\noindent\textbf{Resilience to Hash Saturation. }To validate the stability of the hash mapping as the file count ($N$) exceeds the grid capacity ($W \times H$), we stress-tested the system by injecting up to 204,000 files into the $128 \times 128$ grid. Table~\ref{tab:hyperscale_saturation} shows that even at 99.99\% saturation (high collision state), the system maintains stability. Unlike traditional hash tables where collisions degrade performance to $O(N)$, our Max-Risk Pooling strategy ($\text{Grid}[h] = \max(\text{Grid}[h], s)$) ensures that tensor construction remains strictly $O(1)$. Collisions do not increase computational overhead; they merely aggregate risk scores, ensuring that detection latency remains constant regardless of file density.

\begin{table}[t]
\centering
\caption{Hash Saturation Analysis. High collision rates do not impact processing overhead due to $O(1)$ Max-Risk Pooling.}
\label{tab:hyperscale_saturation}
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{r c c}
\toprule
Files ($N$) & Grid Saturation & Avg. Collisions \\
\midrule
10,000 & 45.47\% & 0.61 \\
50,000 & 95.21\% & 3.05 \\
100,000 & 99.87\% & 6.10 \\
\rowcolor{gray!10} 
204,000 & 99.99\% & 12.45 \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Component Overhead. }Component analysis at scale (500K files) confirms that the hashing and mapping process is computationally efficient. The \texttt{io\_uring} based file reading consumes 90.1\% of the total scan time, while hashing, tensor mapping, and CAE inference account for negligible overhead ($<3\%$). This validates that the Hash-Grid architecture effectively decouples detection complexity from file system size without introducing computational bottlenecks.



%=====================================================================
\subsection{Fleet-Scale Scalability (RQ7)}
\label{eval_fleet}
%=====================================================================

A key requirement for distributed systems is demonstrating scalability across a fleet of nodes under realistic network conditions. We evaluate the capability of \DeepVis to verify a large distributed cluster by deploying 100 concurrent nodes in a public cloud environment.

\noindent\textbf{Experimental Setup and Orchestration at Scale. }Deploying 100 concurrent nodes presents significant orchestration challenges, including API rate limits and network saturation. To mitigate these, we distributed the fleet across three geographically distant GCP regions: \texttt{us-central1} (Iowa), \texttt{us-east1} (South Carolina), and \texttt{us-west1} (Oregon). A hierarchical orchestration architecture was employed where a single bastion node (\texttt{deepvis-mid}) located in \texttt{asia-northeast3} (Seoul) coordinated the entire US-based fleet via GCP's internal VPC network. This cross-region control plane demonstrates effective management of global deployments without co-location. Each e2-micro node, provisioned with a custom Golden Image containing the Rust-based \DeepVis scanner, performed a full scan of local directories upon activation. The resulting $128 \times 128 \times 3$ RGB tensors were transmitted to the aggregator via the asynchronous protocol.

\begin{figure}[t]
  \centering
  \subfloat[Linear Scalability]{
    \includegraphics[width=0.8\linewidth]{Figures/fig_fleet_scalability.pdf}
    \label{fig:fleet_scale}
  } \\
  \subfloat[Geo-Stability]{
    \includegraphics[width=0.8\linewidth]{Figures/fig_fleet_vis.pdf}
    \label{fig:fleet_heatmap}
  }
  \caption{\textbf{Fleet-Scale Performance.} (a) \textbf{Linear Scalability:} Throughput increases linearly with fleet size, reaching $\approx$206k files/s at 100 nodes. (b) \textbf{Geo-Stability:} The latency heatmap across 100 nodes shows consistent performance (avg 4.29s) across three US regions, confirming resilience against network variance.}
  \label{fig:fleet_perf}
\end{figure}

\noindent\textbf{Linear Scalability. }Figure~\ref{fig:fleet_perf}(a) demonstrates the linear scaling capability of \DeepVis. As the fleet size scales from 1 to 100 nodes, the aggregate verification throughput grows proportionally, reaching \textbf{206,611 files/sec}. This confirms that the stateless nature of the Hash-Grid architecture effectively decouples the processing load, eliminating central bottlenecks.

\noindent\textbf{Global Stability. }Figure~\ref{fig:fleet_perf}(b) visualizes the scan latency distribution across the 100 nodes. Despite the geographical dispersion, \DeepVis maintains a tight latency bound with an average of 4.29s and a maximum of 6.0s. Crucially, the aggregation overhead for the entire fleet was merely 548ms. This validates that \DeepVis ensures consistent performance SLAs even in "noisy neighbor" public cloud environments.

\noindent\textbf{Network Efficiency. }Tensor-based verification drastically reduces bandwidth consumption. Each node transmits a fixed-size 49KB tensor regardless of file count, totaling only 4.9MB for the 100-node fleet. This represents a 100$\times$ reduction in network overhead compared to provenance-based systems, which can exceed 500MB under similar workloads.

\begin{table}[h]
\centering
\caption{\textbf{Component Time Breakdown (Cold Cache).} I/O dominates ($\approx$90\%), confirming that \\DeepVis computational overhead (Hashing, Entropy, Tensor Update) is negligible.}
\label{tab:overhead_breakdown}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l rrrr}
\toprule
\textbf{Component} & \textbf{10K} & \textbf{100K} & \textbf{200K} & \textbf{500K} \\
\midrule
Traversal & 127ms (14\%) & 1.74s (11\%) & 3.93s (9.5\%) & 8.88s (7.0\%) \\
\textbf{I/O (Header Read)} & \textbf{675ms (77\%)} & \textbf{12.77s (84\%)} & \textbf{36.09s (87\%)} & \textbf{113.2s (89.9\%)} \\
Hashing & 24ms (2.8\%) & 209ms (1.4\%) & 418ms (1.0\%) & 1.31s (1.0\%) \\
Entropy Calc & 30ms (3.4\%) & 322ms (2.1\%) & 557ms (1.3\%) & 1.86s (1.5\%) \\
Tensor Update & 21ms (2.4\%) & 132ms (0.9\%) & 321ms (0.8\%) & 746ms (0.6\%) \\
\midrule
\textbf{Total Time} & \textbf{877ms} & \textbf{15.17s} & \textbf{41.31s} & \textbf{126.0s} \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Computational Overhead Analysis. }Table~\ref{tab:overhead_breakdown} presents the execution time breakdown across varying file counts ranging from 10K to 500K under a cold cache scenario. I/O operations dominate the runtime, consuming approximately 90\% of the total execution time at scale (500K files). Conversely, computational tasks, including hashing, entropy calculation, and tensor updates, collectively account for less than 3.1\% of the total time. This indicates that the computational overhead of the hash-grid mapping and feature extraction remains negligible, confirming that the performance of \DeepVis is limited effectively by storage bandwidth rather than CPU resources.



%=====================================================================
\subsection{Ablation Study: Component Contribution Analysis}
\label{eval_ablation}
%=====================================================================

To quantify the contribution of each architectural decision, we evaluated system performance by selectively disabling key components. Table~\ref{tab:ablation} separates \textbf{Performance-critical} components (affecting throughput) from \textbf{Accuracy-critical} components (affecting detection).

\begin{table}[t]
\centering
\caption{Component-wise Ablation Study. Performance components (I/O, Sampling) determine throughput; Accuracy components (Hash-Grid, RGB) determine detection capability.}
\label{tab:ablation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l l | c c}
\toprule
Category & Configuration & Rate (files/s) & F1-Score \\
\midrule
\multirow{3}{*}{\textbf{Performance}} 
    & Baseline (Sync + Full Read) & 1,455 & -- \\
    & + Async I/O (\texttt{io\_uring}) & 2,910 & -- \\
    & + Header Sampling (128B) & 39,993 & -- \\
\midrule
\multirow{3}{*}{\textbf{Accuracy}} 
    & Entropy Only (R-channel) & -- & 0.25 \\
    & + Hash-Grid ($L_\infty$) & -- & 0.35 \\
    & + RGB Fusion (Full DeepVis) & -- & \textbf{0.98} \\
\midrule
\rowcolor{gray!15}
\multicolumn{2}{l|}{\textbf{DeepVis (All Components)}} & \textbf{39,993} & \textbf{0.98} \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent\textbf{Performance Optimization. }Replacing synchronous operations with asynchronous I/O doubles the throughput from 1,455 files/s to 2,910 files/s. This indicates that eliminating kernel-user context switching overhead improves efficiency even when reading entire files. The most significant gain stems from header sampling. Limiting the read scope to the first 128 bytes increases the scan rate to 39,993 files/s, representing a 27$\times$ speedup. This confirms that minimizing I/O volume is the primary factor for achieving hyperscale monitoring capabilities.

\noindent\textbf{Detection Fidelity. }Relying solely on entropy results in a poor F1-score of 0.25 due to false positives from benign compressed files. Applying the hash-grid architecture with local max pooling raises the score to 0.35, as it preserves sparse signals that are otherwise diluted by global pooling. However, the definitive improvement is achieved through multi-modal fusion. Integrating context and structure channels with entropy enables combinatorial reasoning, increasing the F1-score to 0.98. This demonstrates that architectural isolation must be paired with orthogonal feature sets to distinguish malicious artifacts from legitimate system noise.