\section{Introduction}
\label{Introduction}

In the era of cloud computing, ensuring the integrity of workloads is a foundational security requirement. From container orchestration platforms like Kubernetes to large-scale HPC clusters, operators must guarantee that the file systems of thousands of nodes remain free from unauthorized modifications. However, modern DevOps practices create a fundamental tension between security and agility. Traditional File Integrity Monitoring (FIM) tools, designed for static servers, generate thousands of false positive alerts on every deployment, overwhelming Security Operations Centers (SOCs) with ``alert fatigue.'' Meanwhile, advanced persistent threats (APTs) exploit this noise to install stealthy user-space rootkits that evade detection.

Consider a routine scenario where an administrator deploys a new container image or updates a package on a fleet of Ubuntu servers. This operation modifies thousands of filesâ€”libraries, binaries, and configurations. For traditional FIM tools like AIDE~\cite{aide} or Tripwire~\cite{tripwire}, each modification is a potential violation. SOCs face an impossible choice: investigate thousands of impossible-to-verify alerts daily or disable FIM during maintenance windows, creating blind spots.

To address this, recent research has pivoted towards log-based anomaly detection~\cite{du2017deeplog} or provenance graph analysis~\cite{cheng2024kairos}. While effective for tracking runtime behavior, these approaches face fundamental scalability limitations in hyperscale environments. Provenance systems impose a 5--20\% runtime overhead due to heavy kernel instrumentation (e.g., \texttt{auditd} or eBPF), making them prohibitive for latency-sensitive workloads. Furthermore, they track *events* rather than *state*, meaning they cannot detect a rootkit that was dropped before monitoring started.

We propose a paradigm shift: **File System Fingerprinting**. Instead of tracing every system call, we aim to verify the integrity of the entire file system state in constant time ($O(1)$), regardless of the number of files. However, applying deep learning to file systems poses a unique challenge. Unlike images (fixed grids) or time series (ordered sequences), a file system is an unordered set of variable-length paths. A naive attempt to vectorize this data (e.g., sorting files) suffers from the \textit{Ordering Problem}, where a single file addition shifts the entire representation, destroying spatial locality.

\DeepVis distinguishes itself by implementing the first **Hash-Based Spatial Representation** for file systems. By mapping unordered files to a fixed-size 2D tensor via deterministic hashing, \DeepVis ensures shift invariance: adding a file only affects a specific local region of the tensor, not the global structure. This enables the use of Convolutional Neural Networks (CNNs) to "see" the file system as an image. Furthermore, we address the \textit{Attacker's Paradox}, where legitimate updates create diffuse noise (high global error) while stealthy attacks create sparse signals (low global error). We utilize Local Max Detection ($L_\infty$) to pinpoint these sparse anomalies.

In this paper, we present \DeepVis, a highly scalable integrity verification framework designed for hyperscale distributed systems. \DeepVis adopts a spatial snapshot approach and integrates three key techniques to achieve scalability and precision. The goal of \DeepVis is to 1) decouple inference complexity from the file count, 2) resolve the statistical asymmetry between diffuse updates and sparse attacks, and 3) eliminate runtime overhead on the host kernel. To achieve these goals, \DeepVis 1) transforms file metadata into a fixed-size tensor using hash-based partitioning, 2) utilizes a Convolutional Autoencoder with Local Max detection to identify spatial anomalies, and 3) operates on storage snapshots to ensure zero impact on running workloads. Our evaluation on production infrastructure across Ubuntu, CentOS, and Debian demonstrates that \DeepVis achieves an F1-score of 0.96 with zero false positives and enables 168$\times$ more frequent monitoring than traditional FIM. 