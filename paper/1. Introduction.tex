\section{Introduction}
Cloud computing provides a computational model distinct from traditional on-premise environments by abstracting physical infrastructure into dynamic, ephemeral resources. From container orchestration platforms such as Kubernetes to large-scale HPC clusters, ensuring the integrity of workloads is a foundational requirement. Operators must guarantee that the file systems of thousands of nodes remain free from unauthorized modifications. However, modern DevOps practices create a fundamental tension between security and agility. Frequent deployments and updates generate massive file churn, rendering traditional security models obsolete.

To address this, two primary strategies are commonly used: File Integrity Monitoring (FIM) and Runtime Behavioral Analysis. FIM tools such as AIDE~\cite{aide} and Tripwire~\cite{tripwire} rely on cryptographic hashing to detect static changes, providing strong integrity guarantees. Conversely, runtime monitors such as Falco~\cite{falco} and OSSEC~\cite{ossec} trace system calls to detect anomalous execution. Our work focuses on static integrity verification, as preserving the baseline state is essential for detecting dormant threats and performing post-incident forensics.

However, traditional integrity verification faces a fundamental scalability challenge. As the number of files ($N$) grows, the scan latency increases linearly ($O(N)$), causing severe I/O bottlenecks in hyperscale storage. This is problematic because modern cloud instances, despite high CPU throughput, have limited storage bandwidth. For example, scanning a filesystem with millions of small files using synchronous system calls results in excessive context switching and blocking I/O. Beyond the performance cost, the "Alert Fatigue" problem further limits usability: legitimate updates generate thousands of false positives, masking true threats~\cite{arp2022dos}. Thus, the operational cost exceeds the theoretical benefit, forcing operators to disable monitoring during maintenance windows.

\begin{figure}[t]
\centering
% Shared Legend at Top
\includegraphics[width=0.95\columnwidth]{Figures/fig_motivation_legend.pdf}
\vspace{-1mm}
% Subfigures side by side (2x1)
\subfloat[Scalability]{
    \includegraphics[width=0.47\columnwidth]{Figures/fig_motivation_a.pdf}
    \label{fig:motivation_scale}
}
\hfill
\subfloat[Alert Fatigue]{
    \includegraphics[width=0.47\columnwidth]{Figures/fig_motivation_b.pdf}
    \label{fig:motivation_alert}
}
\caption{(a) Synchronous scanning exhibits $O(N)$ latency; \DeepVis achieves near-constant time. (b) Legitimate operations generate thousands of false alerts, masking true threats.}
\label{fig:motivation}
\end{figure}

Figure~\ref{fig:motivation} compares the scalability and precision of \DeepVis against AIDE, a widely deployed FIM tool, on a GCP production instance. As depicted in Figure~\ref{fig:motivation}(a), AIDE scan time increases linearly, reaching 15 seconds for 1M files, whereas \DeepVis maintains near-constant latency (under 2 seconds) due to its parallelized asynchronous pipeline. Figure~\ref{fig:motivation}(b) highlights the detection capability: during routine package updates, AIDE generates over 2,000 false positives that obscure a single rootkit injection. In contrast, \DeepVis correctly identifies the rootkit while producing zero false alerts. These results highlight a key limitation: synchronous hashing and rule-based matching cannot support hyperscale verification. To overcome this, the system must utilize a \textit{File System Fingerprinting} approach, where the entire state is transformed into a fixed-size representation to decouple verification complexity from the file count.

\begin{table}[t]
\caption{Comparison with prior work across four key capabilities: Asynchronous I/O (Async), Obfuscation Resilience (Obfusc.), Zero-Day Detection (0-Day), and Low Overhead (Low Ovhd.).}
\centering
\scriptsize
\begin{tabular}{p{2.0cm}|>{\raggedright\arraybackslash}p{2.2cm}|p{0.6cm}|p{0.8cm}|p{0.6cm}|p{0.6cm}}
\toprule
\textbf{Study} & \textbf{Target Approach} & \textbf{Async} & \textbf{Obfusc.} & \textbf{0-Day} & \textbf{Low Ovhd.} \\
\midrule
AIDE~\cite{aide} & Full-Hash FIM &  & \checkmark &  &  \\
Tripwire~\cite{tripwire} & Full-Hash FIM &  & \checkmark &  &  \\
ClamAV~\cite{clamav} & Signature Scanning &  &  &  & \checkmark \\
Falco~\cite{falco} & Runtime/eBPF & \checkmark &  & \checkmark &  \\
Unicorn~\cite{unicorn} & Provenance Graph &  & \checkmark & \checkmark &  \\
OSSEC~\cite{ossec} & Log Analysis &  &  &  & \checkmark \\
Set-AE~\cite{zaheer2017deepsets} & Deep Sets Learning & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
\textbf{\DeepVis} & \textbf{Hash-Grid Tensor} & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\label{tab:intro_comparison}
\end{table}

Many previous studies, as summarized in Table~\ref{tab:intro_comparison}, have explored works to enhance the scalability of system monitoring. Several works~\cite{aide, tripwire} focus on cryptographic exactness but suffer from $O(N)$ scalability limits. Runtime approaches~\cite{falco, unicorn} utilize eBPF or provenance graphs to detect zero-day threats but incur continuous runtime overhead (5--20\%) and cannot detect dormant artifacts. Deep learning-based approaches, such as Set-AE~\cite{zaheer2017deepsets}, attempt to learn system states but fail to detect sparse anomalies due to signal dilution in global pooling.

\DeepVis distinguishes itself from prior works by departing from both sequential scanning and global pooling. Most previous studies rely on unordered set processing or linear file walking, which constrains performance to file count or dilutes attack signals. In contrast, \DeepVis adopts a \textbf{Hash-Based Spatial Representation} that maps unordered files to a fixed-size 2D tensor. By ensuring shift invariance via deterministic hashing, \DeepVis enables the use of Convolutional Neural Networks (CNNs) to process the file system as an image. Furthermore, it addresses the \textit{MSE Paradox}—where diffuse update noise masks sparse attack signals—by utilizing Local Max ($L_\infty$) detection. This allows \DeepVis to isolate specific anomalies without being affected by the global noise floor.

In this paper, we propose \DeepVis, a highly scalable integrity verification framework designed for hyperscale distributed systems. Specifically, \DeepVis (1) transforms file metadata into a fixed-size tensor using hash-based partitioning to achieve $O(1)$ inference latency, (2) utilizes a Hash-Grid Parallel CAE with Local Max detection to pinpoint sparse anomalies amidst system churn, and (3) employs an asynchronous \texttt{io\_uring} snapshot engine to maximize I/O throughput. Our evaluation on production infrastructure demonstrates that \DeepVis achieves 100\% recall on active threats with a 0.6\% repository alert rate and enables 168$\times$ more frequent monitoring than traditional FIM.