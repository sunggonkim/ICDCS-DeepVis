\section{Background}
\label{Background}

In this section, we formalize the core challenges in distributed file system monitoring that motivate \DeepVis. These challenges---the \textit{Ordering Problem} and the \textit{Diffuse-vs-Sparse Anomaly Paradox}---are fundamental to any system monitoring unordered, high-churn data sources.

\subsection{Distributed File System Monitoring}

File system consistency verification is critical for distributed systems. From cloud storage services such as AWS EBS and Azure Files to container orchestration platforms such as Kubernetes and Docker to HPC clusters like Lustre and GPFS, operators must detect unauthorized modifications without impacting system performance. Approaches are generally categorized into two types: integrity scanning and provenance analysis.

\noindent
\textbf{Traditional Integrity Scanning (FIM). }
Tools such as AIDE~\cite{aide} and Tripwire~\cite{tripwire} maintain a database of file attributes including hashes, permissions, and sizes. They operate by periodically scanning the file system and reporting deviations from a static baseline. Their design goal is exhaustive monitoring which involves detecting any change from the recorded state. While effective for static servers, this approach suffers from $O(N)$ complexity bottlenecks. As the file count grows, the scan duration increases linearly. Furthermore, in modern DevOps environments where continuous deployment is standard, a routine update modifies thousands of files. This generates a massive volume of alerts which leads to Alert Fatigue. Operators are often forced to disable monitoring during maintenance windows which creates blind spots.

\noindent
\textbf{Provenance-Based Analysis. }
Provenance systems~\cite{unicorn, cheng2024kairos, flash2024} build causal graphs from system calls to detect behavioral anomalies. By tracking information flow between processes and files, they achieve high precision and can distinguish between benign and malicious activities based on context. However, these systems require heavy kernel instrumentation using frameworks such as \texttt{auditd} or CamFlow. This imposes a runtime overhead of 5--20\% which is prohibitive for latency-sensitive workloads. Additionally, the graph generation and storage costs grow with the system activity level rather than the file system size.

\DeepVis is designed to address the limitations of both paradigms. It eliminates the runtime overhead of provenance systems by operating on storage snapshots and resolves the scalability bottleneck of FIM by decoupling inference complexity from the file count.

\subsection{The Ordering Problem in Spatial Representation}

To apply deep learning for anomaly detection, the file system state must be represented as a structured input tensor. However, file systems pose unique challenges compared to image or time-series data. Unlike images which have a fixed spatial grid or time series which have an inherent temporal sequence, file systems are unordered sets of variable-length paths.

% TODO: Add shift problem illustration figure
% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=8.5cm]{figures/shift_problem.png}
%   \caption{The Ordering Problem in Distributed File Systems.}
%   \label{fig:shift_problem}
% \end{figure}

The fundamental \textit{Ordering Problem} occurs when vectorizing file systems. A naive approach is to sort files by path and map them to a linear vector or 2D grid. Consider a sorted list of files $[A, B, C]$. If a single new file $A.1$ is installed, the sorted list becomes $[A, A.1, B, C]$. Consequently, the data for files $B$ and $C$ shifts to new positions in the vector.

This phenomenon is the \textit{Shift Problem}. For a Convolutional Neural Network (CNN) trained on spatial locality, this shift is catastrophic. The network learns that a specific coordinate $(x, y)$ corresponds to the features of file $B$. When a new file is inserted, that coordinate now contains the features of $A.1$ or a neighbor. This destroys the learned spatial patterns and causes the model to flag the entire file system as anomalous. This fragility makes sorted representations unsuitable for dynamic environments where files are frequently added or removed.

To solve this, \DeepVis introduces \textit{Hash-Based Spatial Mapping}. Instead of relying on sorting, we map each file to a fixed coordinate derived deterministically from its path hash. We formalize this mapping $\Phi$ as:
\begin{equation}
    \Phi(path) = (\text{Hash}(path) \pmod W, \lfloor \text{Hash}(path) / W \rfloor \pmod H)
\end{equation}
This ensures Shift Invariance. The coordinate of a file depends only on its own path. Adding a new file populates a specific pixel but does not perturb the positions of existing files. This transforms the unordered set into a stable spatial tensor suitable for CNN inference.

\subsection{The MSE Paradox: Diffuse vs. Sparse Signals}

A fundamental asymmetry exists in distributed system updates compared to attacks. This asymmetry causes traditional reconstruction-based anomaly detection to fail. We term this the \textit{MSE Paradox}.

Legitimate system updates, such as \texttt{apt-get upgrade}, affect a large number of files (diffuse noise). Thousands of binaries and libraries change simultaneously, but the entropy change per file is small. In contrast, stealthy rootkits typically modify a very small number of files (sparse signal) to maintain persistence, but the entropy change for those specific files is large due to packing or encryption.

Standard autoencoders use Mean Squared Error (MSE) as a loss function which averages the error across all pixels.
\begin{itemize}
    \item \textbf{Legitimate Update:} High aggregate error due to thousands of small changes.
    \item \textbf{Stealthy Attack:} Low aggregate error due to a single localized change.
\end{itemize}
If a global threshold is set to detect the attack, it generates false positives for every update. If the threshold is raised to tolerate updates, the attack is missed.

To overcome this, \DeepVis employs \textit{Local Max Detection} ($L_\infty$). Instead of averaging errors, we monitor the maximum pixel-wise reconstruction error. A stealthy rootkit produces a sharp spike in the error map at its specific coordinate. By focusing on the local maximum, \DeepVis can identify sparse anomalies even in the presence of diffuse background noise from legitimate updates.

\subsection{Entropy as an Attack Signature}

A key observation motivating \DeepVis is that packed or encrypted malware exhibits statistically different entropy patterns compared to legitimate system files. Figure~\ref{fig:entropy_combined} presents the entropy analysis: (a) the distribution measured from 913 real files extracted from an Ubuntu 22.04 Docker image, and (b-d) the byte-level patterns explaining why different file types exhibit different entropy values.

\begin{figure*}[!htb]
    \centering
    \subfloat[Entropy Distribution: Benign vs Rootkit]{
        \includegraphics[width=0.48\textwidth]{figures/entropy_real_analysis.pdf}
        \label{fig:entropy_hist}
    }
    \hfill
    \subfloat[Normal Text File (H $\approx$ 4.2)]{
        \includegraphics[width=0.16\textwidth]{figures/Background_entrophy/Background_Normal_text.pdf}
        \label{fig:entropy_text}
    }
    \subfloat[System Binary / ELF (H $\approx$ 6.1)]{
        \includegraphics[width=0.16\textwidth]{figures/Background_entrophy/Background_System_binaray.pdf}
        \label{fig:entropy_elf}
    }
    \subfloat[Packed Rootkit (H $\approx$ 7.9)]{
        \includegraphics[width=0.16\textwidth]{figures/Background_entrophy/Background_Rootkit.pdf}
        \label{fig:entropy_rootkit}
    }
    \caption{Entropy as an attack signature. (a) Distribution of Shannon entropy from real Ubuntu files ($n=913$) vs. simulated rootkits ($n=100$): benign files cluster below 6.5 bits/byte, while packed/encrypted rootkits exceed 7.0. (b-d) Byte value distributions explaining entropy differences: text files use limited ASCII range, ELF binaries show structured patterns, and encrypted rootkits exhibit uniform distribution---the ``Attacker's Paradox.''}
    \label{fig:entropy_combined}
\end{figure*}

\noindent
\textbf{Key Observation.} Legitimate system files---including text configurations, libraries, and executables---rarely exceed 6.5 bits/byte entropy. In contrast, attackers commonly employ packing (e.g., UPX) or encryption (e.g., AES) to evade signature-based detection and hinder reverse engineering. These transformations produce near-uniform byte distributions with entropy approaching the theoretical maximum of 8 bits/byte. This creates a statistical anomaly that \DeepVis exploits: high-entropy files in system directories are inherently suspicious.