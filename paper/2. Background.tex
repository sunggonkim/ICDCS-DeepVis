

\section{Background}
\label{sec:background}






\subsection{Integrity Verification at Cloud Scale}

Modern cloud infrastructure demands file integrity monitoring that balances scalability, detection coverage, and operational overhead~\cite{aide,tripwire,samhain,falco,unicorn}. Contemporary solutions partition into file-level scanning and runtime behavioral analysis, each exhibiting distinct limitations.

\noindent\textbf{File-level Integrity Scanning.} AIDE~\cite{aide} and Tripwire~\cite{tripwire} establish integrity through cryptographic hashing of entire files against known baselines. While effective in static environments, their $O(N \times \text{Size})$ complexity becomes prohibitive in dynamic hyperscale systems. Full scans routinely exceed maintenance windows, necessitating temporary monitoring suspension. Routine system updates further generate massive false positive alerts that overwhelm Security Operations Centers.

\noindent\textbf{Runtime Behavioral Analysis.} Falco~\cite{falco} and provenance graph systems~\cite{unicorn} intercept kernel events to detect anomalous execution patterns. These approaches incur substantial continuous overhead (5-20\% CPU) from pervasive system instrumentation. A critical limitation emerges from their event-based architecture: they cannot detect threats predating monitor deployment, creating a cold-start vulnerability for persistent rootkits.

File-level scanning remains indispensable for compliance validation, image verification, and forensic analysis due to comprehensive state coverage. However, synchronous sequential processing induces I/O bottlenecks and operational overload at scale. \DeepVis resolves these constraints through asynchronous I/O, spatial hash mapping, and neural anomaly detection, enabling production-grade filesystem integrity.


\begin{figure*}[!t]
    \centering
    \subfloat[Combined]{
        \includegraphics[width=0.45\columnwidth]{Figures/Background_entrophy/entropy_combined_a.pdf}
        \label{fig:entropy_hist}
    }%
    \hfill
    \subfloat[Text]{
        \includegraphics[width=0.45\columnwidth]{Figures/Background_entrophy/Background_Normal_text.pdf}
        \label{fig:entropy_text}
    }
    \hfill
    \subfloat[ELF Binary]{
        \includegraphics[width=0.45\columnwidth]{Figures/Background_entrophy/Background_System_binaray.pdf}
        \label{fig:entropy_elf}
    }%
    \hfill
    \subfloat[Packed Rootkit]{
        \includegraphics[width=0.45\columnwidth]{Figures/Background_entrophy/Background_Rootkit.pdf}
        \label{fig:entropy_rootkit}
    }
    \caption{File fingerprint analysis via byte-value histograms. (a) Combined entropy distribution across file types. (b) Text files use only printable ASCII, resulting in low entropy ($H \approx 4.8$) and zero null bytes. (c) ELF binaries show structured headers with significant zero-padding (40--85\% null bytes) for section alignment, yielding $H \approx 6.0$. (d) Packed rootkits eliminate all structure and null bytes ($<$1\%), maximizing entropy near the theoretical limit ($H \approx 8.0$).}
    \label{fig:entropy_combined}
\end{figure*}


\subsection{The Attacker Paradox: Entropy and Structure}

Detecting evasive malware without relying on signatures requires analyzing the statistical properties of binary files. Malware authors face a fundamental trade-off between concealing code and maintaining the structural validity required by operating system loaders. Two statistical dimensions distinguish malicious from benign files: Entropy and Structural Density.

Figure~\ref{fig:entropy_combined} illustrates these distinctions through byte-value histograms. Text files (Figure~\ref{fig:entropy_combined}b) concentrate in the printable ASCII range, yielding low entropy ($H \approx 4.8$) and zero null bytes due to high redundancy. Legitimate ELF (Executable and Linkable Format) binaries (Figure~\ref{fig:entropy_combined}c) display characteristic 0x00 peaks resulting from operating system requirements for 4KB page alignment. Compilers insert null-byte padding to align sections such as \texttt{.text} (code) and \texttt{.data} (variables) to page boundaries, producing moderate entropy ($H \approx 6.0$) with 40--85\% null byte concentration. In contrast, packed or encrypted malware (Figure~\ref{fig:entropy_combined}d) exhibits a nearly uniform distribution across all byte values, approaching maximum entropy ($H \approx 8.0$) with less than 1\% null bytes.

\noindent\textbf{The Attacker Paradox.} This statistical distinction creates a fundamental dilemma for malware authors. Native rootkits such as Diamorphine maintain structural compatibility with OS loaders by mimicking the layout of legitimate binaries, yet they remain vulnerable to signature-based detection tools such as YARA because their code contains known byte sequences. To evade signatures, attackers employ packing tools such as UPX (Ultimate Packer for eXecutables), which compress executables by 50--70\% and prepend decryption stubs. While packing successfully conceals signatures, it inevitably eliminates the section alignment padding and produces uniform byte distributions, obliterating the structural fingerprint of legitimate files and pushing entropy toward the theoretical maximum of 8.0 bits per byte. Consequently, attackers must choose between two undesirable outcomes: exposing their code to signature detection or creating a detectable statistical anomaly.

\noindent\textbf{Why Existing Methods Fail.} As discussed in Section~\ref{sec:background}, signature-based file integrity monitoring tools such as AIDE succeed against native rootkits but miss packed variants entirely. Conversely, entropy-based heuristics detect compression artifacts yet generate false positives on benign high-entropy files such as compressed archives and encrypted configurations. Neither approach captures the full threat landscape without sacrificing precision. The fundamental limitation stems from how these tools process filesystem data. Traditional sequential scanning ignores spatial relationships among files and exhibits linear scaling ($O(N)$) with file count, making them unsuitable for cloud-scale systems as shown in Figure~\ref{fig:motivation}. Moreover, set-based anomaly detection methods attempt to aggregate statistical features across entire filesystems, causing individual malicious signals to become subsumed within the variance of benign files. This signal dilution problem makes detection impossible when routine system updates create diffuse noise that exceeds any sparse attack signal.

\noindent\textbf{A Multi-Modal Approach.} Overcoming the Attacker Paradox requires simultaneously addressing both signature evasion and structural anomalies. Entropy identifies compression-based evasion artifacts, structural analysis exposes binary format violations, and contextual signals such as file path and permissions distinguish legitimate outliers from malicious anomalies. This orthogonal feature space enables threat detection regardless of whether attackers pursue signature evasion or structural stealth. However, realizing this multi-modal approach at cloud scale requires a fundamentally different architecture. Rather than sequential file scanning or aggregate feature pooling, \DeepVis projects the entire filesystem into a fixed-size tensor representation where multi-modal anomalies manifest as localized spatial spikes. This transformation enables rapid anomaly detection through convolutional processing, achieving constant-time inference independent of dataset size while maintaining the detection coverage of all three modalities.

