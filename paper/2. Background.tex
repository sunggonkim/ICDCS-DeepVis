\section{Background}
\label{sec:background}

\subsection{Integrity Verification at Cloud Scale}
%%\vspace{-.1cm}

Several approaches have been designed to monitor file system integrity on modern cloud infrastructure, each offering different trade-offs between scalability, detection coverage, and operational overhead~\cite{aide,tripwire,samhain,falco,unicorn}. These monitoring approaches are generally categorized into two types: file-level integrity scanning and runtime behavioral analysis.

\noindent
\textbf{File-level integrity scanning. }File-level integrity scanning tools such as AIDE~\cite{aide} and Tripwire~\cite{tripwire} compute cryptographic hashes of files and compare them against a known static baseline. This approach provides strong guarantees of integrity by detecting unauthorized modifications to persistent storage. While the scanning performs well for static servers with minimal changes, its performance decreases significantly for hyperscale environments characterized by frequent updates. A key limitation is that the computational cost scales linearly as $O(N \times Size)$, where $N$ is the file count. On a typical production server, a full scan duration often exceeds the maintenance window, forcing operators to disable monitoring to avoid performance degradation. Furthermore, every file modification generates an alert. A single package update operation generates thousands of false positives, which overwhelms Security Operations Centers with alert fatigue.

\noindent
\textbf{Runtime behavioral analysis. }Runtime behavioral analysis tracks system call sequences to detect anomalous execution patterns in real time~\cite{falco, unicorn}. Systems such as Falco~\cite{falco} and provenance graph analyzers~\cite{unicorn} intercept kernel events to identify malicious behavior. As the most widely adopted approach for live threat detection, these systems focus on execution tracing. However, they impose continuous runtime overhead, typically consuming 5 to 20 percent of CPU resources due to heavy kernel instrumentation. More critically, they track events rather than state. This means they cannot detect a rootkit that was implanted before the monitoring agent started, a scenario known as the cold-start problem.

Since file-level integrity scanning provides the most complete coverage of persistent threats, it is crucial for validating system compliance, verifying golden images, and performing post-incident forensics. Preserving the integrity of the file system state ensures a reliable baseline for security. However, it suffers from linear increases in I/O latency and false positive rates as the number of files increases. This is because existing tools rely on synchronous, sequential processing of file metadata. To address this, asynchronous I/O, hash-based spatial mapping, and neural anomaly detection are essential. \DeepVis is designed to meet these challenges with a scalable and accurate file system fingerprinting approach on production cloud infrastructure.

\subsection{The Attacker Paradox: Entropy and Structure}
%%\vspace{-.1cm}

To effectively detect evasive malware without relying on fragile signatures, it is essential to analyze the statistical properties of binary files. Modern malware authors face a fundamental trade-off between concealing their code and maintaining the structural validity required by the operating system loader. We identify two orthogonal dimensions that distinguish malicious payloads from benign system files: Entropy and Structural Density.

\begin{figure*}[!t]
    \centering
    \subfloat[Combined]{
        \includegraphics[width=0.45\columnwidth]{Figures/Background_entrophy/entropy_combined_a.pdf}
        \label{fig:entropy_hist}
    }%
    \hfill
    \subfloat[Text]{
        \includegraphics[width=0.45\columnwidth]{Figures/Background_entrophy/Background_Normal_text.pdf}
        \label{fig:entropy_text}
    }
    \hfill
    \subfloat[ELF Binary]{
        \includegraphics[width=0.45\columnwidth]{Figures/Background_entrophy/Background_System_binaray.pdf}
        \label{fig:entropy_elf}
    }%
    \hfill
    \subfloat[Packed Rootkit]{
        \includegraphics[width=0.45\columnwidth]{Figures/Background_entrophy/Background_Rootkit.pdf}
        \label{fig:entropy_rootkit}
    }
    \caption{File fingerprint analysis via byte-value histograms. (a) Combined entropy distribution across file types. (b) Text files use only printable ASCII, resulting in low entropy ($H \approx 4.8$) and zero null bytes. (c) ELF binaries show structured headers with significant zero-padding (40--85\% null bytes) for section alignment, yielding $H \approx 6.0$. (d) Packed rootkits eliminate all structure and null bytes ($<$1\%), maximizing entropy near the theoretical limit ($H \approx 8.0$).}
    \label{fig:entropy_combined}
\end{figure*}

Figure~\ref{fig:entropy_combined} illustrates the statistical differences through byte-value histograms across varying file types. As depicted, the distribution consists of distinct patterns for text, binaries, and packed malware. Text files (Figure~\ref{fig:entropy_combined}b) exhibit a characteristic spike in the printable ASCII range. Legitimate ELF binaries (Figure~\ref{fig:entropy_combined}c) show a prominent peak at 0x00. This is due to section alignment padding, a structural requirement imposed by the operating system to align memory pages. In contrast, packed or encrypted malware (Figure~\ref{fig:entropy_combined}d) displays a nearly uniform distribution across all byte values. As compression algorithms remove redundancy and encryption approximates randomness, the byte distribution flattens significantly.

This distinction creates the Attacker Paradox. Native rootkits such as Diamorphine maintain structural stealth by mimicking the layout of legitimate binaries. However, they remain vulnerable to signature-based detection tools such as YARA because their code contains known byte sequences. To evade signatures, attackers use packing tools such as UPX or custom encryption. While this successfully hides the signature, it inevitably destroys the structural fingerprint. As shown in Figure~\ref{fig:entropy_combined}d, the packing process maximizes information density, pushing the Shannon entropy toward the theoretical limit of 8.0 bits per byte and eliminating the zero-padding signal. Consequently, an attacker must choose between exposing a signature or creating a statistical anomaly. \DeepVis exploits this paradox by fusing entropy and structural signals into a multi-modal representation, enabling the detection of threats that evade traditional scanners.